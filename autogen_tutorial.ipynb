{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Gen Tutorial\n",
    "Note book written by John Adeojo\n",
    "Founder, and Chief Data Scientist at [Data-centric Solutions](https://www.data-centric-solutions.com/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "import openai\n",
    "\n",
    "script_dir = \"C:/Users/johna/OneDrive/Documents/api_keys/\"\n",
    "\n",
    "# Mount Google Drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "def get_apikey(script_dir=script_dir):\n",
    "    \"\"\"\n",
    "    Reads API key from a configuration file.\n",
    "\n",
    "    This function opens a configuration file named \"apikeys.yml\", reads the API key for OpenAI\n",
    "\n",
    "    Returns:\n",
    "    api_key (str): The OpenAI API key.\n",
    "    \"\"\"\n",
    "    # Update the script_dir path to point to the correct location in your Google Drive\n",
    "    script_dir = script_dir\n",
    "    file_path = os.path.join(script_dir, \"apikeys.yml\")\n",
    "\n",
    "    with open(file_path, 'r') as yamlfile:\n",
    "        loaded_yamlfile = yaml.safe_load(yamlfile)\n",
    "        API_KEY = loaded_yamlfile['openai']['api_key']\n",
    "\n",
    "    return API_KEY\n",
    "\n",
    "# Call the function to get the API key\n",
    "openai.api_key = get_apikey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen \n",
    "\n",
    "index_path = \"G:/My Drive/Data-Centric Solutions/07. Blog Posts/AutoGen/autogen_tutorial/indexes/\"\n",
    "configurations_path = \"G:/My Drive/Data-Centric Solutions/07. Blog Posts/AutoGen/autogen_tutorial/\"\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"configurations.json\",\n",
    "    file_location=configurations_path,\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-tubro-16k\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 1: Does a query based search for Wikipages\n",
    "from typing import Any, List\n",
    "import wikipedia\n",
    "from llama_index import download_loader, VectorStoreIndex, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.text_splitter import get_default_text_splitter\n",
    "import openai\n",
    "from pydantic import BaseModel\n",
    "from llama_index.program import OpenAIPydanticProgram\n",
    "from utils import get_apikey\n",
    "from typing import Callable, Dict, Optional, Union, List, Tuple, Any\n",
    "from llama_hub.wikipedia.base import WikipediaReader\n",
    "from llama_index import StorageContext\n",
    "from llama_index import load_index_from_storage\n",
    "import json\n",
    "from autogen import oai\n",
    "\n",
    "class WikiPageList(BaseModel):\n",
    "    \"Data model for WikiPageList\"\n",
    "    pages: list\n",
    "\n",
    "def wikipage_list(wikipediapages):\n",
    "        openai.api_key = get_apikey()\n",
    "\n",
    "        prompt_template_str = \"\"\"\n",
    "        Given the input {query}, \n",
    "        return the list of Wikipedia pages.\n",
    "        \"\"\"\n",
    "        program = OpenAIPydanticProgram.from_defaults(\n",
    "            output_cls=WikiPageList,\n",
    "            prompt_template_str=prompt_template_str,\n",
    "            verbose=True\n",
    "        )\n",
    "        return program(query=wikipediapages)\n",
    "\n",
    "def search_wikipedia(\n",
    "        query: str, lang: str = \"en\", results_limit: int = 5, **load_kwargs: Any\n",
    "    ) -> List[List]:\n",
    "    wikipedia.set_lang(lang)\n",
    "    return wikipedia.search(query, results=results_limit)\n",
    "\n",
    "def create_wikidocs(query: str) -> list:  # Added type hint\n",
    "    loader = WikipediaReader()  # Removed redefinition of WikipediaReader\n",
    "    return loader.load_data(pages=query)\n",
    "\n",
    "def save_index(index) -> None:\n",
    "    # index.set_index_id(\"vector_index\")\n",
    "    index.storage_context.persist(index_path)\n",
    "\n",
    "def index_wikipedia_pages(wikipediapages: str) -> VectorStoreIndex:  # Added type hint\n",
    "    global index\n",
    "    wikipageslist = wikipage_list(wikipediapages)\n",
    "    documents = create_wikidocs(wikipageslist)\n",
    "    text_splits = get_default_text_splitter(chunk_size=150, chunk_overlap=45)\n",
    "    parser = SimpleNodeParser.from_defaults(text_splitter=text_splits)\n",
    "    service_context = ServiceContext.from_defaults(node_parser=parser)\n",
    "    index =  VectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    save_index(index)\n",
    "\n",
    "\n",
    "def load_index(filepath: str):\n",
    "        # rebuild storage context\n",
    "        # storage_context = StorageContext.from_defaults(persist_dir=\"indexes\")\n",
    "        storage_context = StorageContext.from_defaults(persist_dir=index_path)\n",
    "        # load index\n",
    "        return load_index_from_storage(storage_context)\n",
    "\n",
    "def read_json_file(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def query_vector_db(\n",
    "        search_string: str,\n",
    "        file_path = index_path,\n",
    "        n_results: int = 10\n",
    "        # **kwargs,\n",
    ") -> None:  # Updated return type to None as the function now writes to a file\n",
    "    index = load_index(filepath=index_path)\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"compact\", verbose=True, similarity_top_k=n_results\n",
    "    )\n",
    "    nodes = query_engine.query(search_string).source_nodes\n",
    "    retrieved_context = {\n",
    "        # \"ids\": [],\n",
    "        \"text\": []\n",
    "    }\n",
    "    for node in nodes:\n",
    "        # retrieved_context[\"ids\"].append(node.node.id_)\n",
    "        retrieved_context[\"text\"].append(node.node.text)\n",
    "    \n",
    "    file_path = index_path + \"retrieved_context.json\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(retrieved_context, f)\n",
    "    \n",
    "    return retrieved_context\n",
    "\n",
    "# def generate_response(task:str, config_list=config_list):\n",
    "#      context = read_json_file(index_path + \"retrieved_context.json\")\n",
    "#      response = oai.Completion.create(\n",
    "#           config_list=config_list,\n",
    "#           prompt=f\"Use `{context}` to help you respond to the `{task}`\"\n",
    "#      )\n",
    "#      return response\n",
    "\n",
    "def generate_response(task:str):\n",
    "    openai.api_key = get_apikey()\n",
    "    context = read_json_file(index_path + \"retrieved_context.json\")\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": task},\n",
    "            {\"role\": \"assistant\", \"content\": f\"Use the information `{context}` to help you respond to the `{task}`\"}\n",
    "        ]\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "# # Call the function\n",
    "# def query_vector_db(\n",
    "#         n_results: int = 20,\n",
    "#         search_string: str = \"\",\n",
    "#         # **kwargs,\n",
    "# ) -> Dict[str, Union[List[str], List[List[str]]]]:\n",
    "#     openai.api_key = get_apikey()\n",
    "#     index = load_index(filepath=index_path)\n",
    "#     query_engine = index.as_query_engine(\n",
    "#         response_mode=\"compact\", verbose=True, similarity_top_k=n_results\n",
    "#     )\n",
    "#     nodes = query_engine.query(search_string).source_nodes\n",
    "#     results = {\n",
    "#         \"ids\": [],\n",
    "#         \"text\": []\n",
    "#     }\n",
    "#     for node in nodes:\n",
    "#         results[\"ids\"].append(node.node.id_)\n",
    "#         results[\"text\"].append(node.node.text)\n",
    "#     return results\n",
    "\n",
    "# def retrieve_docs(problem: str, n_results: int = 20, search_string: str = \"\", **kwargs):\n",
    "#     results = query_vector_db(problem, n_results, search_string)\n",
    "#     return results\n",
    "#     print(\"doc_ids: \", results[\"ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"search_wikipedia\",\n",
    "            \"description\": \"Use this to search for relevant Wikipedia pages\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A query to search and identify relevant Wikipedia pages\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"name\": \"index_wikipedia_pages\",\n",
    "            \"description\": \"Use this to index wikipedia pages\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"wikipediapages\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The output of the search_wikipedia function\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"wikipediapages\"],\n",
    "            },\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"name\": \"query_vector_db\",\n",
    "            \"description\": \"Useful for retrieving relevant context from the index\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"search_string\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"A query for which relevant context is required from the index\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"search_string\"],\n",
    "            },\n",
    "        },\n",
    "\n",
    "        {\n",
    "            \"name\": \"generate_response\",\n",
    "            \"description\": \"Useful to generate a response based on additional context provided by wikipedia search\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"task\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"task\": \"Your understanding of the task\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"task\"],\n",
    "            },\n",
    "        },\n",
    "        \n",
    "    ],\n",
    "    \"config_list\": config_list,\n",
    "    \"request_timeout\": 120,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent\n",
    "# import openai\n",
    "# from utils import get_apikey\n",
    "# from llama_index import StorageContext\n",
    "# from llama_index import load_index_from_storage\n",
    "\n",
    "\n",
    "# class WikipediaRetrieverUserProxyAgent(RetrieveUserProxyAgent):\n",
    "#     def __init__(self, index_path=index_path, **kwargs):\n",
    "#         super().__init__(**kwargs)  # Fixed superclass constructor call\n",
    "#         # openai.api_key = get_apikey()\n",
    "#         self.index_path = index_path\n",
    "\n",
    "#     @staticmethod\n",
    "#     def load_index(filepath: str):\n",
    "#         # rebuild storage context\n",
    "#         storage_context = StorageContext.from_defaults(persist_dir=\"indexes\")\n",
    "#         # load index\n",
    "#         return load_index_from_storage(storage_context, index_id=\"vector_index\")\n",
    "    \n",
    "#     def query_vector_db(\n",
    "#             self,\n",
    "#             n_results: int = 20,\n",
    "#             search_string: str = \"\",\n",
    "#             **kwargs,\n",
    "#     ) -> Dict[str, Union[List[str], List[List[str]]]]:\n",
    "#         index = self.load_index(self.index_path)\n",
    "#         query_engine = index.as_query_engine(\n",
    "#             response_mode=\"compact\", verbose=True, similarity_top_k=n_results\n",
    "#         )\n",
    "#         nodes = query_engine.query(search_string).source_nodes\n",
    "#         results = {\n",
    "#             \"ids\": [],\n",
    "#             \"text\": []\n",
    "#         }\n",
    "#         for node in nodes:\n",
    "#             results[\"ids\"].append(node.node.id_)\n",
    "#             results[\"text\"].append(node.node.text)\n",
    "#         return results \n",
    "\n",
    "#     def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs):\n",
    "#         results = self.query_vector_db(problem, n_results, search_string)\n",
    "#         self._results = results\n",
    "#         print(\"doc_ids: \", results[\"ids\"])\n",
    "\n",
    "# # class WikipediaRetrieverUserProxyAgent(RetrieveUserProxyAgent):\n",
    "# #     def __init__(self, index, **kwargs):\n",
    "# #         super().__init__(**kwargs)  # Fixed superclass constructor call\n",
    "# #         # Set the OpenAI API key when the class is instantiated\n",
    "# #         openai.api_key = get_apikey()\n",
    "# #         self.index=index\n",
    "\n",
    "# #     def query_vector_db(\n",
    "# #             self,\n",
    "# #             n_results: int = 20,\n",
    "# #             search_string: str = \"\",\n",
    "# #             **kwargs,\n",
    "# #     ) -> Dict[str, Union[List[str], List[List[str]]]]:\n",
    "# #         query_engine = self.index.as_query_engine(\n",
    "# #             response_mode=\"compact\", verbose=True, similarity_top_k=n_results\n",
    "# #         )\n",
    "# #         nodes = query_engine.query(search_string)\n",
    "# #         results = {\n",
    "# #             \"ids\": [],\n",
    "# #             \"text\": []\n",
    "# #         }\n",
    "# #         for node in nodes:\n",
    "# #             results[\"ids\"].append(node.node.id_)\n",
    "# #             results[\"text\"].append(node.node.text) \n",
    "    \n",
    "# #     def retrieve_docs(self, problem: str, n_results: int = 20, search_string: str = \"\", **kwargs):\n",
    "# #         results = self.query_vector_db(problem, n_results, search_string)\n",
    "# #         self._results = results\n",
    "# #         print(\"doc_ids: \", results[\"ids\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "What is the population of London?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-03 18:53:30] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n",
      "[autogen.oai.completion: 11-03 18:53:31] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n",
      "\u001b[33mwikisearch_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: search_wikipedia *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"query\": \"Population of London\"\n",
      "}\n",
      "\u001b[32m*****************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION search_wikipedia...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"search_wikipedia\" *****\u001b[0m\n",
      "['Demography of London', 'Greater London', 'Ethnic groups in London', 'London', 'List of London boroughs']\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-03 18:53:32] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n",
      "[autogen.oai.completion: 11-03 18:53:34] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n",
      "\u001b[33mwiki_index_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: index_wikipedia_pages *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"wikipediapages\": \"Demography of London, Greater London, London\"\n",
      "}\n",
      "\u001b[32m**********************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION index_wikipedia_pages...\u001b[0m\n",
      "Function call: WikiPageList with args: {\n",
      "  \"pages\": [\n",
      "    {\n",
      "      \"title\": \"Demography of London\",\n",
      "      \"url\": \"https://en.wikipedia.org/wiki/Demography_of_London\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"Greater London\",\n",
      "      \"url\": \"https://en.wikipedia.org/wiki/Greater_London\"\n",
      "    },\n",
      "    {\n",
      "      \"title\": \"London\",\n",
      "      \"url\": \"https://en.wikipedia.org/wiki/London\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"index_wikipedia_pages\" *****\u001b[0m\n",
      "None\n",
      "\u001b[32m******************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-03 18:53:54] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n",
      "[autogen.oai.completion: 11-03 18:53:56] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n",
      "\u001b[33mretrieve_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: query_vector_db *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"search_string\": \"Population of London\"\n",
      "}\n",
      "\u001b[32m****************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION query_vector_db...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"query_vector_db\" *****\u001b[0m\n",
      "Error: 'Response' object is not iterable\n",
      "\u001b[32m************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-03 18:54:02] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n",
      "[autogen.oai.completion: 11-03 18:54:04] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n",
      "\u001b[33mresponse_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: generate_response *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"task\": \"What is the population of London?\"\n",
      "}\n",
      "\u001b[32m******************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION generate_response...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"generate_response\" *****\u001b[0m\n",
      "Error: [Errno 2] No such file or directory: 'G:/My Drive/Data-Centric Solutions/07. Blog Posts/AutoGen/autogen_tutorial/indexes/retrieved_context.json'\n",
      "\u001b[32m**************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-03 18:54:08] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-03 18:54:09] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n",
      "[autogen.oai.completion: 11-03 18:54:11] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n",
      "\u001b[33mretrieve_agent\u001b[0m (to chat_manager):\n",
      "\n",
      "I'm sorry, I don't have the current population of London right now. But according to a 2019 estimate, Greater London had a population of about 9 million people. \n",
      "\n",
      "For the most accurate data, you might want to check the latest statistics from the UK Office for National Statistics or other relevant bodies.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "[autogen.oai.completion: 11-03 18:54:18] {788} WARNING - Completion was provided with a config_list, but the list was empty. Adopting default OpenAI behavior, which reads from the 'model' parameter instead.\n"
     ]
    }
   ],
   "source": [
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    system_message= \"You are the manager that ensures all the agents take neccessary steps to respond to the task.\",\n",
    "    code_execution_config={\"work_dir\": \"coding\"},\n",
    ")\n",
    "\n",
    "wikisearch_agent = autogen.AssistantAgent(\n",
    "    name=\"wikisearch_agent\",\n",
    "    system_message=\"Your job is to search for Wikipedia pages relevant to the task with the search_wikipedia tool.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "# wikise_agent = autogen.AssistantAgent(\n",
    "#     name=\"wikisearch_agent\",\n",
    "#     system_message=\"Your job is to search for relevant Wikipedia pages with the search_wikipedia tool.\",\n",
    "#     llm_config=llm_config,\n",
    "# )\n",
    "\n",
    "wiki_index_agent = autogen.AssistantAgent(\n",
    "    name=\"wiki_index_agent\",\n",
    "    system_message=\"Index relevant Wikipedia pages using the index_wikipedia_pages tool as suggested by the wikisearch_agent\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "retrieve_agent = autogen.AssistantAgent(\n",
    "    name=\"retrieve_agent\",\n",
    "    system_message=\"Retrieve relevant from the index information with the query_vector_db tool and communicate it\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "response_agent = autogen.AssistantAgent(\n",
    "    name=\"response_agent\",\n",
    "    system_message=\"Generate a response to the task based using the generate_response tool.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "\n",
    "\n",
    "# retrieve_response_agent = WikipediaRetrieverUserProxyAgent(\n",
    "#     name=\"retrieve_response_agent\",\n",
    "#     system_message=\"Generate response based on context\",\n",
    "#     human_input_mode=\"NEVER\",\n",
    "#     llm_config=llm_config,\n",
    "#     index_path=index_path\n",
    "    \n",
    "# )\n",
    "\n",
    "user_proxy.register_function(\n",
    "    function_map={\n",
    "        \"search_wikipedia\": search_wikipedia,\n",
    "        \"index_wikipedia_pages\":index_wikipedia_pages,\n",
    "        \"query_vector_db\":query_vector_db,\n",
    "        \"generate_response\":generate_response\n",
    "    }\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, wikisearch_agent, wiki_index_agent, retrieve_agent, response_agent], messages=[], max_round=100)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "\n",
    "user_proxy.initiate_chat(manager, message=\"What is the population of London?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
