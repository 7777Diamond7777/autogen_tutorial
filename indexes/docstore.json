{"docstore/metadata": {"ab184f8d-c760-412a-811c-1ba1cc8fe001": {"doc_hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "d8e52e57-c639-4549-8810-1ffa5dcaacaa": {"doc_hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "30517438-0b0f-44d2-8c9b-b3a469e5cfcc": {"doc_hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "65363817-5336-4d6c-bfb7-08d430301961": {"doc_hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "042edef9-b751-4724-8c31-1b1c502bdbc4": {"doc_hash": "962981222acc1c21d45907648bda363cfad2de15af326f281845775bedb7d3eb", "ref_doc_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001"}, "d9bee6e0-0494-4c2a-b4e9-88b6be0c408c": {"doc_hash": "0c2234155ad365113a421afce997de3fe619b228ecc1d0ebb4035b92780d2776", "ref_doc_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001"}, "2c308e00-297d-4274-831f-6526049e6691": {"doc_hash": "c466713a5503b561ec5f2a6cf517e4368a1a4b1d4845d7f2168603f4c1619089", "ref_doc_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001"}, "0d299ea1-def4-4e06-8320-c1c351cefe01": {"doc_hash": "6cd97b8e3ead7222dcd52a0735672a481789ec7a4c32b514c2d955e22697951f", "ref_doc_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001"}, "3aca4643-ff45-4ebb-a3bf-a2d0e7770665": {"doc_hash": "0740eef3429494747690e5f83fc0e1defd112f9663783c4ae8351de327395c3b", "ref_doc_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001"}, "f8d3eb6b-479f-4023-a950-7bc3ca480d12": {"doc_hash": "5772a10f8ef11fafa68cafea60bcb2d786bfdb76b404a9e5e9930aab5ab79954", "ref_doc_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001"}, "7bd111ce-34ab-4314-a3bc-ddfcd30bb2b6": {"doc_hash": "48b19da756eaec87a30d93fb98014dfb5dff097eaf1c9f7744b26bcad6c4130e", "ref_doc_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001"}, "aae2e016-7238-4cb0-939f-e6070a8aa2b0": {"doc_hash": "2925e5fbde318108d056e4696db5836ab5fb3eb0b261a6e82b48e17b75f91484", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "d270bfb9-b64d-4290-93b6-9bc0809d9f72": {"doc_hash": "40da7088811be1f6d0340445de714838efedc638fea4635d4ebec523dc289c82", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "d63bf570-de6e-4923-a692-79b972e8f7d3": {"doc_hash": "a7ae88c1c7f0a81436ce2840b99354193448df5ebc46817d815b8813d43d9638", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "93d2cd52-2b06-4b7c-9537-6bfdc89a54dd": {"doc_hash": "5279228e8bbadcd1850e399e77c87ba9fa2e29e6b2d5a042018799bae686b7b6", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "12a650d5-e667-4693-9374-2b69a03b3105": {"doc_hash": "15773b81ee349eaad78c33afa54f032944e15a8f6687638c62e219cfcfa20acf", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "ba35a5ec-c74a-44f2-bb73-bdd946eaca53": {"doc_hash": "a32032682fc6a5cb7f1912aa6f0ed61c16d2470fdb2a6d9db1f27a7727d0d881", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "87850a17-646f-484d-8adc-8ebb7997d04a": {"doc_hash": "204277825445b44facc907c01a93317af3922863eb07cfc4f99e1bf19e847b2d", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "546e8a40-0e9a-4836-80e3-26eea1c95f35": {"doc_hash": "66496028bacda89d398d23d89968c6f548f378e4e7ba9a0b43f98ba83f695734", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "f0f0ffbd-6f4f-4d5e-ae19-763997e3e798": {"doc_hash": "bbc9b539d140dc0056fa755e7bc3444099dc578097ec1b6fcf5ce939fc9fefc2", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "4c64169d-1b94-40a4-8b8c-de43afec38a9": {"doc_hash": "bae542a7e22b33dafefb473e613fe686edc0a49d1695925e4436e84c84a76ebe", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "9240cce1-811f-4bb8-80c4-610d31ed8dac": {"doc_hash": "77735136098f9c86bff7ed478d02653a7db3a196c1e53c9a5ec3d2bd52c1d74f", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "e461dcfe-4b53-4350-84b2-ea0fdc9c1293": {"doc_hash": "b59d9021afddea7520cc2bce663a02232af234146773f2bcefa38bcaba37423b", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "15cdbad5-4acd-41da-8bb5-bc07c0710d00": {"doc_hash": "278cb8ada367b74a733fbc93f143506e85d3a8c0d905b88a2360e9f1fb8e0d8c", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "b8bf6fae-609f-40c7-9d2a-4b67964724fe": {"doc_hash": "157e7a2ebea1fb376082f62d71732230058a4226ee8528a697cd3685677d8a18", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "ea183e0a-5dcf-4aca-8fc6-fac83ebd942a": {"doc_hash": "3b5302c27bc70a457e98e6bf9f5507e76128ca00751288e0be144baa5a017b2c", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "1b807f77-f3b1-4277-af65-2bc92fb8f713": {"doc_hash": "2333204d516b855471e6f57f3119fcd5fcd07d6c5ccc41fe155c1f4dc8e74382", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "0e574837-6d90-42f9-9879-33a3e0dec967": {"doc_hash": "ccdc3ef88a6ef2ddfd4d8de1f01cf3919b7e11a22c07408498009786125214da", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "593b5131-d881-49bb-b3e9-5379e5466ac2": {"doc_hash": "eb43d96b43f759be658d37130a4e26bd5f7024d2e59f355b1121b4f272cfbaaa", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "a4bda4c5-1458-4b2a-bb57-e0f4ad833f59": {"doc_hash": "8c4a21d95a83663e3467fd11c2825fd78da945bc4ceddd54c2571f160d507a49", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "b0d89754-d4f7-420f-acc4-430472dc2e7a": {"doc_hash": "4cf344624b9f9ef220a2ca799f867919967256e6eed812ce20e655d725b4bb3f", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "8db0584b-a51e-4010-a9c4-3718632d43e0": {"doc_hash": "d1ebc4fe474513353f65c7f89d71de9c0c70b1fef1916007267678d4c6ed1fba", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "3fc9e14a-e721-47a9-b556-7b5751ad1a69": {"doc_hash": "c1e35f781fe8ebbe8da468ce799843cbe12d6db7ed231decbf64c3dfbd315998", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "c4f010f0-118f-4eb0-b956-5de06903cfa2": {"doc_hash": "4a8fd957ae9a1d743e4155db314448c63b03f1efcf45c8cb5a9a13c327c1ed9f", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "ad60a16f-6058-4440-ba73-fecbb2e10134": {"doc_hash": "f5a60a2d561ffe231be0e05283877c3101372212506fd08b75d71667055bf5d3", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "d6969ce0-d9c8-49e2-aa92-f35d981c10de": {"doc_hash": "752aac4c9fc2dcc3cf7587868602135cd40941cd55e9505a461a98fb86ddc3e5", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "2145fe14-0623-467a-9a70-0b4be7ffd744": {"doc_hash": "39d33279c5c39ca4a968cedd56b3aaf6578133b93ed1cfa5ec73f2369758df6a", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "aa1dbc24-3b70-4a7b-a09f-e05f62f85363": {"doc_hash": "62c201ac77733253c47121dea4a313d1b62d4e5b000d3a557c8f0dba4734baa3", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "43e95adb-dcc3-47bb-9d35-1e42b4c9bce6": {"doc_hash": "249b827b84867249e734d3a7a9fb67a676e96350c8ef6e732b441297ca97de85", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "070f5bb1-d53e-4c4b-830c-83b53a1b20dd": {"doc_hash": "c9a98442bd3be9fab9d5f6e207c5df1388a63731506ea5736e3bb0d54a7b7c4a", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "03d6e643-8914-4a8e-a200-2627e93fc952": {"doc_hash": "93901994b5fe5f898fd6939abc155d687f79f13e76a8430a62369b92daff1338", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "6e6505f3-1b0f-428f-b72f-b283ed3b9bee": {"doc_hash": "e716d10a07bdbdc5c5432157d22b2d8b3a7ed12eac9e4ee248e3d5e027d130d4", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "63bb412f-cc66-43f8-a42d-b0f00b896c84": {"doc_hash": "4f518af2f9b10cb0e75fd8eac3852352d02e4e3d86c729e41fe796829006676e", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "d98a3d14-b13e-459a-a706-98da4cabd9a6": {"doc_hash": "e51b49ad0f03e09f2cc569451d45a133eed49597e56eb0c918aded4914085c8f", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "e71fe797-b5a4-410e-be86-66632df1cea7": {"doc_hash": "eaeee95eaf16fb3cf58191fccbcd211e9aee48b0967273d9cd6237271a6baed3", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "caabac1a-04d2-46e5-9ef5-d769c1197ceb": {"doc_hash": "a42b143a8b9385fba41632a54dcbb13299f11ac60a960c2f2d8c8a85de5dd5f2", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "5eff0e2c-96ba-4971-b74d-6bdd581b5d45": {"doc_hash": "36661ea1c6239721c1e56c5250e7b1b83b506a000cb2f365f0d4738625c2fbb4", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "0d6545b5-779a-4d61-8d4f-65bc48bbeb34": {"doc_hash": "f886567830c355bd62a3b9240b84a1e0421990e25c2dd325022aa1b07d5201c9", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "e18fbb09-ad51-4a5b-abc7-2e0199837f68": {"doc_hash": "8613b460366862c73d8ed61eafbc1c6a77fb8869ddf1b594141187fa77dd1b05", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "6ab731ed-436c-472f-8af6-7f2ff8db3ebf": {"doc_hash": "7a48da9959766e183373aa9b4ca5a4f512ea10bc6272a9454422a5b90d7d2120", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "02f37161-01e6-4be4-b903-cdda8ee5db03": {"doc_hash": "7c1ccf7ee9b696ea065b1aee4f9fb71b6520beac5bde94887837e55a4292feb5", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "0cc21dd4-d3c5-4d0c-91f3-459545059e5a": {"doc_hash": "c9a85462a57409a17278f50729a94069b35975579716d7b9b32b5a65120ef621", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "882e9675-a534-4714-a7b0-e2ee539ef6d5": {"doc_hash": "091907f1b34e343e553fcbf3d337d4ae27ca9ba71943aa32f555df434f732338", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "c067fafb-01af-4f55-bb98-4448ce136780": {"doc_hash": "54d51caa9d973f3f43272327f156ed94f51daca21e10d4cd950174978341aed4", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "9ce172bd-0660-46ce-a683-a32d5fa9f975": {"doc_hash": "5167eed6d039d37ae25b8664c0c9ca90e51ae9518bc20787689feb094e69c10c", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "fedd03f1-7661-495e-919f-bc326d0eee63": {"doc_hash": "4224966dc5bd5635c56d05339b8351dd4dbc5983da2baf4a12fc64d6f398556b", "ref_doc_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa"}, "eb7882f6-cda1-4c50-b0de-c670755f0775": {"doc_hash": "962981222acc1c21d45907648bda363cfad2de15af326f281845775bedb7d3eb", "ref_doc_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc"}, "4867c6f0-db3e-43b4-be54-9b631158da63": {"doc_hash": "0c2234155ad365113a421afce997de3fe619b228ecc1d0ebb4035b92780d2776", "ref_doc_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc"}, "7c9c08cc-729b-46a0-9d97-9db67fd7a53f": {"doc_hash": "c466713a5503b561ec5f2a6cf517e4368a1a4b1d4845d7f2168603f4c1619089", "ref_doc_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc"}, "46660992-36a4-450e-bc57-4871f3733ad0": {"doc_hash": "6cd97b8e3ead7222dcd52a0735672a481789ec7a4c32b514c2d955e22697951f", "ref_doc_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc"}, "fd8e1107-a286-4a48-8433-ce7067338e76": {"doc_hash": "0740eef3429494747690e5f83fc0e1defd112f9663783c4ae8351de327395c3b", "ref_doc_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc"}, "a36a5e4b-93a5-47af-8938-c71355dd27cb": {"doc_hash": "5772a10f8ef11fafa68cafea60bcb2d786bfdb76b404a9e5e9930aab5ab79954", "ref_doc_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc"}, "067e6976-1d78-44f6-a1b3-e8d70b337530": {"doc_hash": "48b19da756eaec87a30d93fb98014dfb5dff097eaf1c9f7744b26bcad6c4130e", "ref_doc_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc"}, "fdf71849-d38d-40e6-ac92-fc5063ce1561": {"doc_hash": "e699bb31200526dc96a3fc5bdf912cf8889bd0eabca141f2757b754260050eb8", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "23aa273d-a776-454b-b599-691c5ef8f2c1": {"doc_hash": "0e82e4ca9bf91d13242bdd42007b60aca1acbb30b16cd24bbd084e0264a947bf", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "2be8cca2-c579-4f24-8609-801ee3c988dc": {"doc_hash": "ec663d3b4a226c685888a43fd045372d12c7e9d535799c574af5cde93c038e1b", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "d73866da-49bc-471d-b20a-c8c506be2ce5": {"doc_hash": "be65bea8daefbbc0afbfd93c096f9704a4320858069f14d1f11453cd88e3ab68", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "e9d5a6d0-4aac-48e3-9771-7cefafd9d857": {"doc_hash": "ed9b7112b73006ce769610e2b0e87cf580fe46e5b641471d232822e82a716844", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "d111559a-76cb-43bf-801b-b9e82bfe92db": {"doc_hash": "c5280245199ed1cdd9a5b770758d9df31c90563f43da32edf2170a7d3bafc328", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "fcc64e6c-0718-4eb2-8283-bfd3d0e21de4": {"doc_hash": "3a0c469d52f790ca8b22f150c3a535c3e57407362d4111cae4630b97a5e2e12d", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "c1655410-1d99-48aa-abf1-bc37f679eee5": {"doc_hash": "db4a2304380594d0c8b6b4b816b9e950c1ff6f75394ee011cdba9bfee557aac2", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "9230d434-6f20-4780-b715-0ea35874a80f": {"doc_hash": "e07992500e07c8f18baf6b6d409465016a5f8393965f038fe016c112c7ca09bc", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "6242057d-19c2-440d-a21d-e0b78a357c10": {"doc_hash": "bef24af47dbb2bdf14da3663638d7524246d85d2141b50742a61ac4873a56b9c", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "b8df5e67-02dd-42ad-80a7-dd8541387f8d": {"doc_hash": "66787f28f23186ed8fbbb9a875deea86bc796e386361577f2d43b3f52835ec1a", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "a2db172b-2255-40da-9047-78e2fe627598": {"doc_hash": "a6feabba37b7165555b11d8e451d4bc50a8ebdbb381e898fd5d57a588bc254e4", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "8a4474f1-718b-41f0-b5ef-85f5eda713c9": {"doc_hash": "bff0873d63147e4a3b2fda062fe851de44aa92525272863e7e678d7b0a15dba9", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "4419fd28-5091-4e41-97dd-ab737346391b": {"doc_hash": "6c35b049fbb4f4611fe2cb326511cb3a681d0ee02e3018a233fd011d12efe88f", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "c74b22da-33c7-4853-9f12-ba1eb5029004": {"doc_hash": "ed0213063d89acb0e8d442346ff85211db828cfa94378625737ca837ed9bc321", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "0f79b9e9-8a3c-4dd7-8d4c-217a396c4d74": {"doc_hash": "775ab9da9ae5f838f62daed45c622fa3393a342bd0279b70df5dcea7c25500ff", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "fa15c857-9e1e-4f0c-9da6-8fb2efb31ab1": {"doc_hash": "1a1c0c34bdabed8dfe184c3a814c55c3ef7c201cd876ce05e97671f095832090", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "b691ee06-afd7-4fd2-b456-366c024208c6": {"doc_hash": "f3185d4072f0bd8725445b57de35147bdee5415640f6fdfdf5b2810581dda49d", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "570af176-733d-4f36-8770-3255c534391c": {"doc_hash": "35f5221a231e626ea41983b52b7952189c31fb803e375ea8d4b3b1c30ed4c94e", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "60b50137-4a7d-426c-86bf-3c98ebb88a0a": {"doc_hash": "b8faf8a2edc1598b7bef82f912ae1acba2416038bd7f4d20ded106beb6a81479", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "aa280449-4d53-42bf-ac81-153b123044e3": {"doc_hash": "9aa6f8b890a43eab85945dc8d9971091b054be32d6bb039b495e1c25ac81d127", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "1578dfbb-2b39-477d-a77e-e92ab04390f1": {"doc_hash": "d5d50bb35e1e7761baacfd82c840f3caf90c2b26f2e32f51d3548d130d4409cb", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "d7098edc-db47-4a53-b513-e683c1adb530": {"doc_hash": "3bd4f11b8d09db6e9c65e6424a034a926af1e158ebc3c2d9821276f8d1003b57", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "cb7cd854-1a79-4046-af28-cd663855f957": {"doc_hash": "1821d8ca8e1846e9ce277e8e0738dc07004bceba0f19f0d27746d2c9fffb45b2", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "8a6e4da9-99b0-4594-a4d4-6d8553356aac": {"doc_hash": "630d1c12b6bb8c1122be90a4155bb8fe0855cdbf56f758209671a0f0541a1930", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "3bb84b19-fc82-4e58-ad90-3532bba67f4d": {"doc_hash": "3f077c14dcdb60ac7b9c7821e80f9d753ac3cc75cb51cc599ff76d52ab44bcb8", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "75121b9f-721c-4abf-b29f-983265977ec4": {"doc_hash": "c13de5bb2d1e0984dd42a265df874f18016ebc5776a621ea333aaf689727b452", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "8778830c-3d49-4dad-bd4c-080be200c8d2": {"doc_hash": "cd25e8ef90c905d6ed7be2fb9c54724593d9f64f86da5498cbee3a40e08f2da2", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "af170c16-e002-479b-971b-c8c7a29ccbb0": {"doc_hash": "edf5fd876adae6dcea561c12b2795dcc3b18c7e66e58c35185c3beb37183736e", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "45fd9949-e19e-4bfe-b443-186b464973f0": {"doc_hash": "05589caa74eba4e79b0274545341527f61de09e611f417b3dd09607aacb50ec3", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "f7d7a1ba-be32-45af-9216-34266157bba6": {"doc_hash": "700cbf195e61ae3f02890186b87a5cf271914637cfb234fe2301e0e7d2c700b3", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "e37d5612-5f7f-4030-990c-791b5497a819": {"doc_hash": "410371d31de5bc10be5cd60cc3e97cc5ce9110f2f496977169209849b9ed40df", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "c6554df6-82ae-4d6f-a6e2-8cbecc0ecea6": {"doc_hash": "1c01192f784a6ac60da40e77fced5135576e8122d6ef96a99abb7daf175526e9", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "b4b92fc7-7b46-47a6-b87f-1786ff556f4c": {"doc_hash": "558cd09b78124b13b3c407a434ce767bfb5f447d0ec8fa138c11c96d7cb26680", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "d40f2aae-8322-4d24-83f8-dcb9eacb59db": {"doc_hash": "9be8ea61ac2b6c3d75b560a1951854fbce85fe936555cf1593783ef75c1c5a65", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "d5b94312-ee42-4e5f-a955-dfa0430f8008": {"doc_hash": "fcc61864deb451b49e9a0d0a03511dfcf969fd61af367e579ae4e2b14999e4cb", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "290e9d95-16d9-4e85-880e-1d316c9443df": {"doc_hash": "9d8c7b3575e1e5ec82f2d81f3194ea27cd2b591e092a3979e7c9aa3b848050af", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "11e65c59-21b6-495d-bbc5-54298c09598c": {"doc_hash": "9ec0d5ffaef03ba3e9b3d66a18d33f2cb80c305ae45cc318078ea0a64dbe7ea3", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "7b069af8-c943-4fb0-939f-5809a58eeb76": {"doc_hash": "6df6bc87f5d6fa91d367d1711ec9ff14376895eaceeae018e78cb2deaab3f934", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "772e1dbb-0835-49c6-ab2a-2180a3f771fd": {"doc_hash": "fc8c22701a62a606f06f0dc7e2e7fc8b7dbf4d4af857cb0cdc98f9926bbeb053", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "8daa2da7-00f7-4d5a-90f5-fa1df3cd8908": {"doc_hash": "66bffc058a3d23d3a2a9d79bd88d1ec2da419702329125cfbc1e934a0bdf8ee6", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "6195c20e-fc01-4d0b-9dee-801976e855ca": {"doc_hash": "cd12ac3848c160e95a7eab66e8478848029bb56a8901abad4878a24c955d0a00", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "72e70df6-1bcf-45ed-a43f-18cee1431209": {"doc_hash": "755f856f82861f65a029939882c6a53019a2d5bc977541fb83c2a7546f816733", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "df2ac080-26d7-4347-9f0c-a3e097959365": {"doc_hash": "7eb6abe9261fb699354cb63bdb6f081ad992cb951ce202eb30189fe1fb253c77", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "6c19ff9a-3d85-473c-bf54-f306495fae04": {"doc_hash": "db98a330363de3b04eb9444db2dc898fa0500f450868c0e3869cdab4583c4023", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "5129b3ff-01ab-4c25-8e2d-abc44be4b62e": {"doc_hash": "04429430131729963980fc931c105a1b4131404869bcbf9c621b6a567154663e", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "cf4c59b0-be0d-4047-95ee-f8261fec9e11": {"doc_hash": "5aaf6db180c6c5fb65cddd3f2fbffb5ca280800c0249d2cec755c41de6adab84", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "c7993032-c771-46f6-9019-da47b37813ae": {"doc_hash": "d66b46adfdf1eabd3a86d9260ee4294700dcc6a821d5f2898379bc8c4e6756a8", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "f32305e1-faf3-43fa-bca1-c9a8b3ff45b9": {"doc_hash": "ab01cc93eef819e12afa25545a8e4c9b7ca5cc331605eaa142968038a7053fc6", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "a1912be1-07e2-4e6b-b577-9c8148b714ce": {"doc_hash": "811559fc0f64ce22599c8a181370673feba0cb515c2b736e621db84b661ce139", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "4c857409-72e7-460d-b8d4-316fd13856c6": {"doc_hash": "b6c1413e67e7d1cb50432ee10fc3a22f90a607b41c8de7c18aa489b5ba4fab81", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "e061f964-72eb-486f-bf2b-77a476712fd5": {"doc_hash": "daa57beaa0297d6621f21a2b6ee0daabd8323f578a8c81fb03cecdee33bd0f12", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "64df011c-f258-4eee-bb38-f13816abcc4a": {"doc_hash": "be0d41e74d00d5dcc31ebea5602b812f1780e525ee1e04ae2f3b1582757b5356", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "dc084ac4-97b1-4297-8fd2-96e5452fe484": {"doc_hash": "4e0093a437d01a7ec7ade4d67b3661940abc80b21a70eccc7b2fc955f7d0605d", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "bdcf2ecb-e0ca-42fb-83f8-f0ebae060696": {"doc_hash": "e919fb5b711cfd5d0c9b52e9b2e89c06981b6101efe23f11bc301c7b42bc04c5", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "ec0111d0-404b-48fc-96af-e1861ff7c2cf": {"doc_hash": "65ddd08ff03ef2ee13d72aff2cf87085c783825f98c96d3c5fb0a2b0a70a9fda", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "2d04f17c-7afa-4e47-8c30-ec989dcd0e1a": {"doc_hash": "06b1627bb443cd99ba06e2af2100be1fbcc30a20247cda2599e257b6d84ec250", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "02820d6b-5f5e-4dfc-90d0-d7ab9ae9402e": {"doc_hash": "32841dd5ecdbb3215eb17e900f1d2f11f9dcb4b215c52daa9c0b845b4cbbe98d", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "277b92e6-a05b-4d83-af32-cebb2203090b": {"doc_hash": "8d569446b8cdc3c09e4a973765333f58b7f9767ec1f5278be4859845ad8ee1ae", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "b60653b7-3fbe-497e-ac43-df0ffe9177dd": {"doc_hash": "3dec2d7bbf3279c3eedc1beeda41460e9c472f453acad8465c676016a4555d0e", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "2d433ebe-7697-4aaf-9aab-9fe004c4412c": {"doc_hash": "f51317c9a1498a08c84eab680a6c750091902a974d805f6c8d53f2ed851b2943", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "be5c068d-5450-4c4e-88f1-c04163b345b1": {"doc_hash": "f3114a13315953815f3970ccdd8a46393d2cde5d6384682a67d2b9bf730c10a9", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "cdb0ad52-7a9d-4f4b-b561-e6b422f74002": {"doc_hash": "534a86bb5c3abdf374d882061f35535573ed385d355c3c2a41b6da814b84862a", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "d950918f-c029-4bd2-b943-fd13e36392d4": {"doc_hash": "a76ad939c95ab9a7c74f5434805e16b54d8af43fc6edb2e4acb4f4a5cdacfc03", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "bdd68646-85b9-4e9e-86a3-f8197a1ae594": {"doc_hash": "b6e0f0c619710e95b31a1b04fab808bb7bdbe55b584f5b910ed812b814d30287", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "68837bf5-a10c-4ea0-873c-3636efee4108": {"doc_hash": "1f1589f277ce8948cb75c9bddc7579900328d3b17429209156912c5690fee0a3", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "46313293-0201-45e0-9a4b-a0db39c0cf85": {"doc_hash": "1a0c16c5001f68f735d97558ff6349d13c310d7b544eb849cf8a19aaddf11bff", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "cc61d19e-f3b0-4e0f-aef7-4fa3d71e6bf9": {"doc_hash": "2aa23f526e2ce51fecb650d0492ab6640f13fdd1286882d4f923e93c6a6e6fe1", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "9d069afd-6296-4a25-bbd9-699780c73856": {"doc_hash": "0daa55c949812a1f09cf5f3e624eb84f2f06b5e922a806baf0cd2051670d6a5b", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "d65371b3-499b-4700-9b3c-e30619e15c0f": {"doc_hash": "fbec50344cd21836d0eddcac349046f3601be36a91757a5ae77a21e6443cdd21", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "c8bc0862-97bd-4ca8-8217-fc0585303cac": {"doc_hash": "4ff8d26ecd19072cdab5d156cc1bba9887cda4bbd390c2b906c7c917700382f8", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "d6242ba1-4b98-4595-be68-48a170bf9120": {"doc_hash": "9d2c3e7cde9324c9eb757879c3b8de77a5ee925f07c8f643aa312bf13ab3c2c4", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "ac35d610-8788-42e0-b097-50f0d50e51de": {"doc_hash": "65db32c4b752f6bef10e4da5824f691ce28e7e1cb5b33b212e2550a65714a601", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "fddbddea-bc4d-4f8b-b38b-9e9bcdb6ae3e": {"doc_hash": "b80eb37da4b9df7b10665e59c8a2e6f9ac60eab8a0a42b580c05adbf3a7c6634", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "595aebba-f9e5-413d-b27c-547e074b82e7": {"doc_hash": "6f3d041df908079bd1185e831752cbbc702a4fbd96ebf5ad527be94872853e8d", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "70f378c9-0dbe-492b-a7a1-70ae65c4d445": {"doc_hash": "2be7d9031e7331e38c08e427a1f106540e00c030688b556ed6dfdf1b72737ef0", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "1fafb5e2-7d1e-4be8-a5ea-666ae33cf0c4": {"doc_hash": "ced4e8ae64266bd377f0879136bb1410a1e16833e5c0ae9e6b5edbdff199d358", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "276f85bf-ecad-4562-a48d-8a7a07742bae": {"doc_hash": "fdc00dee40369ef3162da46de28a28d9e257a0de361a07347f987a687697e786", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "e8511380-03a2-42ee-9304-9a6044348b4a": {"doc_hash": "3f17ae9e449428509c86f52ebfb120a8347d008c6470ed296483a0e88ccbd955", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "4bcbb9c5-077e-4e4e-b40c-f6a75032a85f": {"doc_hash": "b685517f5b7b16083ec962aa28df8129a9897253b148242419690a53757294cb", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "263fff6d-4c29-47c2-a639-42e8632f22be": {"doc_hash": "f41a7ef58b78871da2fb1064ed3952e1db7a3496995f598bb493eef69e6478cf", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "cede9f70-aeea-4752-a903-ff301f877f77": {"doc_hash": "8ed1709e5e3b6a168fe02db4c9e84b42825b6704782215a476de5954477c3872", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "42a45e7b-2556-47a9-a9e7-9b3a1f2584fd": {"doc_hash": "3f452f564c2d7fe17f259d4556db6d579129a5c6dcc8a19a4f077ad77870ef40", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}, "34fb0525-c09e-43d9-af23-6cd13886c5a7": {"doc_hash": "40eb882a8163af4b498f5034b7be09bd23ea097d51ac9604d05a02dea1ccbad3", "ref_doc_id": "65363817-5336-4d6c-bfb7-08d430301961"}}, "docstore/data": {"042edef9-b751-4724-8c31-1b1c502bdbc4": {"__data__": {"id_": "042edef9-b751-4724-8c31-1b1c502bdbc4", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "3": {"node_id": "d9bee6e0-0494-4c2a-b4e9-88b6be0c408c", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "0c2234155ad365113a421afce997de3fe619b228ecc1d0ebb4035b92780d2776"}}, "hash": "962981222acc1c21d45907648bda363cfad2de15af326f281845775bedb7d3eb", "text": "The AI Safety Summit was an international conference discussing the safety and regulation of artificial intelligence. It was held at Bletchley Park, Milton Keynes, United Kingdom, on 1\u20132 November 2023. It was the first ever global summit on artificial intelligence.\n\n\n== Background ==\nThe prime minister of the United Kingdom, Rishi Sunak, has made AI one of the priorities of his government, announcing that the UK would host a global AI Safety conference in autumn 2023.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d9bee6e0-0494-4c2a-b4e9-88b6be0c408c": {"__data__": {"id_": "d9bee6e0-0494-4c2a-b4e9-88b6be0c408c", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "042edef9-b751-4724-8c31-1b1c502bdbc4", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "962981222acc1c21d45907648bda363cfad2de15af326f281845775bedb7d3eb"}, "3": {"node_id": "2c308e00-297d-4274-831f-6526049e6691", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "c466713a5503b561ec5f2a6cf517e4368a1a4b1d4845d7f2168603f4c1619089"}}, "hash": "0c2234155ad365113a421afce997de3fe619b228ecc1d0ebb4035b92780d2776", "text": "== Venue ==\nBletchley Park was a World War II codebreaking facility established by the British government on the site of a Victorian manor and is in the British city of Milton Keynes. It has played an important role in the history of computing, with some of the first modern computers being built at the facility.\n\n\n== Agenda ==\nThe tech entrepreneur Elon Musk and Sunak did a live interview on AI safety on 2 November on X.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2c308e00-297d-4274-831f-6526049e6691": {"__data__": {"id_": "2c308e00-297d-4274-831f-6526049e6691", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "d9bee6e0-0494-4c2a-b4e9-88b6be0c408c", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "0c2234155ad365113a421afce997de3fe619b228ecc1d0ebb4035b92780d2776"}, "3": {"node_id": "0d299ea1-def4-4e06-8320-c1c351cefe01", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "6cd97b8e3ead7222dcd52a0735672a481789ec7a4c32b514c2d955e22697951f"}}, "hash": "c466713a5503b561ec5f2a6cf517e4368a1a4b1d4845d7f2168603f4c1619089", "text": "== Agenda ==\nThe tech entrepreneur Elon Musk and Sunak did a live interview on AI safety on 2 November on X.\n\n\n== Outcomes ==\n28 countries at the summit, including the United States, China, and the European Union, have issued an agreement known as the Bletchley Declaration, calling for international co-operation to manage the challenges and risks of artificial intelligence. Emphasis has been placed on regulating \"Frontier AI\", a term for the latest and most powerful AI systems.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0d299ea1-def4-4e06-8320-c1c351cefe01": {"__data__": {"id_": "0d299ea1-def4-4e06-8320-c1c351cefe01", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "2c308e00-297d-4274-831f-6526049e6691", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "c466713a5503b561ec5f2a6cf517e4368a1a4b1d4845d7f2168603f4c1619089"}, "3": {"node_id": "3aca4643-ff45-4ebb-a3bf-a2d0e7770665", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "0740eef3429494747690e5f83fc0e1defd112f9663783c4ae8351de327395c3b"}}, "hash": "6cd97b8e3ead7222dcd52a0735672a481789ec7a4c32b514c2d955e22697951f", "text": "Emphasis has been placed on regulating \"Frontier AI\", a term for the latest and most powerful AI systems. Concerns that have been raised at the summit include the potential use of AI for terrorism, criminal activity, and warfare, as well as existential risk posed to humanity as a whole.\nThe president of the United States, Joe Biden, signed an executive order requiring AI developers to share safety results with the US government. The US government also announced the creation of an American AI Safety Institute, as part of the National Institute of Standards and Technology.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3aca4643-ff45-4ebb-a3bf-a2d0e7770665": {"__data__": {"id_": "3aca4643-ff45-4ebb-a3bf-a2d0e7770665", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "0d299ea1-def4-4e06-8320-c1c351cefe01", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "6cd97b8e3ead7222dcd52a0735672a481789ec7a4c32b514c2d955e22697951f"}, "3": {"node_id": "f8d3eb6b-479f-4023-a950-7bc3ca480d12", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "5772a10f8ef11fafa68cafea60bcb2d786bfdb76b404a9e5e9930aab5ab79954"}}, "hash": "0740eef3429494747690e5f83fc0e1defd112f9663783c4ae8351de327395c3b", "text": "The US government also announced the creation of an American AI Safety Institute, as part of the National Institute of Standards and Technology.\n\n\n== Future ==\nThe next AI Safety Summit is planned to be hosted by South Korea in mid-2024, followed by France around late-2024.\n\n\n== Notable attendees ==\nThe following individuals are expected to attend the summit:\nRishi Sunak, prime minister of the United Kingdom\nKamala Harris, vice president of the United States\nCharles III, king of the United Kingdom (attending virtually)\nElon Musk, CEO of Tesla,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f8d3eb6b-479f-4023-a950-7bc3ca480d12": {"__data__": {"id_": "f8d3eb6b-479f-4023-a950-7bc3ca480d12", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "3aca4643-ff45-4ebb-a3bf-a2d0e7770665", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "0740eef3429494747690e5f83fc0e1defd112f9663783c4ae8351de327395c3b"}, "3": {"node_id": "7bd111ce-34ab-4314-a3bc-ddfcd30bb2b6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "48b19da756eaec87a30d93fb98014dfb5dff097eaf1c9f7744b26bcad6c4130e"}}, "hash": "5772a10f8ef11fafa68cafea60bcb2d786bfdb76b404a9e5e9930aab5ab79954", "text": "prime minister of the United Kingdom\nKamala Harris, vice president of the United States\nCharles III, king of the United Kingdom (attending virtually)\nElon Musk, CEO of Tesla, owner of SpaceX, Neuralink, and xAI\nGiorgia Meloni, prime minister of Italy\nUrsula von der Leyen, president of the European Commission\nSam Altman, CEO of OpenAI\nNick Clegg, former British politician and president of global affairs at Meta Platforms\nMustafa Suleyman, co-founder of DeepMind\nMichelle Donelan,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7bd111ce-34ab-4314-a3bc-ddfcd30bb2b6": {"__data__": {"id_": "7bd111ce-34ab-4314-a3bc-ddfcd30bb2b6", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ab184f8d-c760-412a-811c-1ba1cc8fe001", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "f8d3eb6b-479f-4023-a950-7bc3ca480d12", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "5772a10f8ef11fafa68cafea60bcb2d786bfdb76b404a9e5e9930aab5ab79954"}}, "hash": "48b19da756eaec87a30d93fb98014dfb5dff097eaf1c9f7744b26bcad6c4130e", "text": "CEO of OpenAI\nNick Clegg, former British politician and president of global affairs at Meta Platforms\nMustafa Suleyman, co-founder of DeepMind\nMichelle Donelan, UK secretary of state for Science, Innovation and Technology\nV\u011bra Jourov\u00e1, the European Commission\u2019s vice-president for Values and Transparency\nGina Raimondo, United States secretary of commerce\nWu Zhaohui, Chinese vice-minister of science and technology\n\n\n== References ==", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "aae2e016-7238-4cb0-939f-e6070a8aa2b0": {"__data__": {"id_": "aae2e016-7238-4cb0-939f-e6070a8aa2b0", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "3": {"node_id": "d270bfb9-b64d-4290-93b6-9bc0809d9f72", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "40da7088811be1f6d0340445de714838efedc638fea4635d4ebec523dc289c82"}}, "hash": "2925e5fbde318108d056e4696db5836ab5fb3eb0b261a6e82b48e17b75f91484", "text": "AI safety is an interdisciplinary field concerned with preventing accidents, misuse, or other harmful consequences that could result from artificial intelligence (AI) systems. It encompasses machine ethics and AI alignment, which aim to make AI systems moral and beneficial, and AI safety encompasses technical problems including monitoring systems for risks and making them highly reliable. Beyond AI research, it involves developing norms and policies that promote safety.\n\n\n== Motivations ==\nAI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology \u2013 though surveys suggest that experts take high consequence risks seriously.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d270bfb9-b64d-4290-93b6-9bc0809d9f72": {"__data__": {"id_": "d270bfb9-b64d-4290-93b6-9bc0809d9f72", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "aae2e016-7238-4cb0-939f-e6070a8aa2b0", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "2925e5fbde318108d056e4696db5836ab5fb3eb0b261a6e82b48e17b75f91484"}, "3": {"node_id": "d63bf570-de6e-4923-a692-79b972e8f7d3", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "a7ae88c1c7f0a81436ce2840b99354193448df5ebc46817d815b8813d43d9638"}}, "hash": "40da7088811be1f6d0340445de714838efedc638fea4635d4ebec523dc289c82", "text": "== Motivations ==\nAI researchers have widely differing opinions about the severity and primary sources of risk posed by AI technology \u2013 though surveys suggest that experts take high consequence risks seriously. In two surveys of AI researchers, the median respondent was optimistic about AI overall, but placed a 5% probability on an \u201cextremely bad (e.g. human extinction)\u201d outcome of advanced AI. In a 2022 survey of the Natural language processing (NLP) community,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d63bf570-de6e-4923-a692-79b972e8f7d3": {"__data__": {"id_": "d63bf570-de6e-4923-a692-79b972e8f7d3", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "d270bfb9-b64d-4290-93b6-9bc0809d9f72", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "40da7088811be1f6d0340445de714838efedc638fea4635d4ebec523dc289c82"}, "3": {"node_id": "93d2cd52-2b06-4b7c-9537-6bfdc89a54dd", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "5279228e8bbadcd1850e399e77c87ba9fa2e29e6b2d5a042018799bae686b7b6"}}, "hash": "a7ae88c1c7f0a81436ce2840b99354193448df5ebc46817d815b8813d43d9638", "text": "human extinction)\u201d outcome of advanced AI. In a 2022 survey of the Natural language processing (NLP) community, 37% agreed or weakly agreed that it is plausible that AI decisions could lead to a catastrophe that is \u201cat least as bad as an all-out nuclear war.\u201d Scholars discuss current risks from critical systems failures, bias, and AI enabled surveillance; emerging risks from technological unemployment, digital manipulation, and weaponization; and speculative risks from losing control of future artificial general intelligence (AGI) agents.Some have criticized concerns about AGI,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "93d2cd52-2b06-4b7c-9537-6bfdc89a54dd": {"__data__": {"id_": "93d2cd52-2b06-4b7c-9537-6bfdc89a54dd", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "d63bf570-de6e-4923-a692-79b972e8f7d3", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "a7ae88c1c7f0a81436ce2840b99354193448df5ebc46817d815b8813d43d9638"}, "3": {"node_id": "12a650d5-e667-4693-9374-2b69a03b3105", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "15773b81ee349eaad78c33afa54f032944e15a8f6687638c62e219cfcfa20acf"}}, "hash": "5279228e8bbadcd1850e399e77c87ba9fa2e29e6b2d5a042018799bae686b7b6", "text": "bias, and AI enabled surveillance; emerging risks from technological unemployment, digital manipulation, and weaponization; and speculative risks from losing control of future artificial general intelligence (AGI) agents.Some have criticized concerns about AGI, such as Andrew Ng who compared them in 2015 to \"worrying about overpopulation on Mars when we have not even set foot on the planet yet.\" Stuart J. Russell on the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it.\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "12a650d5-e667-4693-9374-2b69a03b3105": {"__data__": {"id_": "12a650d5-e667-4693-9374-2b69a03b3105", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "93d2cd52-2b06-4b7c-9537-6bfdc89a54dd", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "5279228e8bbadcd1850e399e77c87ba9fa2e29e6b2d5a042018799bae686b7b6"}, "3": {"node_id": "ba35a5ec-c74a-44f2-bb73-bdd946eaca53", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "a32032682fc6a5cb7f1912aa6f0ed61c16d2470fdb2a6d9db1f27a7727d0d881"}}, "hash": "15773b81ee349eaad78c33afa54f032944e15a8f6687638c62e219cfcfa20acf", "text": "\" Stuart J. Russell on the other side urges caution, arguing that \"it is better to anticipate human ingenuity than to underestimate it.\"\n\n\n== Background ==\nRisks from AI began to be seriously discussed at the start of the computer age:\n\nMoreover, if we move in the direction of making machines which learn and whose behavior is modified by experience, we must face the fact that every degree of independence we give the machine is a degree of possible defiance of our wishes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ba35a5ec-c74a-44f2-bb73-bdd946eaca53": {"__data__": {"id_": "ba35a5ec-c74a-44f2-bb73-bdd946eaca53", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "12a650d5-e667-4693-9374-2b69a03b3105", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "15773b81ee349eaad78c33afa54f032944e15a8f6687638c62e219cfcfa20acf"}, "3": {"node_id": "87850a17-646f-484d-8adc-8ebb7997d04a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "204277825445b44facc907c01a93317af3922863eb07cfc4f99e1bf19e847b2d"}}, "hash": "a32032682fc6a5cb7f1912aa6f0ed61c16d2470fdb2a6d9db1f27a7727d0d881", "text": "From 2008 to 2009, the Association for the Advancement of Artificial Intelligence (AAAI) commissioned a study to explore and address potential long-term societal influences of AI research and development. The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "87850a17-646f-484d-8adc-8ebb7997d04a": {"__data__": {"id_": "87850a17-646f-484d-8adc-8ebb7997d04a", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "ba35a5ec-c74a-44f2-bb73-bdd946eaca53", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "a32032682fc6a5cb7f1912aa6f0ed61c16d2470fdb2a6d9db1f27a7727d0d881"}, "3": {"node_id": "546e8a40-0e9a-4836-80e3-26eea1c95f35", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "66496028bacda89d398d23d89968c6f548f378e4e7ba9a0b43f98ba83f695734"}}, "hash": "204277825445b44facc907c01a93317af3922863eb07cfc4f99e1bf19e847b2d", "text": "The panel was generally skeptical of the radical views expressed by science-fiction authors but agreed that \"additional research would be valuable on methods for understanding and verifying the range of behaviors of complex computational systems to minimize unexpected outcomes.\"In 2011, Roman Yampolskiy introduced the term \"AI safety engineering\" at the Philosophy and Theory of Artificial Intelligence conference, listing prior failures of AI systems and arguing that \"the frequency and seriousness of such events will steadily increase as AIs become more capable.\"In 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "546e8a40-0e9a-4836-80e3-26eea1c95f35": {"__data__": {"id_": "546e8a40-0e9a-4836-80e3-26eea1c95f35", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "87850a17-646f-484d-8adc-8ebb7997d04a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "204277825445b44facc907c01a93317af3922863eb07cfc4f99e1bf19e847b2d"}, "3": {"node_id": "f0f0ffbd-6f4f-4d5e-ae19-763997e3e798", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "bbc9b539d140dc0056fa755e7bc3444099dc578097ec1b6fcf5ce939fc9fefc2"}}, "hash": "66496028bacda89d398d23d89968c6f548f378e4e7ba9a0b43f98ba83f695734", "text": "\"In 2014, philosopher Nick Bostrom published the book Superintelligence: Paths, Dangers, Strategies. He has the opinion that the rise of AGI has the potential to create various societal issues, ranging from the displacement of the workforce by AI, manipulation of political and military structures, to even the possibility of human extinction. His argument that future advanced systems may pose a threat to human existence prompted Elon Musk, Bill Gates, and Stephen Hawking to voice similar concerns.\nIn 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f0f0ffbd-6f4f-4d5e-ae19-763997e3e798": {"__data__": {"id_": "f0f0ffbd-6f4f-4d5e-ae19-763997e3e798", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "546e8a40-0e9a-4836-80e3-26eea1c95f35", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "66496028bacda89d398d23d89968c6f548f378e4e7ba9a0b43f98ba83f695734"}, "3": {"node_id": "4c64169d-1b94-40a4-8b8c-de43afec38a9", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "bae542a7e22b33dafefb473e613fe686edc0a49d1695925e4436e84c84a76ebe"}}, "hash": "bbc9b539d140dc0056fa755e7bc3444099dc578097ec1b6fcf5ce939fc9fefc2", "text": "In 2015, dozens of artificial intelligence experts signed an open letter on artificial intelligence calling for research on the societal impacts of AI and outlining concrete directions. To date, the letter has been signed by over 8000 people including Yann LeCun, Shane Legg, Yoshua Bengio, and Stuart Russell.\nIn the same year, a group of academics led by professor Stuart Russell founded the Center for Human-Compatible AI at the University of California Berkeley and the Future of Life Institute awarded $6.5 million in grants for research aimed at \"ensuring artificial intelligence (AI) remains safe, ethical and beneficial.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4c64169d-1b94-40a4-8b8c-de43afec38a9": {"__data__": {"id_": "4c64169d-1b94-40a4-8b8c-de43afec38a9", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "f0f0ffbd-6f4f-4d5e-ae19-763997e3e798", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "bbc9b539d140dc0056fa755e7bc3444099dc578097ec1b6fcf5ce939fc9fefc2"}, "3": {"node_id": "9240cce1-811f-4bb8-80c4-610d31ed8dac", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "77735136098f9c86bff7ed478d02653a7db3a196c1e53c9a5ec3d2bd52c1d74f"}}, "hash": "bae542a7e22b33dafefb473e613fe686edc0a49d1695925e4436e84c84a76ebe", "text": "\"In 2016, the White House Office of Science and Technology Policy and Carnegie Mellon University announced The Public Workshop on Safety and Control for Artificial Intelligence, which was one of a sequence of four White House workshops aimed at investigating \"the advantages and drawbacks\" of AI. In the same year, Concrete Problems in AI Safety \u2013 one of the first and most influential technical AI Safety agendas \u2013 was published.In 2017, the Future of Life Institute sponsored the Asilomar Conference on Beneficial AI, where more than 100 thought leaders formulated principles for beneficial AI including \"Race Avoidance: Teams developing AI systems should actively cooperate to avoid corner-cutting on safety standards.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9240cce1-811f-4bb8-80c4-610d31ed8dac": {"__data__": {"id_": "9240cce1-811f-4bb8-80c4-610d31ed8dac", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "4c64169d-1b94-40a4-8b8c-de43afec38a9", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "bae542a7e22b33dafefb473e613fe686edc0a49d1695925e4436e84c84a76ebe"}, "3": {"node_id": "e461dcfe-4b53-4350-84b2-ea0fdc9c1293", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "b59d9021afddea7520cc2bce663a02232af234146773f2bcefa38bcaba37423b"}}, "hash": "77735136098f9c86bff7ed478d02653a7db3a196c1e53c9a5ec3d2bd52c1d74f", "text": "\"In 2018, the DeepMind Safety team outlined AI safety problems in specification, robustness, and assurance. The following year, researchers organized a workshop at ICLR that focused on these problem areas.In 2021, Unsolved Problems in ML Safety was published, outlining research directions in robustness, monitoring, alignment, and systemic safety.In 2023, Rishi Sunak said he wants the United Kingdom to be the \"geographical home of global AI safety regulation\" and to host the first global summit on AI safety.\n\n\n== Research foci ==\nAI safety research areas include robustness, monitoring, and alignment.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e461dcfe-4b53-4350-84b2-ea0fdc9c1293": {"__data__": {"id_": "e461dcfe-4b53-4350-84b2-ea0fdc9c1293", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "9240cce1-811f-4bb8-80c4-610d31ed8dac", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "77735136098f9c86bff7ed478d02653a7db3a196c1e53c9a5ec3d2bd52c1d74f"}, "3": {"node_id": "15cdbad5-4acd-41da-8bb5-bc07c0710d00", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "278cb8ada367b74a733fbc93f143506e85d3a8c0d905b88a2360e9f1fb8e0d8c"}}, "hash": "b59d9021afddea7520cc2bce663a02232af234146773f2bcefa38bcaba37423b", "text": "== Research foci ==\nAI safety research areas include robustness, monitoring, and alignment.\n\n\n=== Robustness ===\n\n\n==== Adversarial robustness ====\nAI systems are often vulnerable to adversarial examples or \u201cinputs to machine learning (ML) models that an attacker has intentionally designed to cause the model to make a mistake\u201d. For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "15cdbad5-4acd-41da-8bb5-bc07c0710d00": {"__data__": {"id_": "15cdbad5-4acd-41da-8bb5-bc07c0710d00", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "e461dcfe-4b53-4350-84b2-ea0fdc9c1293", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "b59d9021afddea7520cc2bce663a02232af234146773f2bcefa38bcaba37423b"}, "3": {"node_id": "b8bf6fae-609f-40c7-9d2a-4b67964724fe", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "157e7a2ebea1fb376082f62d71732230058a4226ee8528a697cd3685677d8a18"}}, "hash": "278cb8ada367b74a733fbc93f143506e85d3a8c0d905b88a2360e9f1fb8e0d8c", "text": "For example, in 2013, Szegedy et al. discovered that adding specific imperceptible perturbations to an image could cause it to be misclassified with high confidence. This continues to be an issue with neural networks, though in recent work the perturbations are generally large enough to be perceptible.\nAll of the images on the right are predicted to be an ostrich after the perturbation is applied. (Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.Adversarial robustness is often associated with security.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b8bf6fae-609f-40c7-9d2a-4b67964724fe": {"__data__": {"id_": "b8bf6fae-609f-40c7-9d2a-4b67964724fe", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "15cdbad5-4acd-41da-8bb5-bc07c0710d00", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "278cb8ada367b74a733fbc93f143506e85d3a8c0d905b88a2360e9f1fb8e0d8c"}, "3": {"node_id": "ea183e0a-5dcf-4aca-8fc6-fac83ebd942a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "3b5302c27bc70a457e98e6bf9f5507e76128ca00751288e0be144baa5a017b2c"}}, "hash": "157e7a2ebea1fb376082f62d71732230058a4226ee8528a697cd3685677d8a18", "text": "(Left) is a correctly predicted sample, (center) perturbation applied magnified by 10x, (right) adversarial example.Adversarial robustness is often associated with security. Researchers demonstrated that an audio signal could be imperceptibly modified so that speech-to-text systems transcribe it to any message the attacker chooses. Network intrusion and malware detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.\nModels that represent objectives (reward models) must also be adversarially robust.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ea183e0a-5dcf-4aca-8fc6-fac83ebd942a": {"__data__": {"id_": "ea183e0a-5dcf-4aca-8fc6-fac83ebd942a", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "b8bf6fae-609f-40c7-9d2a-4b67964724fe", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "157e7a2ebea1fb376082f62d71732230058a4226ee8528a697cd3685677d8a18"}, "3": {"node_id": "1b807f77-f3b1-4277-af65-2bc92fb8f713", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "2333204d516b855471e6f57f3119fcd5fcd07d6c5ccc41fe155c1f4dc8e74382"}}, "hash": "3b5302c27bc70a457e98e6bf9f5507e76128ca00751288e0be144baa5a017b2c", "text": "Network intrusion and malware detection systems also must be adversarially robust since attackers may design their attacks to fool detectors.\nModels that represent objectives (reward models) must also be adversarially robust. For example, a reward model might estimate how helpful a text response is and a language model might be trained to maximize this score. Researchers have shown that if a language model is trained for long enough, it will leverage the vulnerabilities of the reward model to achieve a better score and perform worse on the intended task. This issue can be addressed by improving the adversarial robustness of the reward model.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1b807f77-f3b1-4277-af65-2bc92fb8f713": {"__data__": {"id_": "1b807f77-f3b1-4277-af65-2bc92fb8f713", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "ea183e0a-5dcf-4aca-8fc6-fac83ebd942a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "3b5302c27bc70a457e98e6bf9f5507e76128ca00751288e0be144baa5a017b2c"}, "3": {"node_id": "0e574837-6d90-42f9-9879-33a3e0dec967", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "ccdc3ef88a6ef2ddfd4d8de1f01cf3919b7e11a22c07408498009786125214da"}}, "hash": "2333204d516b855471e6f57f3119fcd5fcd07d6c5ccc41fe155c1f4dc8e74382", "text": "This issue can be addressed by improving the adversarial robustness of the reward model. More generally, any AI system used to evaluate another AI system must be adversarially robust. This could include monitoring tools, since they could also potentially be tampered with to produce a higher reward.\n\n\n=== Monitoring ===\n\n\n==== Estimating uncertainty ====\nIt is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0e574837-6d90-42f9-9879-33a3e0dec967": {"__data__": {"id_": "0e574837-6d90-42f9-9879-33a3e0dec967", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "1b807f77-f3b1-4277-af65-2bc92fb8f713", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "2333204d516b855471e6f57f3119fcd5fcd07d6c5ccc41fe155c1f4dc8e74382"}, "3": {"node_id": "593b5131-d881-49bb-b3e9-5379e5466ac2", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "eb43d96b43f759be658d37130a4e26bd5f7024d2e59f355b1121b4f272cfbaaa"}}, "hash": "ccdc3ef88a6ef2ddfd4d8de1f01cf3919b7e11a22c07408498009786125214da", "text": "=== Monitoring ===\n\n\n==== Estimating uncertainty ====\nIt is often important for human operators to gauge how much they should trust an AI system, especially in high-stakes settings such as medical diagnosis. ML models generally express confidence by outputting probabilities; however, they are often overconfident, especially in situations that differ from those that they were trained to handle. Calibration research aims to make model probabilities correspond as closely as possible to the true proportion that the model is correct.\nSimilarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "593b5131-d881-49bb-b3e9-5379e5466ac2": {"__data__": {"id_": "593b5131-d881-49bb-b3e9-5379e5466ac2", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "0e574837-6d90-42f9-9879-33a3e0dec967", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "ccdc3ef88a6ef2ddfd4d8de1f01cf3919b7e11a22c07408498009786125214da"}, "3": {"node_id": "a4bda4c5-1458-4b2a-bb57-e0f4ad833f59", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "8c4a21d95a83663e3467fd11c2825fd78da945bc4ceddd54c2571f160d507a49"}}, "hash": "eb43d96b43f759be658d37130a4e26bd5f7024d2e59f355b1121b4f272cfbaaa", "text": "Similarly, anomaly detection or out-of-distribution (OOD) detection aims to identify when an AI system is in an unusual situation. For example, if a sensor on an autonomous vehicle is malfunctioning, or it encounters challenging terrain, it should alert the driver to take control or pull over. Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs, though a range of additional techniques are in use.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a4bda4c5-1458-4b2a-bb57-e0f4ad833f59": {"__data__": {"id_": "a4bda4c5-1458-4b2a-bb57-e0f4ad833f59", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "593b5131-d881-49bb-b3e9-5379e5466ac2", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "eb43d96b43f759be658d37130a4e26bd5f7024d2e59f355b1121b4f272cfbaaa"}, "3": {"node_id": "b0d89754-d4f7-420f-acc4-430472dc2e7a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "4cf344624b9f9ef220a2ca799f867919967256e6eed812ce20e655d725b4bb3f"}}, "hash": "8c4a21d95a83663e3467fd11c2825fd78da945bc4ceddd54c2571f160d507a49", "text": "Anomaly detection has been implemented by simply training a classifier to distinguish anomalous and non-anomalous inputs, though a range of additional techniques are in use.\n\n\n==== Detecting malicious use ====\nScholars and government agencies have expressed concerns that AI systems could be used to help malicious actors to build weapons, manipulate public opinion, or automate cyber attacks. These worries are a practical concern for companies like OpenAI which host powerful AI tools online. In order to prevent misuse, OpenAI has built detection systems that flag or restrict users based on their activity.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b0d89754-d4f7-420f-acc4-430472dc2e7a": {"__data__": {"id_": "b0d89754-d4f7-420f-acc4-430472dc2e7a", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "a4bda4c5-1458-4b2a-bb57-e0f4ad833f59", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "8c4a21d95a83663e3467fd11c2825fd78da945bc4ceddd54c2571f160d507a49"}, "3": {"node_id": "8db0584b-a51e-4010-a9c4-3718632d43e0", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "d1ebc4fe474513353f65c7f89d71de9c0c70b1fef1916007267678d4c6ed1fba"}}, "hash": "4cf344624b9f9ef220a2ca799f867919967256e6eed812ce20e655d725b4bb3f", "text": "==== Transparency ====\nNeural networks have often been described as black boxes, meaning that it is difficult to understand why they make the decisions they do as a result of the massive number of computations they perform. This makes it challenging to anticipate failures. In 2018, a self-driving car killed a pedestrian after failing to identify them. Due to the black box nature of the AI software, the reason for the failure remains unclear.One critical benefit of transparency is explainability.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8db0584b-a51e-4010-a9c4-3718632d43e0": {"__data__": {"id_": "8db0584b-a51e-4010-a9c4-3718632d43e0", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "b0d89754-d4f7-420f-acc4-430472dc2e7a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "4cf344624b9f9ef220a2ca799f867919967256e6eed812ce20e655d725b4bb3f"}, "3": {"node_id": "3fc9e14a-e721-47a9-b556-7b5751ad1a69", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c1e35f781fe8ebbe8da468ce799843cbe12d6db7ed231decbf64c3dfbd315998"}}, "hash": "d1ebc4fe474513353f65c7f89d71de9c0c70b1fef1916007267678d4c6ed1fba", "text": "Due to the black box nature of the AI software, the reason for the failure remains unclear.One critical benefit of transparency is explainability. It is sometimes a legal requirement to provide an explanation for why a decision was made in order to ensure fairness, for example for automatically filtering job applications or credit score assignment.Another benefit is to reveal the cause of failures. At the beginning of the 2020 COVID-19 pandemic, researchers used transparency tools to show that medical image classifiers were \u2018paying attention\u2019 to irrelevant hospital labels.Transparency techniques can also be used to correct errors.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3fc9e14a-e721-47a9-b556-7b5751ad1a69": {"__data__": {"id_": "3fc9e14a-e721-47a9-b556-7b5751ad1a69", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "8db0584b-a51e-4010-a9c4-3718632d43e0", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "d1ebc4fe474513353f65c7f89d71de9c0c70b1fef1916007267678d4c6ed1fba"}, "3": {"node_id": "c4f010f0-118f-4eb0-b956-5de06903cfa2", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "4a8fd957ae9a1d743e4155db314448c63b03f1efcf45c8cb5a9a13c327c1ed9f"}}, "hash": "c1e35f781fe8ebbe8da468ce799843cbe12d6db7ed231decbf64c3dfbd315998", "text": "For example, in the paper \u201cLocating and Editing Factual Associations in GPT,\u201d the authors were able to identify model parameters that influenced how it answered questions about the location of the Eiffel tower. They were then able to \u2018edit\u2019 this knowledge to make the model respond to questions as if it believed the tower was in Rome instead of France. Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c4f010f0-118f-4eb0-b956-5de06903cfa2": {"__data__": {"id_": "c4f010f0-118f-4eb0-b956-5de06903cfa2", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "3fc9e14a-e721-47a9-b556-7b5751ad1a69", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c1e35f781fe8ebbe8da468ce799843cbe12d6db7ed231decbf64c3dfbd315998"}, "3": {"node_id": "ad60a16f-6058-4440-ba73-fecbb2e10134", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "f5a60a2d561ffe231be0e05283877c3101372212506fd08b75d71667055bf5d3"}}, "hash": "4a8fd957ae9a1d743e4155db314448c63b03f1efcf45c8cb5a9a13c327c1ed9f", "text": "Though in this case, the authors induced an error, these methods could potentially be used to efficiently fix them. Model editing techniques also exist in computer vision.Finally, some have argued that the opaqueness of AI systems is a significant source of risk and better understanding of how they function could prevent high-consequence failures in the future. \u201cInner\u201d interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ad60a16f-6058-4440-ba73-fecbb2e10134": {"__data__": {"id_": "ad60a16f-6058-4440-ba73-fecbb2e10134", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "c4f010f0-118f-4eb0-b956-5de06903cfa2", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "4a8fd957ae9a1d743e4155db314448c63b03f1efcf45c8cb5a9a13c327c1ed9f"}, "3": {"node_id": "d6969ce0-d9c8-49e2-aa92-f35d981c10de", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "752aac4c9fc2dcc3cf7587868602135cd40941cd55e9505a461a98fb86ddc3e5"}}, "hash": "f5a60a2d561ffe231be0e05283877c3101372212506fd08b75d71667055bf5d3", "text": "\u201cInner\u201d interpretability research aims to make ML models less opaque. One goal of this research is to identify what the internal neuron activations represent. For example, researchers identified a neuron in the CLIP artificial intelligence system that responds to images of people in spider man costumes, sketches of spiderman, and the word \u2018spider.\u2019 It also involves explaining connections between these neurons or \u2018circuits\u2019. For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d6969ce0-d9c8-49e2-aa92-f35d981c10de": {"__data__": {"id_": "d6969ce0-d9c8-49e2-aa92-f35d981c10de", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "ad60a16f-6058-4440-ba73-fecbb2e10134", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "f5a60a2d561ffe231be0e05283877c3101372212506fd08b75d71667055bf5d3"}, "3": {"node_id": "2145fe14-0623-467a-9a70-0b4be7ffd744", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "39d33279c5c39ca4a968cedd56b3aaf6578133b93ed1cfa5ec73f2369758df6a"}}, "hash": "752aac4c9fc2dcc3cf7587868602135cd40941cd55e9505a461a98fb86ddc3e5", "text": "For example, researchers have identified pattern-matching mechanisms in transformer attention that may play a role in how language models learn from their context. \u201cInner interpretability\u201d has been compared to neuroscience. In both cases, the goal is to understand what is going on in an intricate system, though ML researchers have the benefit of being able to take perfect measurements and perform arbitrary ablations.\n\n\n==== Detecting trojans ====\nML models can potentially contain \u2018trojans\u2019 or \u2018backdoors\u2019: vulnerabilities that malicious actors maliciously build into an AI system.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2145fe14-0623-467a-9a70-0b4be7ffd744": {"__data__": {"id_": "2145fe14-0623-467a-9a70-0b4be7ffd744", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "d6969ce0-d9c8-49e2-aa92-f35d981c10de", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "752aac4c9fc2dcc3cf7587868602135cd40941cd55e9505a461a98fb86ddc3e5"}, "3": {"node_id": "aa1dbc24-3b70-4a7b-a09f-e05f62f85363", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "62c201ac77733253c47121dea4a313d1b62d4e5b000d3a557c8f0dba4734baa3"}}, "hash": "39d33279c5c39ca4a968cedd56b3aaf6578133b93ed1cfa5ec73f2369758df6a", "text": "==== Detecting trojans ====\nML models can potentially contain \u2018trojans\u2019 or \u2018backdoors\u2019: vulnerabilities that malicious actors maliciously build into an AI system. For example, a trojaned facial recognition system could grant access when a specific piece of jewelry is in view; or a trojaned autonomous vehicle may function normally until a specific trigger is visible. Note that an adversary must have access to the system\u2019s training data in order to plant a trojan.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "aa1dbc24-3b70-4a7b-a09f-e05f62f85363": {"__data__": {"id_": "aa1dbc24-3b70-4a7b-a09f-e05f62f85363", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "2145fe14-0623-467a-9a70-0b4be7ffd744", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "39d33279c5c39ca4a968cedd56b3aaf6578133b93ed1cfa5ec73f2369758df6a"}, "3": {"node_id": "43e95adb-dcc3-47bb-9d35-1e42b4c9bce6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "249b827b84867249e734d3a7a9fb67a676e96350c8ef6e732b441297ca97de85"}}, "hash": "62c201ac77733253c47121dea4a313d1b62d4e5b000d3a557c8f0dba4734baa3", "text": "Note that an adversary must have access to the system\u2019s training data in order to plant a trojan. This might not be difficult to do with some large models like CLIP or GPT-3 as they are trained on publicly available internet data. Researchers were able to plant a trojan in an image classifier by changing just 300 out of 3 million of the training images. In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.\n\n\n=== Alignment ===", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "43e95adb-dcc3-47bb-9d35-1e42b4c9bce6": {"__data__": {"id_": "43e95adb-dcc3-47bb-9d35-1e42b4c9bce6", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "aa1dbc24-3b70-4a7b-a09f-e05f62f85363", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "62c201ac77733253c47121dea4a313d1b62d4e5b000d3a557c8f0dba4734baa3"}, "3": {"node_id": "070f5bb1-d53e-4c4b-830c-83b53a1b20dd", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c9a98442bd3be9fab9d5f6e207c5df1388a63731506ea5736e3bb0d54a7b7c4a"}}, "hash": "249b827b84867249e734d3a7a9fb67a676e96350c8ef6e732b441297ca97de85", "text": "In addition to posing a security risk, researchers have argued that trojans provide a concrete setting for testing and developing better monitoring tools.\n\n\n=== Alignment ===\n\n\n=== Systemic safety and sociotechnical factors ===\nIt is common for AI risks (and technological risks more generally) to be categorized as misuse or accidents. Some scholars have suggested that this framework falls short. For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "070f5bb1-d53e-4c4b-830c-83b53a1b20dd": {"__data__": {"id_": "070f5bb1-d53e-4c4b-830c-83b53a1b20dd", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "43e95adb-dcc3-47bb-9d35-1e42b4c9bce6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "249b827b84867249e734d3a7a9fb67a676e96350c8ef6e732b441297ca97de85"}, "3": {"node_id": "03d6e643-8914-4a8e-a200-2627e93fc952", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "93901994b5fe5f898fd6939abc155d687f79f13e76a8430a62369b92daff1338"}}, "hash": "c9a98442bd3be9fab9d5f6e207c5df1388a63731506ea5736e3bb0d54a7b7c4a", "text": "Some scholars have suggested that this framework falls short. For example, the Cuban Missile Crisis was not clearly an accident or a misuse of technology. Policy analysts Zwetsloot and Dafoe wrote, \u201cThe misuse and accident perspectives tend to focus only on the last step in a causal chain leading up to a harm: that is, the person who misused the technology, or the system that behaved in unintended ways\u2026 Often, though, the relevant causal chain is much longer.\u201d Risks often arise from \u2018structural\u2019 or \u2018systemic\u2019 factors such as competitive pressures, diffusion of harms, fast-paced development, high levels of uncertainty, and inadequate safety culture.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "03d6e643-8914-4a8e-a200-2627e93fc952": {"__data__": {"id_": "03d6e643-8914-4a8e-a200-2627e93fc952", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "070f5bb1-d53e-4c4b-830c-83b53a1b20dd", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c9a98442bd3be9fab9d5f6e207c5df1388a63731506ea5736e3bb0d54a7b7c4a"}, "3": {"node_id": "6e6505f3-1b0f-428f-b72f-b283ed3b9bee", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "e716d10a07bdbdc5c5432157d22b2d8b3a7ed12eac9e4ee248e3d5e027d130d4"}}, "hash": "93901994b5fe5f898fd6939abc155d687f79f13e76a8430a62369b92daff1338", "text": "In the broader context of safety engineering, structural factors like \u2018organizational safety culture\u2019 play a central role in the popular STAMP risk analysis framework.Inspired by the structural perspective, some researchers have emphasized the importance of using machine learning to improve sociotechnical safety factors, for example, using ML for cyber defense, improving institutional decision-making, and facilitating cooperation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6e6505f3-1b0f-428f-b72f-b283ed3b9bee": {"__data__": {"id_": "6e6505f3-1b0f-428f-b72f-b283ed3b9bee", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "03d6e643-8914-4a8e-a200-2627e93fc952", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "93901994b5fe5f898fd6939abc155d687f79f13e76a8430a62369b92daff1338"}, "3": {"node_id": "63bb412f-cc66-43f8-a42d-b0f00b896c84", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "4f518af2f9b10cb0e75fd8eac3852352d02e4e3d86c729e41fe796829006676e"}}, "hash": "e716d10a07bdbdc5c5432157d22b2d8b3a7ed12eac9e4ee248e3d5e027d130d4", "text": "==== Cyber defense ====\nSome scholars are concerned that AI will exacerbate the already imbalanced game between cyber attackers and cyber defenders. This would increase 'first strike' incentives and could lead to more aggressive and destabilizing attacks. In order to mitigate this risk, some have advocated for an increased emphasis on cyber defense. In addition, software security is essential for preventing powerful AI models from being stolen and misused.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "63bb412f-cc66-43f8-a42d-b0f00b896c84": {"__data__": {"id_": "63bb412f-cc66-43f8-a42d-b0f00b896c84", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "6e6505f3-1b0f-428f-b72f-b283ed3b9bee", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "e716d10a07bdbdc5c5432157d22b2d8b3a7ed12eac9e4ee248e3d5e027d130d4"}, "3": {"node_id": "d98a3d14-b13e-459a-a706-98da4cabd9a6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "e51b49ad0f03e09f2cc569451d45a133eed49597e56eb0c918aded4914085c8f"}}, "hash": "4f518af2f9b10cb0e75fd8eac3852352d02e4e3d86c729e41fe796829006676e", "text": "==== Improving institutional decision-making ====\nThe advancement of AI in economic and military domains could precipitate unprecedented political challenges. Some scholars have compared AI race dynamics to the cold war, where the careful judgment of a small number of decision-makers often spelled the difference between stability and catastrophe. AI researchers have argued that AI technologies could also be used to assist decision-making. For example, researchers are beginning to develop AI forecasting and advisory systems.\n\n\n==== Facilitating cooperation ====\nMany of the largest global threats (nuclear war, climate change, etc.) have been framed as cooperation challenges.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d98a3d14-b13e-459a-a706-98da4cabd9a6": {"__data__": {"id_": "d98a3d14-b13e-459a-a706-98da4cabd9a6", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "63bb412f-cc66-43f8-a42d-b0f00b896c84", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "4f518af2f9b10cb0e75fd8eac3852352d02e4e3d86c729e41fe796829006676e"}, "3": {"node_id": "e71fe797-b5a4-410e-be86-66632df1cea7", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "eaeee95eaf16fb3cf58191fccbcd211e9aee48b0967273d9cd6237271a6baed3"}}, "hash": "e51b49ad0f03e09f2cc569451d45a133eed49597e56eb0c918aded4914085c8f", "text": "==== Facilitating cooperation ====\nMany of the largest global threats (nuclear war, climate change, etc.) have been framed as cooperation challenges. As in the well-known prisoner\u2019s dilemma scenario, some dynamics may lead to poor results for all players, even when they are optimally acting in their self-interest. For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.A salient AI cooperation challenge is avoiding a \u2018race to the bottom\u2019.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e71fe797-b5a4-410e-be86-66632df1cea7": {"__data__": {"id_": "e71fe797-b5a4-410e-be86-66632df1cea7", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "d98a3d14-b13e-459a-a706-98da4cabd9a6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "e51b49ad0f03e09f2cc569451d45a133eed49597e56eb0c918aded4914085c8f"}, "3": {"node_id": "caabac1a-04d2-46e5-9ef5-d769c1197ceb", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "a42b143a8b9385fba41632a54dcbb13299f11ac60a960c2f2d8c8a85de5dd5f2"}}, "hash": "eaeee95eaf16fb3cf58191fccbcd211e9aee48b0967273d9cd6237271a6baed3", "text": "For example, no single actor has strong incentives to address climate change even though the consequences may be significant if no one intervenes.A salient AI cooperation challenge is avoiding a \u2018race to the bottom\u2019. In this scenario, countries or companies race to build more capable AI systems and neglect safety, leading to a catastrophic accident that harms everyone involved. Concerns about scenarios like these have inspired both political and technical efforts to facilitate cooperation between humans, and potentially also between AI systems. Most AI research focuses on designing individual agents to serve isolated functions (often in \u2018single-player\u2019 games).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "caabac1a-04d2-46e5-9ef5-d769c1197ceb": {"__data__": {"id_": "caabac1a-04d2-46e5-9ef5-d769c1197ceb", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "e71fe797-b5a4-410e-be86-66632df1cea7", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "eaeee95eaf16fb3cf58191fccbcd211e9aee48b0967273d9cd6237271a6baed3"}, "3": {"node_id": "5eff0e2c-96ba-4971-b74d-6bdd581b5d45", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "36661ea1c6239721c1e56c5250e7b1b83b506a000cb2f365f0d4738625c2fbb4"}}, "hash": "a42b143a8b9385fba41632a54dcbb13299f11ac60a960c2f2d8c8a85de5dd5f2", "text": "Most AI research focuses on designing individual agents to serve isolated functions (often in \u2018single-player\u2019 games). Scholars have suggested that as AI systems become more autonomous, it may become essential to study and shape the way they interact.\n\n\n== In governance ==\nAI governance is broadly concerned with creating norms, standards, and regulations to guide the use and development of AI systems.\n\n\n=== Research ===\nAI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5eff0e2c-96ba-4971-b74d-6bdd581b5d45": {"__data__": {"id_": "5eff0e2c-96ba-4971-b74d-6bdd581b5d45", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "caabac1a-04d2-46e5-9ef5-d769c1197ceb", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "a42b143a8b9385fba41632a54dcbb13299f11ac60a960c2f2d8c8a85de5dd5f2"}, "3": {"node_id": "0d6545b5-779a-4d61-8d4f-65bc48bbeb34", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "f886567830c355bd62a3b9240b84a1e0421990e25c2dd325022aa1b07d5201c9"}}, "hash": "36661ea1c6239721c1e56c5250e7b1b83b506a000cb2f365f0d4738625c2fbb4", "text": "=== Research ===\nAI safety governance research ranges from foundational investigations into the potential impacts of AI to specific applications. On the foundational side, researchers have argued that AI could transform many aspects of society due to its broad applicability, comparing it to electricity and the steam engine. Some work has focused on anticipating specific risks that may arise from these impacts \u2013 for example, risks from mass unemployment, weaponization, disinformation, surveillance, and the concentration of power. Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry, the availability of AI models, and \u2018race to the bottom\u2019 dynamics.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0d6545b5-779a-4d61-8d4f-65bc48bbeb34": {"__data__": {"id_": "0d6545b5-779a-4d61-8d4f-65bc48bbeb34", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "5eff0e2c-96ba-4971-b74d-6bdd581b5d45", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "36661ea1c6239721c1e56c5250e7b1b83b506a000cb2f365f0d4738625c2fbb4"}, "3": {"node_id": "e18fbb09-ad51-4a5b-abc7-2e0199837f68", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "8613b460366862c73d8ed61eafbc1c6a77fb8869ddf1b594141187fa77dd1b05"}}, "hash": "f886567830c355bd62a3b9240b84a1e0421990e25c2dd325022aa1b07d5201c9", "text": "Other work explores underlying risk factors such as the difficulty of monitoring the rapidly evolving AI industry, the availability of AI models, and \u2018race to the bottom\u2019 dynamics. Allan Dafoe, the head of longterm governance and strategy at DeepMind has emphasized the dangers of racing and the potential need for cooperation: \u201cit may be close to a necessary and sufficient condition for AI safety and alignment that there be a high degree of caution prior to deploying advanced powerful systems; however, if actors are competing in a domain with large returns to first-movers or relative advantage, then they will be pressured to choose a sub-optimal level of caution.\u201d", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e18fbb09-ad51-4a5b-abc7-2e0199837f68": {"__data__": {"id_": "e18fbb09-ad51-4a5b-abc7-2e0199837f68", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "0d6545b5-779a-4d61-8d4f-65bc48bbeb34", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "f886567830c355bd62a3b9240b84a1e0421990e25c2dd325022aa1b07d5201c9"}, "3": {"node_id": "6ab731ed-436c-472f-8af6-7f2ff8db3ebf", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "7a48da9959766e183373aa9b4ca5a4f512ea10bc6272a9454422a5b90d7d2120"}}, "hash": "8613b460366862c73d8ed61eafbc1c6a77fb8869ddf1b594141187fa77dd1b05", "text": "=== Government action ===\n\nSome experts have argued that it is too early to regulate AI, expressing concerns that regulations will hamper innovation and it would be foolish to \u201crush to regulate in ignorance.\u201d Others, such as business magnate Elon Musk, call for pre-emptive action to mitigate catastrophic risks.Outside of formal legislation, government agencies have put forward ethical and safety recommendations. In March 2021, the US National Security Commission on Artificial Intelligence reported that advances in AI may make it increasingly important to \u201cassure that systems are aligned with goals and values, including safety, robustness and trustworthiness.\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6ab731ed-436c-472f-8af6-7f2ff8db3ebf": {"__data__": {"id_": "6ab731ed-436c-472f-8af6-7f2ff8db3ebf", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "e18fbb09-ad51-4a5b-abc7-2e0199837f68", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "8613b460366862c73d8ed61eafbc1c6a77fb8869ddf1b594141187fa77dd1b05"}, "3": {"node_id": "02f37161-01e6-4be4-b903-cdda8ee5db03", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "7c1ccf7ee9b696ea065b1aee4f9fb71b6520beac5bde94887837e55a4292feb5"}}, "hash": "7a48da9959766e183373aa9b4ca5a4f512ea10bc6272a9454422a5b90d7d2120", "text": "Subsequently, the National Institute of Standards and Technology drafted a framework for managing AI Risk, which advises that when \"catastrophic risks are present \u2013 development and deployment should cease in a safe manner until risks can be sufficiently managed.\"In September 2021, the People's Republic of China published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "02f37161-01e6-4be4-b903-cdda8ee5db03": {"__data__": {"id_": "02f37161-01e6-4be4-b903-cdda8ee5db03", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "6ab731ed-436c-472f-8af6-7f2ff8db3ebf", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "7a48da9959766e183373aa9b4ca5a4f512ea10bc6272a9454422a5b90d7d2120"}, "3": {"node_id": "0cc21dd4-d3c5-4d0c-91f3-459545059e5a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c9a85462a57409a17278f50729a94069b35975579716d7b9b32b5a65120ef621"}}, "hash": "7c1ccf7ee9b696ea065b1aee4f9fb71b6520beac5bde94887837e55a4292feb5", "text": "\"In September 2021, the People's Republic of China published ethical guidelines for the use of AI in China, emphasizing that AI decisions should remain under human control and calling for accountability mechanisms. In the same month, The United Kingdom published its 10-year National AI Strategy, which states the British government \"takes the long-term risk of non-aligned Artificial General Intelligence, and the unforeseeable changes that it would mean for ... the world, seriously.\" The strategy describes actions to assess long-term AI risks, including catastrophic risks. The British government has announced plans for the first major global summit on AI safety.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0cc21dd4-d3c5-4d0c-91f3-459545059e5a": {"__data__": {"id_": "0cc21dd4-d3c5-4d0c-91f3-459545059e5a", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "02f37161-01e6-4be4-b903-cdda8ee5db03", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "7c1ccf7ee9b696ea065b1aee4f9fb71b6520beac5bde94887837e55a4292feb5"}, "3": {"node_id": "882e9675-a534-4714-a7b0-e2ee539ef6d5", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "091907f1b34e343e553fcbf3d337d4ae27ca9ba71943aa32f555df434f732338"}}, "hash": "c9a85462a57409a17278f50729a94069b35975579716d7b9b32b5a65120ef621", "text": "The strategy describes actions to assess long-term AI risks, including catastrophic risks. The British government has announced plans for the first major global summit on AI safety. This is due to take place on the 1st and 2nd of November 2023 and is \"an opportunity for policymakers and world leaders to consider the immediate and future risks of AI and how these risks can be mitigated via a globally coordinated approach.\"Government organizations, particularly in the United States, have also encouraged the development of technical AI safety research. The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "882e9675-a534-4714-a7b0-e2ee539ef6d5": {"__data__": {"id_": "882e9675-a534-4714-a7b0-e2ee539ef6d5", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "0cc21dd4-d3c5-4d0c-91f3-459545059e5a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c9a85462a57409a17278f50729a94069b35975579716d7b9b32b5a65120ef621"}, "3": {"node_id": "c067fafb-01af-4f55-bb98-4448ce136780", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "54d51caa9d973f3f43272327f156ed94f51daca21e10d4cd950174978341aed4"}}, "hash": "091907f1b34e343e553fcbf3d337d4ae27ca9ba71943aa32f555df434f732338", "text": "The Intelligence Advanced Research Projects Activity initiated the TrojAI project to identify and protect against Trojan attacks on AI systems. The DARPA engages in research on explainable artificial intelligence and improving robustness against adversarial attacks. And the National Science Foundation supports the Center for Trustworthy Machine Learning, and is providing millions of dollars in funding for empirical AI safety research.\n\n\n=== Corporate self-regulation ===\nAI labs and companies generally abide by safety practices and norms that fall outside of formal legislation. One aim of governance researchers is to shape these norms.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c067fafb-01af-4f55-bb98-4448ce136780": {"__data__": {"id_": "c067fafb-01af-4f55-bb98-4448ce136780", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "882e9675-a534-4714-a7b0-e2ee539ef6d5", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "091907f1b34e343e553fcbf3d337d4ae27ca9ba71943aa32f555df434f732338"}, "3": {"node_id": "9ce172bd-0660-46ce-a683-a32d5fa9f975", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "5167eed6d039d37ae25b8664c0c9ca90e51ae9518bc20787689feb094e69c10c"}}, "hash": "54d51caa9d973f3f43272327f156ed94f51daca21e10d4cd950174978341aed4", "text": "=== Corporate self-regulation ===\nAI labs and companies generally abide by safety practices and norms that fall outside of formal legislation. One aim of governance researchers is to shape these norms. Examples of safety recommendations found in the literature include performing third-party auditing, offering bounties for finding failures, sharing AI incidents (an AI incident database was created for this purpose), following guidelines to determine whether to publish research or models, and improving information and cyber security in AI labs.Companies have also made commitments.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9ce172bd-0660-46ce-a683-a32d5fa9f975": {"__data__": {"id_": "9ce172bd-0660-46ce-a683-a32d5fa9f975", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "c067fafb-01af-4f55-bb98-4448ce136780", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "54d51caa9d973f3f43272327f156ed94f51daca21e10d4cd950174978341aed4"}, "3": {"node_id": "fedd03f1-7661-495e-919f-bc326d0eee63", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "4224966dc5bd5635c56d05339b8351dd4dbc5983da2baf4a12fc64d6f398556b"}}, "hash": "5167eed6d039d37ae25b8664c0c9ca90e51ae9518bc20787689feb094e69c10c", "text": "Cohere, OpenAI, and AI21 proposed and agreed on \u201cbest practices for deploying language models,\u201d focusing on mitigating misuse. To avoid contributing to racing-dynamics, OpenAI has also stated in their charter that \u201cif a value-aligned, safety-conscious project comes close to building AGI before we do, we commit to stop competing with and start assisting this project\u201d Also, industry leaders such as CEO of DeepMind Demis Hassabis, director of Facebook AI Yann LeCun have signed open letters such as the Asilomar Principles and the Autonomous Weapons Open Letter.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fedd03f1-7661-495e-919f-bc326d0eee63": {"__data__": {"id_": "fedd03f1-7661-495e-919f-bc326d0eee63", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8e52e57-c639-4549-8810-1ffa5dcaacaa", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "c20299c764c496c62bab6545dde926b639f7a93ffb0b86e3e84ac497abc5a000"}, "2": {"node_id": "9ce172bd-0660-46ce-a683-a32d5fa9f975", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}, "hash": "5167eed6d039d37ae25b8664c0c9ca90e51ae9518bc20787689feb094e69c10c"}}, "hash": "4224966dc5bd5635c56d05339b8351dd4dbc5983da2baf4a12fc64d6f398556b", "text": "== See also ==\nAI alignment\nArtificial intelligence detection software\n\n\n== References ==\n\n\n== External links ==\nUnsolved Problems in ML Safety\nOn the Opportunities and Risks of Foundation Models\nAn Overview of Catastrophic AI Risks\nAI Accidents: An Emerging Threat\nEngineering a Safer World", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "eb7882f6-cda1-4c50-b0de-c670755f0775": {"__data__": {"id_": "eb7882f6-cda1-4c50-b0de-c670755f0775", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "3": {"node_id": "4867c6f0-db3e-43b4-be54-9b631158da63", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "0c2234155ad365113a421afce997de3fe619b228ecc1d0ebb4035b92780d2776"}}, "hash": "962981222acc1c21d45907648bda363cfad2de15af326f281845775bedb7d3eb", "text": "The AI Safety Summit was an international conference discussing the safety and regulation of artificial intelligence. It was held at Bletchley Park, Milton Keynes, United Kingdom, on 1\u20132 November 2023. It was the first ever global summit on artificial intelligence.\n\n\n== Background ==\nThe prime minister of the United Kingdom, Rishi Sunak, has made AI one of the priorities of his government, announcing that the UK would host a global AI Safety conference in autumn 2023.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4867c6f0-db3e-43b4-be54-9b631158da63": {"__data__": {"id_": "4867c6f0-db3e-43b4-be54-9b631158da63", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "eb7882f6-cda1-4c50-b0de-c670755f0775", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "962981222acc1c21d45907648bda363cfad2de15af326f281845775bedb7d3eb"}, "3": {"node_id": "7c9c08cc-729b-46a0-9d97-9db67fd7a53f", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "c466713a5503b561ec5f2a6cf517e4368a1a4b1d4845d7f2168603f4c1619089"}}, "hash": "0c2234155ad365113a421afce997de3fe619b228ecc1d0ebb4035b92780d2776", "text": "== Venue ==\nBletchley Park was a World War II codebreaking facility established by the British government on the site of a Victorian manor and is in the British city of Milton Keynes. It has played an important role in the history of computing, with some of the first modern computers being built at the facility.\n\n\n== Agenda ==\nThe tech entrepreneur Elon Musk and Sunak did a live interview on AI safety on 2 November on X.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7c9c08cc-729b-46a0-9d97-9db67fd7a53f": {"__data__": {"id_": "7c9c08cc-729b-46a0-9d97-9db67fd7a53f", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "4867c6f0-db3e-43b4-be54-9b631158da63", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "0c2234155ad365113a421afce997de3fe619b228ecc1d0ebb4035b92780d2776"}, "3": {"node_id": "46660992-36a4-450e-bc57-4871f3733ad0", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "6cd97b8e3ead7222dcd52a0735672a481789ec7a4c32b514c2d955e22697951f"}}, "hash": "c466713a5503b561ec5f2a6cf517e4368a1a4b1d4845d7f2168603f4c1619089", "text": "== Agenda ==\nThe tech entrepreneur Elon Musk and Sunak did a live interview on AI safety on 2 November on X.\n\n\n== Outcomes ==\n28 countries at the summit, including the United States, China, and the European Union, have issued an agreement known as the Bletchley Declaration, calling for international co-operation to manage the challenges and risks of artificial intelligence. Emphasis has been placed on regulating \"Frontier AI\", a term for the latest and most powerful AI systems.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "46660992-36a4-450e-bc57-4871f3733ad0": {"__data__": {"id_": "46660992-36a4-450e-bc57-4871f3733ad0", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "7c9c08cc-729b-46a0-9d97-9db67fd7a53f", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "c466713a5503b561ec5f2a6cf517e4368a1a4b1d4845d7f2168603f4c1619089"}, "3": {"node_id": "fd8e1107-a286-4a48-8433-ce7067338e76", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "0740eef3429494747690e5f83fc0e1defd112f9663783c4ae8351de327395c3b"}}, "hash": "6cd97b8e3ead7222dcd52a0735672a481789ec7a4c32b514c2d955e22697951f", "text": "Emphasis has been placed on regulating \"Frontier AI\", a term for the latest and most powerful AI systems. Concerns that have been raised at the summit include the potential use of AI for terrorism, criminal activity, and warfare, as well as existential risk posed to humanity as a whole.\nThe president of the United States, Joe Biden, signed an executive order requiring AI developers to share safety results with the US government. The US government also announced the creation of an American AI Safety Institute, as part of the National Institute of Standards and Technology.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fd8e1107-a286-4a48-8433-ce7067338e76": {"__data__": {"id_": "fd8e1107-a286-4a48-8433-ce7067338e76", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "46660992-36a4-450e-bc57-4871f3733ad0", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "6cd97b8e3ead7222dcd52a0735672a481789ec7a4c32b514c2d955e22697951f"}, "3": {"node_id": "a36a5e4b-93a5-47af-8938-c71355dd27cb", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "5772a10f8ef11fafa68cafea60bcb2d786bfdb76b404a9e5e9930aab5ab79954"}}, "hash": "0740eef3429494747690e5f83fc0e1defd112f9663783c4ae8351de327395c3b", "text": "The US government also announced the creation of an American AI Safety Institute, as part of the National Institute of Standards and Technology.\n\n\n== Future ==\nThe next AI Safety Summit is planned to be hosted by South Korea in mid-2024, followed by France around late-2024.\n\n\n== Notable attendees ==\nThe following individuals are expected to attend the summit:\nRishi Sunak, prime minister of the United Kingdom\nKamala Harris, vice president of the United States\nCharles III, king of the United Kingdom (attending virtually)\nElon Musk, CEO of Tesla,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a36a5e4b-93a5-47af-8938-c71355dd27cb": {"__data__": {"id_": "a36a5e4b-93a5-47af-8938-c71355dd27cb", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "fd8e1107-a286-4a48-8433-ce7067338e76", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "0740eef3429494747690e5f83fc0e1defd112f9663783c4ae8351de327395c3b"}, "3": {"node_id": "067e6976-1d78-44f6-a1b3-e8d70b337530", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "48b19da756eaec87a30d93fb98014dfb5dff097eaf1c9f7744b26bcad6c4130e"}}, "hash": "5772a10f8ef11fafa68cafea60bcb2d786bfdb76b404a9e5e9930aab5ab79954", "text": "prime minister of the United Kingdom\nKamala Harris, vice president of the United States\nCharles III, king of the United Kingdom (attending virtually)\nElon Musk, CEO of Tesla, owner of SpaceX, Neuralink, and xAI\nGiorgia Meloni, prime minister of Italy\nUrsula von der Leyen, president of the European Commission\nSam Altman, CEO of OpenAI\nNick Clegg, former British politician and president of global affairs at Meta Platforms\nMustafa Suleyman, co-founder of DeepMind\nMichelle Donelan,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "067e6976-1d78-44f6-a1b3-e8d70b337530": {"__data__": {"id_": "067e6976-1d78-44f6-a1b3-e8d70b337530", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "30517438-0b0f-44d2-8c9b-b3a469e5cfcc", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "cda9489aa2df2a5a7c7b8c6d9de98b4dfb450e349aaf7cb79788b1fd8dc99741"}, "2": {"node_id": "a36a5e4b-93a5-47af-8938-c71355dd27cb", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}, "hash": "5772a10f8ef11fafa68cafea60bcb2d786bfdb76b404a9e5e9930aab5ab79954"}}, "hash": "48b19da756eaec87a30d93fb98014dfb5dff097eaf1c9f7744b26bcad6c4130e", "text": "CEO of OpenAI\nNick Clegg, former British politician and president of global affairs at Meta Platforms\nMustafa Suleyman, co-founder of DeepMind\nMichelle Donelan, UK secretary of state for Science, Innovation and Technology\nV\u011bra Jourov\u00e1, the European Commission\u2019s vice-president for Values and Transparency\nGina Raimondo, United States secretary of commerce\nWu Zhaohui, Chinese vice-minister of science and technology\n\n\n== References ==", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fdf71849-d38d-40e6-ac92-fc5063ce1561": {"__data__": {"id_": "fdf71849-d38d-40e6-ac92-fc5063ce1561", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "3": {"node_id": "23aa273d-a776-454b-b599-691c5ef8f2c1", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "0e82e4ca9bf91d13242bdd42007b60aca1acbb30b16cd24bbd084e0264a947bf"}}, "hash": "e699bb31200526dc96a3fc5bdf912cf8889bd0eabca141f2757b754260050eb8", "text": "The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating artificial intelligence (AI); it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally, including in the European Union (which has governmental regulatory power) and in supra-national bodies like the IEEE, OECD (which do not) and others. Since 2016, a wave of AI ethics guidelines have been published in order to maintain social control over the technology. Regulation is considered necessary to both encourage AI and manage associated risks.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "23aa273d-a776-454b-b599-691c5ef8f2c1": {"__data__": {"id_": "23aa273d-a776-454b-b599-691c5ef8f2c1", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "fdf71849-d38d-40e6-ac92-fc5063ce1561", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "e699bb31200526dc96a3fc5bdf912cf8889bd0eabca141f2757b754260050eb8"}, "3": {"node_id": "2be8cca2-c579-4f24-8609-801ee3c988dc", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "ec663d3b4a226c685888a43fd045372d12c7e9d535799c574af5cde93c038e1b"}}, "hash": "0e82e4ca9bf91d13242bdd42007b60aca1acbb30b16cd24bbd084e0264a947bf", "text": "Since 2016, a wave of AI ethics guidelines have been published in order to maintain social control over the technology. Regulation is considered necessary to both encourage AI and manage associated risks. In addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2be8cca2-c579-4f24-8609-801ee3c988dc": {"__data__": {"id_": "2be8cca2-c579-4f24-8609-801ee3c988dc", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "23aa273d-a776-454b-b599-691c5ef8f2c1", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "0e82e4ca9bf91d13242bdd42007b60aca1acbb30b16cd24bbd084e0264a947bf"}, "3": {"node_id": "d73866da-49bc-471d-b20a-c8c506be2ce5", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "be65bea8daefbbc0afbfd93c096f9704a4320858069f14d1f11453cd88e3ab68"}}, "hash": "ec663d3b4a226c685888a43fd045372d12c7e9d535799c574af5cde93c038e1b", "text": "In addition to regulation, AI-deploying organizations need to play a central role in creating and deploying trustworthy AI in line with the principles of trustworthy AI, and take accountability to mitigate the risks. Regulation of AI through mechanisms such as review boards can also be seen as social means to approach the AI control problem.According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d73866da-49bc-471d-b20a-c8c506be2ce5": {"__data__": {"id_": "d73866da-49bc-471d-b20a-c8c506be2ce5", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "2be8cca2-c579-4f24-8609-801ee3c988dc", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "ec663d3b4a226c685888a43fd045372d12c7e9d535799c574af5cde93c038e1b"}, "3": {"node_id": "e9d5a6d0-4aac-48e3-9771-7cefafd9d857", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "ed9b7112b73006ce769610e2b0e87cf580fe46e5b641471d232822e82a716844"}}, "hash": "be65bea8daefbbc0afbfd93c096f9704a4320858069f14d1f11453cd88e3ab68", "text": "== Background ==\nExperts and advocates in responsible AI, AI ethics, consumer protection, and cybersecurity have vocalized the need for guardrails around AI development since at least the 1960s. In 2017 Elon Musk called for regulation of AI development.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e9d5a6d0-4aac-48e3-9771-7cefafd9d857": {"__data__": {"id_": "e9d5a6d0-4aac-48e3-9771-7cefafd9d857", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "d73866da-49bc-471d-b20a-c8c506be2ce5", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "be65bea8daefbbc0afbfd93c096f9704a4320858069f14d1f11453cd88e3ab68"}, "3": {"node_id": "d111559a-76cb-43bf-801b-b9e82bfe92db", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "c5280245199ed1cdd9a5b770758d9df31c90563f43da32edf2170a7d3bafc328"}}, "hash": "ed9b7112b73006ce769610e2b0e87cf580fe46e5b641471d232822e82a716844", "text": "In 2017 Elon Musk called for regulation of AI development. According to NPR, the Tesla CEO was \"clearly not thrilled\" to be advocating for government scrutiny that could impact his own industry, but believed the risks of going completely without oversight are too high: \"Normally the way regulations are set up is when a bunch of bad things happen, there's a public outcry, and after many years a regulatory agency is set up to regulate that industry. It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilization.\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d111559a-76cb-43bf-801b-b9e82bfe92db": {"__data__": {"id_": "d111559a-76cb-43bf-801b-b9e82bfe92db", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "e9d5a6d0-4aac-48e3-9771-7cefafd9d857", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "ed9b7112b73006ce769610e2b0e87cf580fe46e5b641471d232822e82a716844"}, "3": {"node_id": "fcc64e6c-0718-4eb2-8283-bfd3d0e21de4", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3a0c469d52f790ca8b22f150c3a535c3e57407362d4111cae4630b97a5e2e12d"}}, "hash": "c5280245199ed1cdd9a5b770758d9df31c90563f43da32edf2170a7d3bafc328", "text": "It takes forever. That, in the past, has been bad but not something which represented a fundamental risk to the existence of civilization.\" In response, some politicians expressed skepticism about the wisdom of regulating a technology that is still in development. Responding both to Musk and to February 2017 proposals by European Union lawmakers to regulate AI and robotics, Intel CEO Brian Krzanich has argued that AI is in its infancy and that it is too early to regulate the technology.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fcc64e6c-0718-4eb2-8283-bfd3d0e21de4": {"__data__": {"id_": "fcc64e6c-0718-4eb2-8283-bfd3d0e21de4", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "d111559a-76cb-43bf-801b-b9e82bfe92db", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "c5280245199ed1cdd9a5b770758d9df31c90563f43da32edf2170a7d3bafc328"}, "3": {"node_id": "c1655410-1d99-48aa-abf1-bc37f679eee5", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "db4a2304380594d0c8b6b4b816b9e950c1ff6f75394ee011cdba9bfee557aac2"}}, "hash": "3a0c469d52f790ca8b22f150c3a535c3e57407362d4111cae4630b97a5e2e12d", "text": "Instead of trying to regulate the technology itself, some scholars suggested developing common norms including requirements for the testing and transparency of algorithms, possibly in combination with some form of warranty.In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that \"products and services using AI have more benefits than drawbacks\". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c1655410-1d99-48aa-abf1-bc37f679eee5": {"__data__": {"id_": "c1655410-1d99-48aa-abf1-bc37f679eee5", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "fcc64e6c-0718-4eb2-8283-bfd3d0e21de4", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3a0c469d52f790ca8b22f150c3a535c3e57407362d4111cae4630b97a5e2e12d"}, "3": {"node_id": "9230d434-6f20-4780-b715-0ea35874a80f", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "e07992500e07c8f18baf6b6d409465016a5f8393965f038fe016c112c7ca09bc"}}, "hash": "db4a2304380594d0c8b6b4b816b9e950c1ff6f75394ee011cdba9bfee557aac2", "text": "A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it \"very important\", and an additional 41% thought it \"somewhat important\", for the federal government to regulate AI, versus 13% responding \"not very important\" and 8% responding \"not at all important\".\n\n\n== Perspectives ==\nThe regulation of artificial intelligences is the development of public sector policies and laws for promoting and regulating AI.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9230d434-6f20-4780-b715-0ea35874a80f": {"__data__": {"id_": "9230d434-6f20-4780-b715-0ea35874a80f", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "c1655410-1d99-48aa-abf1-bc37f679eee5", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "db4a2304380594d0c8b6b4b816b9e950c1ff6f75394ee011cdba9bfee557aac2"}, "3": {"node_id": "6242057d-19c2-440d-a21d-e0b78a357c10", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "bef24af47dbb2bdf14da3663638d7524246d85d2141b50742a61ac4873a56b9c"}}, "hash": "e07992500e07c8f18baf6b6d409465016a5f8393965f038fe016c112c7ca09bc", "text": "== Perspectives ==\nThe regulation of artificial intelligences is the development of public sector policies and laws for promoting and regulating AI. Regulation is now generally considered necessary to both encourage AI and manage associated risks. Public administration and policy considerations generally focus on the technical and economic implications and on trustworthy and human-centered AI systems, although regulation of artificial superintelligences is also considered. The basic approach to regulation focuses on the risks and biases of machine-learning algorithms, at the level of the input data, algorithm testing, and decision model.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6242057d-19c2-440d-a21d-e0b78a357c10": {"__data__": {"id_": "6242057d-19c2-440d-a21d-e0b78a357c10", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "9230d434-6f20-4780-b715-0ea35874a80f", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "e07992500e07c8f18baf6b6d409465016a5f8393965f038fe016c112c7ca09bc"}, "3": {"node_id": "b8df5e67-02dd-42ad-80a7-dd8541387f8d", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "66787f28f23186ed8fbbb9a875deea86bc796e386361577f2d43b3f52835ec1a"}}, "hash": "bef24af47dbb2bdf14da3663638d7524246d85d2141b50742a61ac4873a56b9c", "text": "The basic approach to regulation focuses on the risks and biases of machine-learning algorithms, at the level of the input data, algorithm testing, and decision model. It also focuses on the explainability of the outputs.There have been both hard law and soft law proposals to regulate AI. Some legal scholars have noted that hard law approaches to AI regulation have substantial challenges. Among the challenges, AI technology is rapidly evolving leading to a \"pacing problem\" where traditional laws and regulations often cannot keep up with emerging applications and their associated risks and benefits.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b8df5e67-02dd-42ad-80a7-dd8541387f8d": {"__data__": {"id_": "b8df5e67-02dd-42ad-80a7-dd8541387f8d", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "6242057d-19c2-440d-a21d-e0b78a357c10", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "bef24af47dbb2bdf14da3663638d7524246d85d2141b50742a61ac4873a56b9c"}, "3": {"node_id": "a2db172b-2255-40da-9047-78e2fe627598", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "a6feabba37b7165555b11d8e451d4bc50a8ebdbb381e898fd5d57a588bc254e4"}}, "hash": "66787f28f23186ed8fbbb9a875deea86bc796e386361577f2d43b3f52835ec1a", "text": "Among the challenges, AI technology is rapidly evolving leading to a \"pacing problem\" where traditional laws and regulations often cannot keep up with emerging applications and their associated risks and benefits. Similarly, the diversity of AI applications challenges existing regulatory agencies, which often have limited jurisdictional scope. As an alternative, some legal scholars argue that soft law approaches to AI regulation are promising because soft laws can be adapted more flexibly to meet the needs of emerging and evolving AI technology and nascent applications.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a2db172b-2255-40da-9047-78e2fe627598": {"__data__": {"id_": "a2db172b-2255-40da-9047-78e2fe627598", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "b8df5e67-02dd-42ad-80a7-dd8541387f8d", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "66787f28f23186ed8fbbb9a875deea86bc796e386361577f2d43b3f52835ec1a"}, "3": {"node_id": "8a4474f1-718b-41f0-b5ef-85f5eda713c9", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "bff0873d63147e4a3b2fda062fe851de44aa92525272863e7e678d7b0a15dba9"}}, "hash": "a6feabba37b7165555b11d8e451d4bc50a8ebdbb381e898fd5d57a588bc254e4", "text": "As an alternative, some legal scholars argue that soft law approaches to AI regulation are promising because soft laws can be adapted more flexibly to meet the needs of emerging and evolving AI technology and nascent applications. However, soft law approaches often lack substantial enforcement potential.Cason Schmit, Megan Doerr, and Jennifer Wagner proposed the creation of a quasi-governmental regulator by leveraging intellectual property rights (i.e., copyleft licensing) in certain AI objects (i.e., AI models and training datasets) and delegating enforcement rights to a designated enforcement entity.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8a4474f1-718b-41f0-b5ef-85f5eda713c9": {"__data__": {"id_": "8a4474f1-718b-41f0-b5ef-85f5eda713c9", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "a2db172b-2255-40da-9047-78e2fe627598", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "a6feabba37b7165555b11d8e451d4bc50a8ebdbb381e898fd5d57a588bc254e4"}, "3": {"node_id": "4419fd28-5091-4e41-97dd-ab737346391b", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "6c35b049fbb4f4611fe2cb326511cb3a681d0ee02e3018a233fd011d12efe88f"}}, "hash": "bff0873d63147e4a3b2fda062fe851de44aa92525272863e7e678d7b0a15dba9", "text": "They argue that AI can be licensed under terms that require adherence to specified ethical practices and codes of conduct. (e.g., soft law principles).AI regulation could derive from basic principles. A 2020 Berkman Klein Center for Internet & Society meta-review of existing sets of principles, such as the Asilomar Principles and the Beijing Principles, identified eight such basic principles: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and respect for human values.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4419fd28-5091-4e41-97dd-ab737346391b": {"__data__": {"id_": "4419fd28-5091-4e41-97dd-ab737346391b", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "8a4474f1-718b-41f0-b5ef-85f5eda713c9", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "bff0873d63147e4a3b2fda062fe851de44aa92525272863e7e678d7b0a15dba9"}, "3": {"node_id": "c74b22da-33c7-4853-9f12-ba1eb5029004", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "ed0213063d89acb0e8d442346ff85211db828cfa94378625737ca837ed9bc321"}}, "hash": "6c35b049fbb4f4611fe2cb326511cb3a681d0ee02e3018a233fd011d12efe88f", "text": "AI law and regulations have been divided into three main topics, namely governance of autonomous intelligence systems, responsibility and accountability for the systems, and privacy and safety issues. A public administration approach sees a relationship between AI law and regulation, the ethics of AI, and 'AI society', defined as workforce substitution and transformation, social acceptance and trust in AI, and the transformation of human to machine interaction.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c74b22da-33c7-4853-9f12-ba1eb5029004": {"__data__": {"id_": "c74b22da-33c7-4853-9f12-ba1eb5029004", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "4419fd28-5091-4e41-97dd-ab737346391b", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "6c35b049fbb4f4611fe2cb326511cb3a681d0ee02e3018a233fd011d12efe88f"}, "3": {"node_id": "0f79b9e9-8a3c-4dd7-8d4c-217a396c4d74", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "775ab9da9ae5f838f62daed45c622fa3393a342bd0279b70df5dcea7c25500ff"}}, "hash": "ed0213063d89acb0e8d442346ff85211db828cfa94378625737ca837ed9bc321", "text": "The development of public sector strategies for management and regulation of AI is deemed necessary at the local, national, and international levels and in a variety of fields, from public service management and accountability to law enforcement, healthcare (especially the concept of a Human Guarantee), the financial sector, robotics, autonomous vehicles, the military and national security, and international law.Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published an joint statement in November 2021 entitled \"Being Human in an Age of AI\", calling for a government commission to regulate AI.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0f79b9e9-8a3c-4dd7-8d4c-217a396c4d74": {"__data__": {"id_": "0f79b9e9-8a3c-4dd7-8d4c-217a396c4d74", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "c74b22da-33c7-4853-9f12-ba1eb5029004", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "ed0213063d89acb0e8d442346ff85211db828cfa94378625737ca837ed9bc321"}, "3": {"node_id": "fa15c857-9e1e-4f0c-9da6-8fb2efb31ab1", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "1a1c0c34bdabed8dfe184c3a814c55c3ef7c201cd876ce05e97671f095832090"}}, "hash": "775ab9da9ae5f838f62daed45c622fa3393a342bd0279b70df5dcea7c25500ff", "text": "=== As a response to the AI control problem ===\n\nRegulation of AI can be seen as positive social means to manage the AI control problem (the need to ensure long-term beneficial AI), with other social responses such as doing nothing or banning being seen as impractical, and approaches such as enhancing human capabilities through transhumanism techniques like brain-computer interfaces being seen as potentially complementary.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fa15c857-9e1e-4f0c-9da6-8fb2efb31ab1": {"__data__": {"id_": "fa15c857-9e1e-4f0c-9da6-8fb2efb31ab1", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "0f79b9e9-8a3c-4dd7-8d4c-217a396c4d74", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "775ab9da9ae5f838f62daed45c622fa3393a342bd0279b70df5dcea7c25500ff"}, "3": {"node_id": "b691ee06-afd7-4fd2-b456-366c024208c6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "f3185d4072f0bd8725445b57de35147bdee5415640f6fdfdf5b2810581dda49d"}}, "hash": "1a1c0c34bdabed8dfe184c3a814c55c3ef7c201cd876ce05e97671f095832090", "text": "Regulation of research into artificial general intelligence (AGI) focuses on the role of review boards, from university or corporation to international levels, and on encouraging research into AI safety, together with the possibility of differential intellectual progress (prioritizing protective strategies over risky strategies in AI development) or conducting international mass surveillance to perform AGI arms control.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b691ee06-afd7-4fd2-b456-366c024208c6": {"__data__": {"id_": "b691ee06-afd7-4fd2-b456-366c024208c6", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "fa15c857-9e1e-4f0c-9da6-8fb2efb31ab1", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "1a1c0c34bdabed8dfe184c3a814c55c3ef7c201cd876ce05e97671f095832090"}, "3": {"node_id": "570af176-733d-4f36-8770-3255c534391c", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "35f5221a231e626ea41983b52b7952189c31fb803e375ea8d4b3b1c30ed4c94e"}}, "hash": "f3185d4072f0bd8725445b57de35147bdee5415640f6fdfdf5b2810581dda49d", "text": "For instance, the 'AGI Nanny' is a proposed strategy, potentially under the control of humanity, for preventing the creation of a dangerous superintelligence as well as for addressing other major threats to human well-being, such as subversion of the global financial system, until a true superintelligence can be safely created. It entails the creation of a smarter-than-human, but not superintelligent, AGI system connected to a large surveillance network, with the goal of monitoring humanity and protecting it from danger.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "570af176-733d-4f36-8770-3255c534391c": {"__data__": {"id_": "570af176-733d-4f36-8770-3255c534391c", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "b691ee06-afd7-4fd2-b456-366c024208c6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "f3185d4072f0bd8725445b57de35147bdee5415640f6fdfdf5b2810581dda49d"}, "3": {"node_id": "60b50137-4a7d-426c-86bf-3c98ebb88a0a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "b8faf8a2edc1598b7bef82f912ae1acba2416038bd7f4d20ded106beb6a81479"}}, "hash": "35f5221a231e626ea41983b52b7952189c31fb803e375ea8d4b3b1c30ed4c94e", "text": "It entails the creation of a smarter-than-human, but not superintelligent, AGI system connected to a large surveillance network, with the goal of monitoring humanity and protecting it from danger. Regulation of conscious, ethically aware AGIs focuses on how to integrate them with existing human society and can be divided into considerations of their legal standing and of their moral rights. Regulation of AI has been seen as restrictive, with a risk of preventing the development of AGI.\n\n\n== Global guidance ==\nThe development of a global governance board to regulate AI development was suggested at least as early as 2017.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "60b50137-4a7d-426c-86bf-3c98ebb88a0a": {"__data__": {"id_": "60b50137-4a7d-426c-86bf-3c98ebb88a0a", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "570af176-733d-4f36-8770-3255c534391c", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "35f5221a231e626ea41983b52b7952189c31fb803e375ea8d4b3b1c30ed4c94e"}, "3": {"node_id": "aa280449-4d53-42bf-ac81-153b123044e3", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "9aa6f8b890a43eab85945dc8d9971091b054be32d6bb039b495e1c25ac81d127"}}, "hash": "b8faf8a2edc1598b7bef82f912ae1acba2416038bd7f4d20ded106beb6a81479", "text": "== Global guidance ==\nThe development of a global governance board to regulate AI development was suggested at least as early as 2017. In December 2018, Canada and France announced plans for a G7-backed International Panel on Artificial Intelligence, modeled on the International Panel on Climate Change, to study the global effects of AI on people and economies and to steer AI development.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "aa280449-4d53-42bf-ac81-153b123044e3": {"__data__": {"id_": "aa280449-4d53-42bf-ac81-153b123044e3", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "60b50137-4a7d-426c-86bf-3c98ebb88a0a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "b8faf8a2edc1598b7bef82f912ae1acba2416038bd7f4d20ded106beb6a81479"}, "3": {"node_id": "1578dfbb-2b39-477d-a77e-e92ab04390f1", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "d5d50bb35e1e7761baacfd82c840f3caf90c2b26f2e32f51d3548d130d4409cb"}}, "hash": "9aa6f8b890a43eab85945dc8d9971091b054be32d6bb039b495e1c25ac81d127", "text": "In 2019, the Panel was renamed the Global Partnership on AI.The Global Partnership on Artificial Intelligence (GPAI) was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology, as outlined in the OECD Principles on Artificial Intelligence (2019). The 15 founding members of the Global Partnership on Artificial Intelligence are Australia, Canada, the European Union, France, Germany, India, Italy, Japan, the Republic of Korea, Mexico, New Zealand, Singapore, Slovenia, the USA and the UK.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1578dfbb-2b39-477d-a77e-e92ab04390f1": {"__data__": {"id_": "1578dfbb-2b39-477d-a77e-e92ab04390f1", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "aa280449-4d53-42bf-ac81-153b123044e3", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "9aa6f8b890a43eab85945dc8d9971091b054be32d6bb039b495e1c25ac81d127"}, "3": {"node_id": "d7098edc-db47-4a53-b513-e683c1adb530", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3bd4f11b8d09db6e9c65e6424a034a926af1e158ebc3c2d9821276f8d1003b57"}}, "hash": "d5d50bb35e1e7761baacfd82c840f3caf90c2b26f2e32f51d3548d130d4409cb", "text": "In 2023, the GPAI has 29 members. The GPAI Secretariat is hosted by the OECD in Paris, France. GPAI's mandate covers four themes, two of which are supported by the International Centre of Expertise in Montr\u00e9al for the Advancement of Artificial Intelligence, namely, responsible AI and data governance. A corresponding centre of excellence in Paris will support the other two themes on the future of work, and on innovation and commercialization.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d7098edc-db47-4a53-b513-e683c1adb530": {"__data__": {"id_": "d7098edc-db47-4a53-b513-e683c1adb530", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "1578dfbb-2b39-477d-a77e-e92ab04390f1", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "d5d50bb35e1e7761baacfd82c840f3caf90c2b26f2e32f51d3548d130d4409cb"}, "3": {"node_id": "cb7cd854-1a79-4046-af28-cd663855f957", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "1821d8ca8e1846e9ce277e8e0738dc07004bceba0f19f0d27746d2c9fffb45b2"}}, "hash": "3bd4f11b8d09db6e9c65e6424a034a926af1e158ebc3c2d9821276f8d1003b57", "text": "A corresponding centre of excellence in Paris will support the other two themes on the future of work, and on innovation and commercialization. GPAI also investigated how AI can be leveraged to respond to the COVID-19 pandemic.The OECD AI Principles were adopted in May 2019, and the G20 AI Principles in June 2019. In September 2019 the World Economic Forum issued ten 'AI Government Procurement Guidelines'.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cb7cd854-1a79-4046-af28-cd663855f957": {"__data__": {"id_": "cb7cd854-1a79-4046-af28-cd663855f957", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "d7098edc-db47-4a53-b513-e683c1adb530", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3bd4f11b8d09db6e9c65e6424a034a926af1e158ebc3c2d9821276f8d1003b57"}, "3": {"node_id": "8a6e4da9-99b0-4594-a4d4-6d8553356aac", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "630d1c12b6bb8c1122be90a4155bb8fe0855cdbf56f758209671a0f0541a1930"}}, "hash": "1821d8ca8e1846e9ce277e8e0738dc07004bceba0f19f0d27746d2c9fffb45b2", "text": "In September 2019 the World Economic Forum issued ten 'AI Government Procurement Guidelines'. In February 2020, the European Union published its draft strategy paper for promoting and regulating AI.At the United Nations (UN), several entities have begun to promote and discuss aspects of AI regulation and policy, including the UNICRI Centre for AI and Robotics. In partnership with INTERPOL, UNICRI's Centre issued the report AI and Robotics for Law Enforcement in April 2019 and the follow-up report Towards Responsible AI Innovation in May 2020.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8a6e4da9-99b0-4594-a4d4-6d8553356aac": {"__data__": {"id_": "8a6e4da9-99b0-4594-a4d4-6d8553356aac", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "cb7cd854-1a79-4046-af28-cd663855f957", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "1821d8ca8e1846e9ce277e8e0738dc07004bceba0f19f0d27746d2c9fffb45b2"}, "3": {"node_id": "3bb84b19-fc82-4e58-ad90-3532bba67f4d", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3f077c14dcdb60ac7b9c7821e80f9d753ac3cc75cb51cc599ff76d52ab44bcb8"}}, "hash": "630d1c12b6bb8c1122be90a4155bb8fe0855cdbf56f758209671a0f0541a1930", "text": "In partnership with INTERPOL, UNICRI's Centre issued the report AI and Robotics for Law Enforcement in April 2019 and the follow-up report Towards Responsible AI Innovation in May 2020. At UNESCO's Scientific 40th session in November 2019, the organization commenced a two-year process to achieve a \"global standard-setting instrument on ethics of artificial intelligence\". In pursuit of this goal, UNESCO forums and conferences on AI were held to gather stakeholder views.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3bb84b19-fc82-4e58-ad90-3532bba67f4d": {"__data__": {"id_": "3bb84b19-fc82-4e58-ad90-3532bba67f4d", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "8a6e4da9-99b0-4594-a4d4-6d8553356aac", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "630d1c12b6bb8c1122be90a4155bb8fe0855cdbf56f758209671a0f0541a1930"}, "3": {"node_id": "75121b9f-721c-4abf-b29f-983265977ec4", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "c13de5bb2d1e0984dd42a265df874f18016ebc5776a621ea333aaf689727b452"}}, "hash": "3f077c14dcdb60ac7b9c7821e80f9d753ac3cc75cb51cc599ff76d52ab44bcb8", "text": "In pursuit of this goal, UNESCO forums and conferences on AI were held to gather stakeholder views. A draft text of a Recommendation on the Ethics of AI of the UNESCO Ad Hoc Expert Group was issued in September 2020 and included a call for legislative gaps to be filled. UNESCO tabled the international instrument on the ethics of AI for adoption at its General Conference in November 2021; this was subsequently adopted.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "75121b9f-721c-4abf-b29f-983265977ec4": {"__data__": {"id_": "75121b9f-721c-4abf-b29f-983265977ec4", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "3bb84b19-fc82-4e58-ad90-3532bba67f4d", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3f077c14dcdb60ac7b9c7821e80f9d753ac3cc75cb51cc599ff76d52ab44bcb8"}, "3": {"node_id": "8778830c-3d49-4dad-bd4c-080be200c8d2", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "cd25e8ef90c905d6ed7be2fb9c54724593d9f64f86da5498cbee3a40e08f2da2"}}, "hash": "c13de5bb2d1e0984dd42a265df874f18016ebc5776a621ea333aaf689727b452", "text": "UNESCO tabled the international instrument on the ethics of AI for adoption at its General Conference in November 2021; this was subsequently adopted. While the UN is making progress with the global management of AI, its institutional and legal capability to manage the AGI existential risk is more limited.An initiative of International Telecommunication Union (ITU) in partnership with 40 UN sister agencies, AI for Good is a global platform which aims to identify practical applications of AI to advance the United Nations Sustainable Development Goals and scale those solutions for global impact.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8778830c-3d49-4dad-bd4c-080be200c8d2": {"__data__": {"id_": "8778830c-3d49-4dad-bd4c-080be200c8d2", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "75121b9f-721c-4abf-b29f-983265977ec4", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "c13de5bb2d1e0984dd42a265df874f18016ebc5776a621ea333aaf689727b452"}, "3": {"node_id": "af170c16-e002-479b-971b-c8c7a29ccbb0", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "edf5fd876adae6dcea561c12b2795dcc3b18c7e66e58c35185c3beb37183736e"}}, "hash": "cd25e8ef90c905d6ed7be2fb9c54724593d9f64f86da5498cbee3a40e08f2da2", "text": "It is an action-oriented, global & inclusive United Nations platform fostering development of AI to positively impact health, climate, gender, inclusive prosperity, sustainable infrastructure, and other global development priorities.\n\n\n== Regional and national regulation ==\nThe regulatory and policy landscape for AI is an emerging issue in regional and national jurisdictions globally, for example in the European Union and Russia. Since early 2016, many national, regional and international authorities have begun adopting strategies, actions plans and policy papers on AI.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "af170c16-e002-479b-971b-c8c7a29ccbb0": {"__data__": {"id_": "af170c16-e002-479b-971b-c8c7a29ccbb0", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "8778830c-3d49-4dad-bd4c-080be200c8d2", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "cd25e8ef90c905d6ed7be2fb9c54724593d9f64f86da5498cbee3a40e08f2da2"}, "3": {"node_id": "45fd9949-e19e-4bfe-b443-186b464973f0", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "05589caa74eba4e79b0274545341527f61de09e611f417b3dd09607aacb50ec3"}}, "hash": "edf5fd876adae6dcea561c12b2795dcc3b18c7e66e58c35185c3beb37183736e", "text": "Since early 2016, many national, regional and international authorities have begun adopting strategies, actions plans and policy papers on AI. These documents cover a wide range of topics such as regulation and governance, as well as industrial strategy, research, talent and infrastructure.Different countries have approached the problem in different ways. Regarding the three largest economies, it has been said that \"the United States is following a market-driven approach, China is advancing a state-driven approach, and the EU is pursuing a rights-driven approach.\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "45fd9949-e19e-4bfe-b443-186b464973f0": {"__data__": {"id_": "45fd9949-e19e-4bfe-b443-186b464973f0", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "af170c16-e002-479b-971b-c8c7a29ccbb0", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "edf5fd876adae6dcea561c12b2795dcc3b18c7e66e58c35185c3beb37183736e"}, "3": {"node_id": "f7d7a1ba-be32-45af-9216-34266157bba6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "700cbf195e61ae3f02890186b87a5cf271914637cfb234fe2301e0e7d2c700b3"}}, "hash": "05589caa74eba4e79b0274545341527f61de09e611f417b3dd09607aacb50ec3", "text": "Regarding the three largest economies, it has been said that \"the United States is following a market-driven approach, China is advancing a state-driven approach, and the EU is pursuing a rights-driven approach.\"\n\n\n=== Canada ===\nThe Pan-Canadian Artificial Intelligence Strategy (2017) is supported by federal funding of Can $125 million with the objectives of increasing the number of outstanding AI researchers and skilled graduates in Canada, establishing nodes of scientific excellence at the three major AI centres, developing 'global thought leadership' on the economic, ethical, policy and legal implications of AI advances and supporting a national research community working on AI.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f7d7a1ba-be32-45af-9216-34266157bba6": {"__data__": {"id_": "f7d7a1ba-be32-45af-9216-34266157bba6", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "45fd9949-e19e-4bfe-b443-186b464973f0", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "05589caa74eba4e79b0274545341527f61de09e611f417b3dd09607aacb50ec3"}, "3": {"node_id": "e37d5612-5f7f-4030-990c-791b5497a819", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "410371d31de5bc10be5cd60cc3e97cc5ce9110f2f496977169209849b9ed40df"}}, "hash": "700cbf195e61ae3f02890186b87a5cf271914637cfb234fe2301e0e7d2c700b3", "text": "The Canada CIFAR AI Chairs Program is the cornerstone of the strategy. It benefits from funding of Can$86.5 million over five years to attract and retain world-renowned AI researchers. The federal government appointed an Advisory Council on AI in May 2019 with a focus on examining how to build on Canada's strengths to ensure that AI advancements reflect Canadian values, such as human rights, transparency and openness. The Advisory Council on AI has established a working group on extracting commercial value from Canadian-owned AI and data analytics.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e37d5612-5f7f-4030-990c-791b5497a819": {"__data__": {"id_": "e37d5612-5f7f-4030-990c-791b5497a819", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "f7d7a1ba-be32-45af-9216-34266157bba6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "700cbf195e61ae3f02890186b87a5cf271914637cfb234fe2301e0e7d2c700b3"}, "3": {"node_id": "c6554df6-82ae-4d6f-a6e2-8cbecc0ecea6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "1c01192f784a6ac60da40e77fced5135576e8122d6ef96a99abb7daf175526e9"}}, "hash": "410371d31de5bc10be5cd60cc3e97cc5ce9110f2f496977169209849b9ed40df", "text": "The Advisory Council on AI has established a working group on extracting commercial value from Canadian-owned AI and data analytics. In 2020, the federal government and Government of Quebec announced the opening of the International Centre of Expertise in Montr\u00e9al for the Advancement of Artificial Intelligence, which will advance the cause of responsible development of AI. In June 2022, the government of Canada started a second phase of the Pan-Canadian Artificial Intelligence Strategy.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c6554df6-82ae-4d6f-a6e2-8cbecc0ecea6": {"__data__": {"id_": "c6554df6-82ae-4d6f-a6e2-8cbecc0ecea6", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "e37d5612-5f7f-4030-990c-791b5497a819", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "410371d31de5bc10be5cd60cc3e97cc5ce9110f2f496977169209849b9ed40df"}, "3": {"node_id": "b4b92fc7-7b46-47a6-b87f-1786ff556f4c", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "558cd09b78124b13b3c407a434ce767bfb5f447d0ec8fa138c11c96d7cb26680"}}, "hash": "1c01192f784a6ac60da40e77fced5135576e8122d6ef96a99abb7daf175526e9", "text": "In June 2022, the government of Canada started a second phase of the Pan-Canadian Artificial Intelligence Strategy. In November 2022, Canada has introduced the Digital Charter Implementation Act (Bill C-27), which proposes three acts that have been described as a holistic package of legislation for trust and privacy: the Consumer Privacy Protection Act, the Personal Information and Data Protection Tribunal Act, and the Artificial Intelligence & Data Act (AIDA).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b4b92fc7-7b46-47a6-b87f-1786ff556f4c": {"__data__": {"id_": "b4b92fc7-7b46-47a6-b87f-1786ff556f4c", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "c6554df6-82ae-4d6f-a6e2-8cbecc0ecea6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "1c01192f784a6ac60da40e77fced5135576e8122d6ef96a99abb7daf175526e9"}, "3": {"node_id": "d40f2aae-8322-4d24-83f8-dcb9eacb59db", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "9be8ea61ac2b6c3d75b560a1951854fbce85fe936555cf1593783ef75c1c5a65"}}, "hash": "558cd09b78124b13b3c407a434ce767bfb5f447d0ec8fa138c11c96d7cb26680", "text": "=== China ===\n\nThe regulation of AI in China is mainly governed by the State Council of the People's Republic of China's July 8, 2017 \"A Next Generation Artificial Intelligence Development Plan\" (State Council Document No. 35), in which the Central Committee of the Chinese Communist Party and the State Council of the PRC urged the governing bodies of China to promote the development of AI up to 2030.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d40f2aae-8322-4d24-83f8-dcb9eacb59db": {"__data__": {"id_": "d40f2aae-8322-4d24-83f8-dcb9eacb59db", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "b4b92fc7-7b46-47a6-b87f-1786ff556f4c", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "558cd09b78124b13b3c407a434ce767bfb5f447d0ec8fa138c11c96d7cb26680"}, "3": {"node_id": "d5b94312-ee42-4e5f-a955-dfa0430f8008", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "fcc61864deb451b49e9a0d0a03511dfcf969fd61af367e579ae4e2b14999e4cb"}}, "hash": "9be8ea61ac2b6c3d75b560a1951854fbce85fe936555cf1593783ef75c1c5a65", "text": "35), in which the Central Committee of the Chinese Communist Party and the State Council of the PRC urged the governing bodies of China to promote the development of AI up to 2030. Regulation of the issues of ethical and legal support for the development of AI is accelerating, and policy ensures state control of Chinese companies and over valuable data, including storage of data on Chinese users within the country and the mandatory use of People's Republic of China's national standards for AI, including over big data, cloud computing, and industrial software.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d5b94312-ee42-4e5f-a955-dfa0430f8008": {"__data__": {"id_": "d5b94312-ee42-4e5f-a955-dfa0430f8008", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "d40f2aae-8322-4d24-83f8-dcb9eacb59db", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "9be8ea61ac2b6c3d75b560a1951854fbce85fe936555cf1593783ef75c1c5a65"}, "3": {"node_id": "290e9d95-16d9-4e85-880e-1d316c9443df", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "9d8c7b3575e1e5ec82f2d81f3194ea27cd2b591e092a3979e7c9aa3b848050af"}}, "hash": "fcc61864deb451b49e9a0d0a03511dfcf969fd61af367e579ae4e2b14999e4cb", "text": "In 2021, China published ethical guidelines for the use of AI in China which state that researchers must ensure that AI abides by shared human values, is always under human control, and is not endangering public safety. In 2023, China introduced Interim Measures for the Management of Generative AI Services.\n\n\n=== Council of Europe ===\nThe Council of Europe (CoE) is an international organization which promotes human rights, democracy and the rule of law. It comprises 47 member states, including all 29 Signatories of the European Union's 2018 Declaration of Cooperation on Artificial Intelligence.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "290e9d95-16d9-4e85-880e-1d316c9443df": {"__data__": {"id_": "290e9d95-16d9-4e85-880e-1d316c9443df", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "d5b94312-ee42-4e5f-a955-dfa0430f8008", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "fcc61864deb451b49e9a0d0a03511dfcf969fd61af367e579ae4e2b14999e4cb"}, "3": {"node_id": "11e65c59-21b6-495d-bbc5-54298c09598c", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "9ec0d5ffaef03ba3e9b3d66a18d33f2cb80c305ae45cc318078ea0a64dbe7ea3"}}, "hash": "9d8c7b3575e1e5ec82f2d81f3194ea27cd2b591e092a3979e7c9aa3b848050af", "text": "It comprises 47 member states, including all 29 Signatories of the European Union's 2018 Declaration of Cooperation on Artificial Intelligence. The CoE has created a common legal space in which the members have a legal obligation to guarantee rights as set out in the European Convention on Human Rights. Specifically in relation to AI, \"The Council of Europe's aim is to identify intersecting areas between AI and our standards on human rights, democracy and rule of law, and to develop relevant standard setting or capacity-building solutions\".", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "11e65c59-21b6-495d-bbc5-54298c09598c": {"__data__": {"id_": "11e65c59-21b6-495d-bbc5-54298c09598c", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "290e9d95-16d9-4e85-880e-1d316c9443df", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "9d8c7b3575e1e5ec82f2d81f3194ea27cd2b591e092a3979e7c9aa3b848050af"}, "3": {"node_id": "7b069af8-c943-4fb0-939f-5809a58eeb76", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "6df6bc87f5d6fa91d367d1711ec9ff14376895eaceeae018e78cb2deaab3f934"}}, "hash": "9ec0d5ffaef03ba3e9b3d66a18d33f2cb80c305ae45cc318078ea0a64dbe7ea3", "text": "The large number of relevant documents identified by the CoE include guidelines, charters, papers, reports and strategies. The authoring bodies of these AI regulation documents are not confined to one sector of society and include organizations, companies, bodies and nation-states.\n\n\n=== European Union ===\nThe EU is one of the largest jurisdictions in the world and plays an active role in the global regulation of digital technology through the GDPR, Digital Services Act, the Digital Markets Act.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7b069af8-c943-4fb0-939f-5809a58eeb76": {"__data__": {"id_": "7b069af8-c943-4fb0-939f-5809a58eeb76", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "11e65c59-21b6-495d-bbc5-54298c09598c", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "9ec0d5ffaef03ba3e9b3d66a18d33f2cb80c305ae45cc318078ea0a64dbe7ea3"}, "3": {"node_id": "772e1dbb-0835-49c6-ab2a-2180a3f771fd", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "fc8c22701a62a606f06f0dc7e2e7fc8b7dbf4d4af857cb0cdc98f9926bbeb053"}}, "hash": "6df6bc87f5d6fa91d367d1711ec9ff14376895eaceeae018e78cb2deaab3f934", "text": "=== European Union ===\nThe EU is one of the largest jurisdictions in the world and plays an active role in the global regulation of digital technology through the GDPR, Digital Services Act, the Digital Markets Act. For AI in particular, the Artificial intelligence Act is regarded in 2023 as the most far-reaching regulation of AI worldwide.Most European Union (EU) countries have their own national strategies towards regulating AI, but these are largely convergent. The European Union is guided by a European Strategy on Artificial Intelligence, supported by a High-Level Expert Group on Artificial Intelligence.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "772e1dbb-0835-49c6-ab2a-2180a3f771fd": {"__data__": {"id_": "772e1dbb-0835-49c6-ab2a-2180a3f771fd", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "7b069af8-c943-4fb0-939f-5809a58eeb76", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "6df6bc87f5d6fa91d367d1711ec9ff14376895eaceeae018e78cb2deaab3f934"}, "3": {"node_id": "8daa2da7-00f7-4d5a-90f5-fa1df3cd8908", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "66bffc058a3d23d3a2a9d79bd88d1ec2da419702329125cfbc1e934a0bdf8ee6"}}, "hash": "fc8c22701a62a606f06f0dc7e2e7fc8b7dbf4d4af857cb0cdc98f9926bbeb053", "text": "The European Union is guided by a European Strategy on Artificial Intelligence, supported by a High-Level Expert Group on Artificial Intelligence. In April 2019, the European Commission published its Ethics Guidelines for Trustworthy Artificial Intelligence (AI), following this with its Policy and investment recommendations for trustworthy Artificial Intelligence in June 2019. The EU Commission's High Level Expert Group on Artificial Intelligence carries out work on Trustworthy AI, and the Commission has issued reports on the Safety and Liability Aspects of AI and on the Ethics of Automated Vehicles.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8daa2da7-00f7-4d5a-90f5-fa1df3cd8908": {"__data__": {"id_": "8daa2da7-00f7-4d5a-90f5-fa1df3cd8908", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "772e1dbb-0835-49c6-ab2a-2180a3f771fd", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "fc8c22701a62a606f06f0dc7e2e7fc8b7dbf4d4af857cb0cdc98f9926bbeb053"}, "3": {"node_id": "6195c20e-fc01-4d0b-9dee-801976e855ca", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "cd12ac3848c160e95a7eab66e8478848029bb56a8901abad4878a24c955d0a00"}}, "hash": "66bffc058a3d23d3a2a9d79bd88d1ec2da419702329125cfbc1e934a0bdf8ee6", "text": "The EU Commission's High Level Expert Group on Artificial Intelligence carries out work on Trustworthy AI, and the Commission has issued reports on the Safety and Liability Aspects of AI and on the Ethics of Automated Vehicles. In 2020 the EU Commission sought views on a proposal for AI specific legislation, and that process is ongoing.On February 2, 2020, the European Commission published its White Paper on Artificial Intelligence - A European approach to excellence and trust. The White Paper consists of two main building blocks, an 'ecosystem of excellence' and a 'ecosystem of trust'.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6195c20e-fc01-4d0b-9dee-801976e855ca": {"__data__": {"id_": "6195c20e-fc01-4d0b-9dee-801976e855ca", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "8daa2da7-00f7-4d5a-90f5-fa1df3cd8908", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "66bffc058a3d23d3a2a9d79bd88d1ec2da419702329125cfbc1e934a0bdf8ee6"}, "3": {"node_id": "72e70df6-1bcf-45ed-a43f-18cee1431209", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "755f856f82861f65a029939882c6a53019a2d5bc977541fb83c2a7546f816733"}}, "hash": "cd12ac3848c160e95a7eab66e8478848029bb56a8901abad4878a24c955d0a00", "text": "The White Paper consists of two main building blocks, an 'ecosystem of excellence' and a 'ecosystem of trust'. The 'ecosystem of trust' outlines the EU's approach for a regulatory framework for AI. In its proposed approach, the Commission distinguishes AI applications based on whether they are 'high-risk' or not. Only high-risk AI applications should be in the scope of a future EU regulatory framework.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "72e70df6-1bcf-45ed-a43f-18cee1431209": {"__data__": {"id_": "72e70df6-1bcf-45ed-a43f-18cee1431209", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "6195c20e-fc01-4d0b-9dee-801976e855ca", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "cd12ac3848c160e95a7eab66e8478848029bb56a8901abad4878a24c955d0a00"}, "3": {"node_id": "df2ac080-26d7-4347-9f0c-a3e097959365", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "7eb6abe9261fb699354cb63bdb6f081ad992cb951ce202eb30189fe1fb253c77"}}, "hash": "755f856f82861f65a029939882c6a53019a2d5bc977541fb83c2a7546f816733", "text": "In its proposed approach, the Commission distinguishes AI applications based on whether they are 'high-risk' or not. Only high-risk AI applications should be in the scope of a future EU regulatory framework. An AI application is considered high-risk if it operates in a risky sector (such as healthcare, transport or energy) and is \"used in such a manner that significant risks are likely to arise\".", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "df2ac080-26d7-4347-9f0c-a3e097959365": {"__data__": {"id_": "df2ac080-26d7-4347-9f0c-a3e097959365", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "72e70df6-1bcf-45ed-a43f-18cee1431209", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "755f856f82861f65a029939882c6a53019a2d5bc977541fb83c2a7546f816733"}, "3": {"node_id": "6c19ff9a-3d85-473c-bf54-f306495fae04", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "db98a330363de3b04eb9444db2dc898fa0500f450868c0e3869cdab4583c4023"}}, "hash": "7eb6abe9261fb699354cb63bdb6f081ad992cb951ce202eb30189fe1fb253c77", "text": "An AI application is considered high-risk if it operates in a risky sector (such as healthcare, transport or energy) and is \"used in such a manner that significant risks are likely to arise\". For high-risk AI applications, the requirements are mainly about the : \"training data\", \"data and record-keeping\", \"information to be provided\", \"robustness and accuracy\", and \"human oversight\". There are also requirements specific to certain usages such as remote biometric identification. AI applications that do not qualify as 'high-risk' could be governed by a voluntary labeling scheme.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6c19ff9a-3d85-473c-bf54-f306495fae04": {"__data__": {"id_": "6c19ff9a-3d85-473c-bf54-f306495fae04", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "df2ac080-26d7-4347-9f0c-a3e097959365", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "7eb6abe9261fb699354cb63bdb6f081ad992cb951ce202eb30189fe1fb253c77"}, "3": {"node_id": "5129b3ff-01ab-4c25-8e2d-abc44be4b62e", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "04429430131729963980fc931c105a1b4131404869bcbf9c621b6a567154663e"}}, "hash": "db98a330363de3b04eb9444db2dc898fa0500f450868c0e3869cdab4583c4023", "text": "There are also requirements specific to certain usages such as remote biometric identification. AI applications that do not qualify as 'high-risk' could be governed by a voluntary labeling scheme. As regards compliance and enforcement, the Commission considers prior conformity assessments which could include 'procedures for testing, inspection or certification' and/or 'checks of the algorithms and of the data sets used in the development phase'. A European governance structure on AI in the form of a framework for cooperation of national competent authorities could facilitate the implementation of the regulatory framework.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5129b3ff-01ab-4c25-8e2d-abc44be4b62e": {"__data__": {"id_": "5129b3ff-01ab-4c25-8e2d-abc44be4b62e", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "6c19ff9a-3d85-473c-bf54-f306495fae04", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "db98a330363de3b04eb9444db2dc898fa0500f450868c0e3869cdab4583c4023"}, "3": {"node_id": "cf4c59b0-be0d-4047-95ee-f8261fec9e11", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "5aaf6db180c6c5fb65cddd3f2fbffb5ca280800c0249d2cec755c41de6adab84"}}, "hash": "04429430131729963980fc931c105a1b4131404869bcbf9c621b6a567154663e", "text": "A European governance structure on AI in the form of a framework for cooperation of national competent authorities could facilitate the implementation of the regulatory framework. A January 2021 draft was leaked online on April 14, 2021, before the Commission ultimately presented their official \"Proposal for a Regulation laying down harmonised rules on artificial intelligence (Artificial Intelligence Act)\" a week later. Shortly after, the Artificial Intelligence Act was formally proposed. This includes a fine-tune of the 2020 risk-based approach with, this time, 4 risk categories: \"minimal\", \"limited\", high\" and \"unacceptable\".", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cf4c59b0-be0d-4047-95ee-f8261fec9e11": {"__data__": {"id_": "cf4c59b0-be0d-4047-95ee-f8261fec9e11", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "5129b3ff-01ab-4c25-8e2d-abc44be4b62e", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "04429430131729963980fc931c105a1b4131404869bcbf9c621b6a567154663e"}, "3": {"node_id": "c7993032-c771-46f6-9019-da47b37813ae", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "d66b46adfdf1eabd3a86d9260ee4294700dcc6a821d5f2898379bc8c4e6756a8"}}, "hash": "5aaf6db180c6c5fb65cddd3f2fbffb5ca280800c0249d2cec755c41de6adab84", "text": "This includes a fine-tune of the 2020 risk-based approach with, this time, 4 risk categories: \"minimal\", \"limited\", high\" and \"unacceptable\". The proposal has been severely critiqued in the public debate. Academics are concerned about the various unclear elements in the proposal - such as the broad definition of what constitutes AI - and fear unintended legal implications, especially for vulnerable groups such as patients and migrants. A subsequent version of the AI Act was adopted by the European parliament on June 14, 2023.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c7993032-c771-46f6-9019-da47b37813ae": {"__data__": {"id_": "c7993032-c771-46f6-9019-da47b37813ae", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "cf4c59b0-be0d-4047-95ee-f8261fec9e11", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "5aaf6db180c6c5fb65cddd3f2fbffb5ca280800c0249d2cec755c41de6adab84"}, "3": {"node_id": "f32305e1-faf3-43fa-bca1-c9a8b3ff45b9", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "ab01cc93eef819e12afa25545a8e4c9b7ca5cc331605eaa142968038a7053fc6"}}, "hash": "d66b46adfdf1eabd3a86d9260ee4294700dcc6a821d5f2898379bc8c4e6756a8", "text": "A subsequent version of the AI Act was adopted by the European parliament on June 14, 2023. The AI Act is expected to come into effect in late 2025 or early 2026.Observers have expressed concerns about the multiplication of legislative proposals under the von der Leyen Commission. The speed of the legislative initiatives is partially led by political ambitions of the EU and could put at risk the digital rights of the European citizens, including rights to privacy, especially in the face of uncertain guarantees of data protection through cyber security.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f32305e1-faf3-43fa-bca1-c9a8b3ff45b9": {"__data__": {"id_": "f32305e1-faf3-43fa-bca1-c9a8b3ff45b9", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "c7993032-c771-46f6-9019-da47b37813ae", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "d66b46adfdf1eabd3a86d9260ee4294700dcc6a821d5f2898379bc8c4e6756a8"}, "3": {"node_id": "a1912be1-07e2-4e6b-b577-9c8148b714ce", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "811559fc0f64ce22599c8a181370673feba0cb515c2b736e621db84b661ce139"}}, "hash": "ab01cc93eef819e12afa25545a8e4c9b7ca5cc331605eaa142968038a7053fc6", "text": "Among the stated guiding principles in the variety of legislative proposals in the area of AI under the von der Leyen Commission are the objectives of strategic autonomy and the concept of digital sovereignty.\n\n\n=== Germany ===\nIn November 2020, DIN, DKE and the German Federal Ministry for Economic Affairs and Energy published the first edition of the \"German Standardization Roadmap for Artificial Intelligence\" (NRM KI) and presented it to the public at the Digital Summit of the Federal Government of Germany. NRM KI describes requierements to future regulations and standards in the context of AI.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a1912be1-07e2-4e6b-b577-9c8148b714ce": {"__data__": {"id_": "a1912be1-07e2-4e6b-b577-9c8148b714ce", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "f32305e1-faf3-43fa-bca1-c9a8b3ff45b9", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "ab01cc93eef819e12afa25545a8e4c9b7ca5cc331605eaa142968038a7053fc6"}, "3": {"node_id": "4c857409-72e7-460d-b8d4-316fd13856c6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "b6c1413e67e7d1cb50432ee10fc3a22f90a607b41c8de7c18aa489b5ba4fab81"}}, "hash": "811559fc0f64ce22599c8a181370673feba0cb515c2b736e621db84b661ce139", "text": "NRM KI describes requierements to future regulations and standards in the context of AI. The implementation of the recommendations for action is intended to help to strengthen the German economy and science in the international competition in the field of artificial intelligence and create innovation-friendly conditions for this emerging technology. The first edition is a 200-page long document written by 300 experts. The second edition of the NRM KI was published to coincide with the German government's Digital Summit on December 9, 2022.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4c857409-72e7-460d-b8d4-316fd13856c6": {"__data__": {"id_": "4c857409-72e7-460d-b8d4-316fd13856c6", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "a1912be1-07e2-4e6b-b577-9c8148b714ce", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "811559fc0f64ce22599c8a181370673feba0cb515c2b736e621db84b661ce139"}, "3": {"node_id": "e061f964-72eb-486f-bf2b-77a476712fd5", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "daa57beaa0297d6621f21a2b6ee0daabd8323f578a8c81fb03cecdee33bd0f12"}}, "hash": "b6c1413e67e7d1cb50432ee10fc3a22f90a607b41c8de7c18aa489b5ba4fab81", "text": "The first edition is a 200-page long document written by 300 experts. The second edition of the NRM KI was published to coincide with the German government's Digital Summit on December 9, 2022. DIN coordinated more than 570 participating experts from a wide range of fields from science, industry, civil society and the public sector. The second edition is a 450-page long document.\nOn the one hand, NRM KI covers the focus topics in terms of applications (e.g.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e061f964-72eb-486f-bf2b-77a476712fd5": {"__data__": {"id_": "e061f964-72eb-486f-bf2b-77a476712fd5", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "4c857409-72e7-460d-b8d4-316fd13856c6", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "b6c1413e67e7d1cb50432ee10fc3a22f90a607b41c8de7c18aa489b5ba4fab81"}, "3": {"node_id": "64df011c-f258-4eee-bb38-f13816abcc4a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "be0d41e74d00d5dcc31ebea5602b812f1780e525ee1e04ae2f3b1582757b5356"}}, "hash": "daa57beaa0297d6621f21a2b6ee0daabd8323f578a8c81fb03cecdee33bd0f12", "text": "The second edition is a 450-page long document.\nOn the one hand, NRM KI covers the focus topics in terms of applications (e.g. medicine, mobility, energy & environment, financial services, industrial automation) and fundamental issues (e.g. AI classification, security, certifiability, socio-technical systems, ethics). On the other hand, it provides an overview of the central terms in the field of AI and its environment across a wide range of interest groups and information sources.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "64df011c-f258-4eee-bb38-f13816abcc4a": {"__data__": {"id_": "64df011c-f258-4eee-bb38-f13816abcc4a", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "e061f964-72eb-486f-bf2b-77a476712fd5", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "daa57beaa0297d6621f21a2b6ee0daabd8323f578a8c81fb03cecdee33bd0f12"}, "3": {"node_id": "dc084ac4-97b1-4297-8fd2-96e5452fe484", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "4e0093a437d01a7ec7ade4d67b3661940abc80b21a70eccc7b2fc955f7d0605d"}}, "hash": "be0d41e74d00d5dcc31ebea5602b812f1780e525ee1e04ae2f3b1582757b5356", "text": "On the other hand, it provides an overview of the central terms in the field of AI and its environment across a wide range of interest groups and information sources. In total, the document covers 116 standardisation needs and provides six central recommendations for action.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dc084ac4-97b1-4297-8fd2-96e5452fe484": {"__data__": {"id_": "dc084ac4-97b1-4297-8fd2-96e5452fe484", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "64df011c-f258-4eee-bb38-f13816abcc4a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "be0d41e74d00d5dcc31ebea5602b812f1780e525ee1e04ae2f3b1582757b5356"}, "3": {"node_id": "bdcf2ecb-e0ca-42fb-83f8-f0ebae060696", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "e919fb5b711cfd5d0c9b52e9b2e89c06981b6101efe23f11bc301c7b42bc04c5"}}, "hash": "4e0093a437d01a7ec7ade4d67b3661940abc80b21a70eccc7b2fc955f7d0605d", "text": "In total, the document covers 116 standardisation needs and provides six central recommendations for action.\n\n\n=== G7 ===\nOn 30 October 2023, members of the G7 subscribe to eleven guiding principles for the design, production and implementation of advanced artificial intelligence systems, as well as a voluntary Code of Conduct for artificial intelligence developers in the context of the Hiroshima Process.The agreement receives the applause of Ursula von der Leyen who finds in it the principles of the AI Directive, currently being finalized.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bdcf2ecb-e0ca-42fb-83f8-f0ebae060696": {"__data__": {"id_": "bdcf2ecb-e0ca-42fb-83f8-f0ebae060696", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "dc084ac4-97b1-4297-8fd2-96e5452fe484", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "4e0093a437d01a7ec7ade4d67b3661940abc80b21a70eccc7b2fc955f7d0605d"}, "3": {"node_id": "ec0111d0-404b-48fc-96af-e1861ff7c2cf", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "65ddd08ff03ef2ee13d72aff2cf87085c783825f98c96d3c5fb0a2b0a70a9fda"}}, "hash": "e919fb5b711cfd5d0c9b52e9b2e89c06981b6101efe23f11bc301c7b42bc04c5", "text": "=== Italy ===\nIn October 2023, the Italian privacy authority approved a regulation that provides three principles for therapeutic decisions taken by automated systems: transparency of decision-making processes, human supervision of automated decisions and algorithmic non-discrimination.\n\n\n=== United Kingdom ===\nThe UK supported the application and development of AI in business via the Digital Economy Strategy 2015-2018 introduced at the beginning of 2015 by Innovate UK as part of the UK Digital Strategy.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec0111d0-404b-48fc-96af-e1861ff7c2cf": {"__data__": {"id_": "ec0111d0-404b-48fc-96af-e1861ff7c2cf", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "bdcf2ecb-e0ca-42fb-83f8-f0ebae060696", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "e919fb5b711cfd5d0c9b52e9b2e89c06981b6101efe23f11bc301c7b42bc04c5"}, "3": {"node_id": "2d04f17c-7afa-4e47-8c30-ec989dcd0e1a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "06b1627bb443cd99ba06e2af2100be1fbcc30a20247cda2599e257b6d84ec250"}}, "hash": "65ddd08ff03ef2ee13d72aff2cf87085c783825f98c96d3c5fb0a2b0a70a9fda", "text": "=== United Kingdom ===\nThe UK supported the application and development of AI in business via the Digital Economy Strategy 2015-2018 introduced at the beginning of 2015 by Innovate UK as part of the UK Digital Strategy. In the public sector, the Department for Digital, Culture, Media and Sport advised on data ethics and the Alan Turing Institute provided guidance on responsible design and implementation of AI systems. In terms of cyber security, in 2020 the National Cyber Security Centre has issued guidance on 'Intelligent Security Tools'.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2d04f17c-7afa-4e47-8c30-ec989dcd0e1a": {"__data__": {"id_": "2d04f17c-7afa-4e47-8c30-ec989dcd0e1a", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "ec0111d0-404b-48fc-96af-e1861ff7c2cf", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "65ddd08ff03ef2ee13d72aff2cf87085c783825f98c96d3c5fb0a2b0a70a9fda"}, "3": {"node_id": "02820d6b-5f5e-4dfc-90d0-d7ab9ae9402e", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "32841dd5ecdbb3215eb17e900f1d2f11f9dcb4b215c52daa9c0b845b4cbbe98d"}}, "hash": "06b1627bb443cd99ba06e2af2100be1fbcc30a20247cda2599e257b6d84ec250", "text": "In terms of cyber security, in 2020 the National Cyber Security Centre has issued guidance on 'Intelligent Security Tools'. The following year, the UK published its 10-year National AI Strategy, which describes actions to assess long-term AI risks, including AGI-related catastrophic risks.In March 2023, the UK released the white paper A pro-innovation approach to AI regulation. This white paper presents general AI principles, but leaves significant flexibility to existing regulators in how they adapt these principles to specific areas such as transport or financial markets.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "02820d6b-5f5e-4dfc-90d0-d7ab9ae9402e": {"__data__": {"id_": "02820d6b-5f5e-4dfc-90d0-d7ab9ae9402e", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "2d04f17c-7afa-4e47-8c30-ec989dcd0e1a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "06b1627bb443cd99ba06e2af2100be1fbcc30a20247cda2599e257b6d84ec250"}, "3": {"node_id": "277b92e6-a05b-4d83-af32-cebb2203090b", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d569446b8cdc3c09e4a973765333f58b7f9767ec1f5278be4859845ad8ee1ae"}}, "hash": "32841dd5ecdbb3215eb17e900f1d2f11f9dcb4b215c52daa9c0b845b4cbbe98d", "text": "This white paper presents general AI principles, but leaves significant flexibility to existing regulators in how they adapt these principles to specific areas such as transport or financial markets. In November 2023, the UK hosts the first AI safety summit, with the prime minister Rishi Sunak aiming to position the UK as a leader in AI safety regulation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "277b92e6-a05b-4d83-af32-cebb2203090b": {"__data__": {"id_": "277b92e6-a05b-4d83-af32-cebb2203090b", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "02820d6b-5f5e-4dfc-90d0-d7ab9ae9402e", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "32841dd5ecdbb3215eb17e900f1d2f11f9dcb4b215c52daa9c0b845b4cbbe98d"}, "3": {"node_id": "b60653b7-3fbe-497e-ac43-df0ffe9177dd", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3dec2d7bbf3279c3eedc1beeda41460e9c472f453acad8465c676016a4555d0e"}}, "hash": "8d569446b8cdc3c09e4a973765333f58b7f9767ec1f5278be4859845ad8ee1ae", "text": "In November 2023, the UK hosts the first AI safety summit, with the prime minister Rishi Sunak aiming to position the UK as a leader in AI safety regulation.\n\n\n=== United States ===\nDiscussions on regulation of AI in the United States have included topics such as the timeliness of regulating AI, the nature of the federal regulatory framework to govern and promote AI, including what agency should lead, the regulatory and governing powers of that agency, and how to update regulations in the face of rapidly changing technology, as well as the roles of state governments and courts.As early as 2016, the Obama administration had begun to focus on the risks and regulations for artificial intelligence.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b60653b7-3fbe-497e-ac43-df0ffe9177dd": {"__data__": {"id_": "b60653b7-3fbe-497e-ac43-df0ffe9177dd", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "277b92e6-a05b-4d83-af32-cebb2203090b", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d569446b8cdc3c09e4a973765333f58b7f9767ec1f5278be4859845ad8ee1ae"}, "3": {"node_id": "2d433ebe-7697-4aaf-9aab-9fe004c4412c", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "f51317c9a1498a08c84eab680a6c750091902a974d805f6c8d53f2ed851b2943"}}, "hash": "3dec2d7bbf3279c3eedc1beeda41460e9c472f453acad8465c676016a4555d0e", "text": "In a report titled Preparing For the Future of Artificial Intelligence, the National Science and Technology Council set a precedent to allow researchers to continue to develop new AI technologies with few restrictions. It is stated within the report that \"the approach to regulation of AI-enabled products to protect public safety should be informed by assessment of the aspects of risk....\". These risks would be the principal reason to create any form of regulation, granted that any existing regulation would not apply to AI technology.\nThe first main report was the National Strategic Research and Development Plan for Artificial Intelligence.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2d433ebe-7697-4aaf-9aab-9fe004c4412c": {"__data__": {"id_": "2d433ebe-7697-4aaf-9aab-9fe004c4412c", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "b60653b7-3fbe-497e-ac43-df0ffe9177dd", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3dec2d7bbf3279c3eedc1beeda41460e9c472f453acad8465c676016a4555d0e"}, "3": {"node_id": "be5c068d-5450-4c4e-88f1-c04163b345b1", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "f3114a13315953815f3970ccdd8a46393d2cde5d6384682a67d2b9bf730c10a9"}}, "hash": "f51317c9a1498a08c84eab680a6c750091902a974d805f6c8d53f2ed851b2943", "text": "These risks would be the principal reason to create any form of regulation, granted that any existing regulation would not apply to AI technology.\nThe first main report was the National Strategic Research and Development Plan for Artificial Intelligence. On August 13, 2018, Section 1051 of the Fiscal Year 2019 John S. McCain National Defense Authorization Act (P.L. 115-232) established the National Security Commission on Artificial Intelligence \"to consider the methods and means necessary to advance the development of artificial intelligence, machine learning, and associated technologies to comprehensively address the national security and defense needs of the United States.\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "be5c068d-5450-4c4e-88f1-c04163b345b1": {"__data__": {"id_": "be5c068d-5450-4c4e-88f1-c04163b345b1", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "2d433ebe-7697-4aaf-9aab-9fe004c4412c", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "f51317c9a1498a08c84eab680a6c750091902a974d805f6c8d53f2ed851b2943"}, "3": {"node_id": "cdb0ad52-7a9d-4f4b-b561-e6b422f74002", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "534a86bb5c3abdf374d882061f35535573ed385d355c3c2a41b6da814b84862a"}}, "hash": "f3114a13315953815f3970ccdd8a46393d2cde5d6384682a67d2b9bf730c10a9", "text": "Steering on regulating security-related AI is provided by the National Security Commission on Artificial Intelligence. The Artificial Intelligence Initiative Act (S.1558) is a proposed bill that would establish a federal initiative designed to accelerate research and development on AI for, inter alia, the economic and national security of the United States.On January 7, 2019, following an Executive Order on Maintaining American Leadership in Artificial Intelligence, the White House's Office of Science and Technology Policy released a draft Guidance for Regulation of Artificial Intelligence Applications, which includes ten principles for United States agencies when deciding whether and how to regulate AI.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cdb0ad52-7a9d-4f4b-b561-e6b422f74002": {"__data__": {"id_": "cdb0ad52-7a9d-4f4b-b561-e6b422f74002", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "be5c068d-5450-4c4e-88f1-c04163b345b1", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "f3114a13315953815f3970ccdd8a46393d2cde5d6384682a67d2b9bf730c10a9"}, "3": {"node_id": "d950918f-c029-4bd2-b943-fd13e36392d4", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "a76ad939c95ab9a7c74f5434805e16b54d8af43fc6edb2e4acb4f4a5cdacfc03"}}, "hash": "534a86bb5c3abdf374d882061f35535573ed385d355c3c2a41b6da814b84862a", "text": "In response, the National Institute of Standards and Technology has released a position paper, and the Defense Innovation Board has issued recommendations on the ethical use of AI. A year later, the administration called for comments on regulation in another draft of its Guidance for Regulation of Artificial Intelligence Applications.Other specific agencies working on the regulation of AI include the Food and Drug Administration, which has created pathways to regulate the incorporation of AI in medical imaging.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d950918f-c029-4bd2-b943-fd13e36392d4": {"__data__": {"id_": "d950918f-c029-4bd2-b943-fd13e36392d4", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "cdb0ad52-7a9d-4f4b-b561-e6b422f74002", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "534a86bb5c3abdf374d882061f35535573ed385d355c3c2a41b6da814b84862a"}, "3": {"node_id": "bdd68646-85b9-4e9e-86a3-f8197a1ae594", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "b6e0f0c619710e95b31a1b04fab808bb7bdbe55b584f5b910ed812b814d30287"}}, "hash": "a76ad939c95ab9a7c74f5434805e16b54d8af43fc6edb2e4acb4f4a5cdacfc03", "text": "National Science and Technology Council also published the National Artificial Intelligence Research and Development Strategic Plan, which received public scrutiny and recommendations to further improve it towards enabling Trustworthy AI.In March 2021, the National Security Commission on Artificial Intelligence released their final report. In the report, they stated that \"Advances in AI, including the mastery of more general AI capabilities along one or more dimensions, will likely provide new capabilities and applications. Some of these advances could lead to inflection points or leaps in capabilities.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bdd68646-85b9-4e9e-86a3-f8197a1ae594": {"__data__": {"id_": "bdd68646-85b9-4e9e-86a3-f8197a1ae594", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "d950918f-c029-4bd2-b943-fd13e36392d4", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "a76ad939c95ab9a7c74f5434805e16b54d8af43fc6edb2e4acb4f4a5cdacfc03"}, "3": {"node_id": "68837bf5-a10c-4ea0-873c-3636efee4108", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "1f1589f277ce8948cb75c9bddc7579900328d3b17429209156912c5690fee0a3"}}, "hash": "b6e0f0c619710e95b31a1b04fab808bb7bdbe55b584f5b910ed812b814d30287", "text": "Some of these advances could lead to inflection points or leaps in capabilities. Such advances may also introduce new concerns and risks and the need for new policies, recommendations, and technical advances to assure that systems are aligned with goals and values, including safety, robustness and trustworthiness. The US should monitor advances in AI and make necessary investments in technology and give attention to policy so as to ensure that AI systems and their uses align with our goals and values.\"\nIn June 2022, Senators Rob Portman and Gary Peters introduced the Global Catastrophic Risk Mitigation Act.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "68837bf5-a10c-4ea0-873c-3636efee4108": {"__data__": {"id_": "68837bf5-a10c-4ea0-873c-3636efee4108", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "bdd68646-85b9-4e9e-86a3-f8197a1ae594", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "b6e0f0c619710e95b31a1b04fab808bb7bdbe55b584f5b910ed812b814d30287"}, "3": {"node_id": "46313293-0201-45e0-9a4b-a0db39c0cf85", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "1a0c16c5001f68f735d97558ff6349d13c310d7b544eb849cf8a19aaddf11bff"}}, "hash": "1f1589f277ce8948cb75c9bddc7579900328d3b17429209156912c5690fee0a3", "text": "In June 2022, Senators Rob Portman and Gary Peters introduced the Global Catastrophic Risk Mitigation Act. The bipartisan bill \"would also help counter the risk of artificial intelligence... from being abused in ways that may pose a catastrophic risk\". On October 4, 2022, President Joe Biden unveiled a new AI Bill of Rights, which outlines five protections Americans should have in the AI age: 1. Safe and Effective Systems, 2. Algorithmic Discrimination Protection, 3.Data Privacy, 4. Notice and Explanation, and 5. Human Alternatives, Consideration, and Fallback.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "46313293-0201-45e0-9a4b-a0db39c0cf85": {"__data__": {"id_": "46313293-0201-45e0-9a4b-a0db39c0cf85", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "68837bf5-a10c-4ea0-873c-3636efee4108", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "1f1589f277ce8948cb75c9bddc7579900328d3b17429209156912c5690fee0a3"}, "3": {"node_id": "cc61d19e-f3b0-4e0f-aef7-4fa3d71e6bf9", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "2aa23f526e2ce51fecb650d0492ab6640f13fdd1286882d4f923e93c6a6e6fe1"}}, "hash": "1a0c16c5001f68f735d97558ff6349d13c310d7b544eb849cf8a19aaddf11bff", "text": "Safe and Effective Systems, 2. Algorithmic Discrimination Protection, 3.Data Privacy, 4. Notice and Explanation, and 5. Human Alternatives, Consideration, and Fallback. The Bill was introduced in October 2021 by the Office of Science and Technology Policy (OSTP), a US government department that advises the president on science and technology.In January 2023, the New York City Bias Audit Law (Local Law 144) was enacted by the NYC Council in November 2021.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cc61d19e-f3b0-4e0f-aef7-4fa3d71e6bf9": {"__data__": {"id_": "cc61d19e-f3b0-4e0f-aef7-4fa3d71e6bf9", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "46313293-0201-45e0-9a4b-a0db39c0cf85", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "1a0c16c5001f68f735d97558ff6349d13c310d7b544eb849cf8a19aaddf11bff"}, "3": {"node_id": "9d069afd-6296-4a25-bbd9-699780c73856", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "0daa55c949812a1f09cf5f3e624eb84f2f06b5e922a806baf0cd2051670d6a5b"}}, "hash": "2aa23f526e2ce51fecb650d0492ab6640f13fdd1286882d4f923e93c6a6e6fe1", "text": "Originally due to come into effect on 1st January 2023, the enforcement date for Local Law 144 has been pushed back due to the high volume of comments received during the public hearing on the Department of Consumer and Worker Protection's (DCWP) proposed rules to clarify the requirements of the legislation. It eventually became effective on July 5, 2023. From this date, the companies that are operating and hiring in New York City are prohibited from using automated tools to hire candidates or promote employees, unless the tools have been independently audited for bias.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9d069afd-6296-4a25-bbd9-699780c73856": {"__data__": {"id_": "9d069afd-6296-4a25-bbd9-699780c73856", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "cc61d19e-f3b0-4e0f-aef7-4fa3d71e6bf9", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "2aa23f526e2ce51fecb650d0492ab6640f13fdd1286882d4f923e93c6a6e6fe1"}, "3": {"node_id": "d65371b3-499b-4700-9b3c-e30619e15c0f", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "fbec50344cd21836d0eddcac349046f3601be36a91757a5ae77a21e6443cdd21"}}, "hash": "0daa55c949812a1f09cf5f3e624eb84f2f06b5e922a806baf0cd2051670d6a5b", "text": "From this date, the companies that are operating and hiring in New York City are prohibited from using automated tools to hire candidates or promote employees, unless the tools have been independently audited for bias.\nIn July 2023, the Biden-Harris Administration secured voluntary commitments from seven companies - Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and Open AI - to manage the risks associated with AI.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d65371b3-499b-4700-9b3c-e30619e15c0f": {"__data__": {"id_": "d65371b3-499b-4700-9b3c-e30619e15c0f", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "9d069afd-6296-4a25-bbd9-699780c73856", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "0daa55c949812a1f09cf5f3e624eb84f2f06b5e922a806baf0cd2051670d6a5b"}, "3": {"node_id": "c8bc0862-97bd-4ca8-8217-fc0585303cac", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "4ff8d26ecd19072cdab5d156cc1bba9887cda4bbd390c2b906c7c917700382f8"}}, "hash": "fbec50344cd21836d0eddcac349046f3601be36a91757a5ae77a21e6443cdd21", "text": "In July 2023, the Biden-Harris Administration secured voluntary commitments from seven companies - Amazon, Anthropic, Google, Inflection, Meta, Microsoft, and Open AI - to manage the risks associated with AI. The companies committed to ensure AI products undergo both internal and external security testing before public release; to share information on the management of AI risks with the industry, governments, civil society, and academia; to prioritize cybersecurity and protect proprietary AI system components; to develop mechanisms to inform users when content is AI-generated, such as watermarking; to publicly report on their AI systems' capabilities, limitations, and areas of use; to prioritize research on societal risks posed by AI, including bias, discrimination, and privacy concerns; and to develop AI systems to address societal challenges, ranging from cancer prevention to climate change mitigation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c8bc0862-97bd-4ca8-8217-fc0585303cac": {"__data__": {"id_": "c8bc0862-97bd-4ca8-8217-fc0585303cac", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "d65371b3-499b-4700-9b3c-e30619e15c0f", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "fbec50344cd21836d0eddcac349046f3601be36a91757a5ae77a21e6443cdd21"}, "3": {"node_id": "d6242ba1-4b98-4595-be68-48a170bf9120", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "9d2c3e7cde9324c9eb757879c3b8de77a5ee925f07c8f643aa312bf13ab3c2c4"}}, "hash": "4ff8d26ecd19072cdab5d156cc1bba9887cda4bbd390c2b906c7c917700382f8", "text": "In September 2023, eight additional companies - Adobe, Cohere, IBM, Nvidia, Palantir, Salesforce, Scale AI, and Stability AI - subscribed to these voluntary commitments. The Biden administration, in October 2023, is expected to issue an executive order leveraging the federal government's purchasing power to shape AI regulations, hinting at a proactive governmental stance in regulating AI technologies.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d6242ba1-4b98-4595-be68-48a170bf9120": {"__data__": {"id_": "d6242ba1-4b98-4595-be68-48a170bf9120", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "c8bc0862-97bd-4ca8-8217-fc0585303cac", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "4ff8d26ecd19072cdab5d156cc1bba9887cda4bbd390c2b906c7c917700382f8"}, "3": {"node_id": "ac35d610-8788-42e0-b097-50f0d50e51de", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "65db32c4b752f6bef10e4da5824f691ce28e7e1cb5b33b212e2550a65714a601"}}, "hash": "9d2c3e7cde9324c9eb757879c3b8de77a5ee925f07c8f643aa312bf13ab3c2c4", "text": "The Biden administration, in October 2023, is expected to issue an executive order leveraging the federal government's purchasing power to shape AI regulations, hinting at a proactive governmental stance in regulating AI technologies.\n\n\n=== Brazil ===\nOn September 30, 2021, the Brazilian Chamber of Deputies approved the Brazilian Legal Framework for Artificial Intelligence, Marco Legal da Intelig\u00eancia Artificial, in regulatory efforts for the development and usage of AI technologies and to further stimulate research and innovation in AI solutions aimed at ethics, culture, justice, fairness, and accountability.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ac35d610-8788-42e0-b097-50f0d50e51de": {"__data__": {"id_": "ac35d610-8788-42e0-b097-50f0d50e51de", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "d6242ba1-4b98-4595-be68-48a170bf9120", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "9d2c3e7cde9324c9eb757879c3b8de77a5ee925f07c8f643aa312bf13ab3c2c4"}, "3": {"node_id": "fddbddea-bc4d-4f8b-b38b-9e9bcdb6ae3e", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "b80eb37da4b9df7b10665e59c8a2e6f9ac60eab8a0a42b580c05adbf3a7c6634"}}, "hash": "65db32c4b752f6bef10e4da5824f691ce28e7e1cb5b33b212e2550a65714a601", "text": "This 10 article bill outlines objectives including missions to contribute to the elaboration of ethical principles, promote sustained investments in research, and remove barriers to innovation. Specifically, in article 4, the bill emphasizes the avoidance of discriminatory AI solutions, plurality, and respect for human rights. Furthermore, this act emphasizes the importance of the equality principle in deliberate decision-making algorithms, especially for highly diverse and multiethnic societies like that of Brazil.\nWhen the bill was first released to the public, it faced substantial criticism, alarming the government for critical provisions.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fddbddea-bc4d-4f8b-b38b-9e9bcdb6ae3e": {"__data__": {"id_": "fddbddea-bc4d-4f8b-b38b-9e9bcdb6ae3e", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "ac35d610-8788-42e0-b097-50f0d50e51de", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "65db32c4b752f6bef10e4da5824f691ce28e7e1cb5b33b212e2550a65714a601"}, "3": {"node_id": "595aebba-f9e5-413d-b27c-547e074b82e7", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "6f3d041df908079bd1185e831752cbbc702a4fbd96ebf5ad527be94872853e8d"}}, "hash": "b80eb37da4b9df7b10665e59c8a2e6f9ac60eab8a0a42b580c05adbf3a7c6634", "text": "When the bill was first released to the public, it faced substantial criticism, alarming the government for critical provisions. The underlying issue is that this bill fails to thoroughly and carefully address accountability, transparency, and inclusivity principles. Article VI establishes subjective liability, meaning any individual that is damaged by an AI system and is wishing to receive compensation must specify the stakeholder and prove that there was a mistake in the machine's life cycle. Scholars emphasize that it is out of legal order to assign an individual responsible for proving algorithmic errors given the high degree of autonomy, unpredictability, and complexity of AI systems.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "595aebba-f9e5-413d-b27c-547e074b82e7": {"__data__": {"id_": "595aebba-f9e5-413d-b27c-547e074b82e7", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "fddbddea-bc4d-4f8b-b38b-9e9bcdb6ae3e", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "b80eb37da4b9df7b10665e59c8a2e6f9ac60eab8a0a42b580c05adbf3a7c6634"}, "3": {"node_id": "70f378c9-0dbe-492b-a7a1-70ae65c4d445", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "2be7d9031e7331e38c08e427a1f106540e00c030688b556ed6dfdf1b72737ef0"}}, "hash": "6f3d041df908079bd1185e831752cbbc702a4fbd96ebf5ad527be94872853e8d", "text": "Scholars emphasize that it is out of legal order to assign an individual responsible for proving algorithmic errors given the high degree of autonomy, unpredictability, and complexity of AI systems. This also drew attention to the currently occurring issues with face recognition systems in Brazil leading to unjust arrests by the police, which would then imply that when this bill is adopted, individuals would have to prove and justify these machine errors.\nThe main controversy of this draft bill was directed to three proposed principles.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "70f378c9-0dbe-492b-a7a1-70ae65c4d445": {"__data__": {"id_": "70f378c9-0dbe-492b-a7a1-70ae65c4d445", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "595aebba-f9e5-413d-b27c-547e074b82e7", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "6f3d041df908079bd1185e831752cbbc702a4fbd96ebf5ad527be94872853e8d"}, "3": {"node_id": "1fafb5e2-7d1e-4be8-a5ea-666ae33cf0c4", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "ced4e8ae64266bd377f0879136bb1410a1e16833e5c0ae9e6b5edbdff199d358"}}, "hash": "2be7d9031e7331e38c08e427a1f106540e00c030688b556ed6dfdf1b72737ef0", "text": "The main controversy of this draft bill was directed to three proposed principles. First, the non-discrimination principle, suggests that AI must be developed and used in a way that merely mitigates the possibility of abusive and discriminatory practices. Secondly, the pursuit of neutrality principle lists recommendations for stakeholders to mitigate biases; however, with no obligation to achieve this goal. Lastly, the transparency principle states that a system's transparency is only necessary when there is a high risk of violating fundamental rights. As easily observed, the Brazilian Legal Framework for Artificial Intelligence lacks binding and obligatory clauses and is rather filled with relaxed guidelines.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1fafb5e2-7d1e-4be8-a5ea-666ae33cf0c4": {"__data__": {"id_": "1fafb5e2-7d1e-4be8-a5ea-666ae33cf0c4", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "70f378c9-0dbe-492b-a7a1-70ae65c4d445", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "2be7d9031e7331e38c08e427a1f106540e00c030688b556ed6dfdf1b72737ef0"}, "3": {"node_id": "276f85bf-ecad-4562-a48d-8a7a07742bae", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "fdc00dee40369ef3162da46de28a28d9e257a0de361a07347f987a687697e786"}}, "hash": "ced4e8ae64266bd377f0879136bb1410a1e16833e5c0ae9e6b5edbdff199d358", "text": "As easily observed, the Brazilian Legal Framework for Artificial Intelligence lacks binding and obligatory clauses and is rather filled with relaxed guidelines. In fact, experts emphasize that this bill may even make accountability for AI discriminatory biases even harder to achieve. Compared to the EU's proposal of extensive risk-based regulations, the Brazilian Bill has 10 articles proposing vague and generic recommendations.\nCompared to the multistakeholder participation approach taken previously in the 2000s when drafting the Brazilian Internet Bill of Rights, Marco Civil da Internet, the Brazilian Bill is assessed to significantly lack perspective.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "276f85bf-ecad-4562-a48d-8a7a07742bae": {"__data__": {"id_": "276f85bf-ecad-4562-a48d-8a7a07742bae", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "1fafb5e2-7d1e-4be8-a5ea-666ae33cf0c4", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "ced4e8ae64266bd377f0879136bb1410a1e16833e5c0ae9e6b5edbdff199d358"}, "3": {"node_id": "e8511380-03a2-42ee-9304-9a6044348b4a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3f17ae9e449428509c86f52ebfb120a8347d008c6470ed296483a0e88ccbd955"}}, "hash": "fdc00dee40369ef3162da46de28a28d9e257a0de361a07347f987a687697e786", "text": "Compared to the multistakeholder participation approach taken previously in the 2000s when drafting the Brazilian Internet Bill of Rights, Marco Civil da Internet, the Brazilian Bill is assessed to significantly lack perspective. Multistakeholderism, more commonly referred to as Multistakeholder Governance, is defined as the practice of bringing multiple stakeholders to participate in dialogue, decision-making, and implementation of responses to jointly perceived problems. In the context of regulatory AI, this multistakeholder perspective captures the trade-offs and varying perspectives of different stakeholders with specific interests, which helps maintain transparency and broader efficacy.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e8511380-03a2-42ee-9304-9a6044348b4a": {"__data__": {"id_": "e8511380-03a2-42ee-9304-9a6044348b4a", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "276f85bf-ecad-4562-a48d-8a7a07742bae", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "fdc00dee40369ef3162da46de28a28d9e257a0de361a07347f987a687697e786"}, "3": {"node_id": "4bcbb9c5-077e-4e4e-b40c-f6a75032a85f", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "b685517f5b7b16083ec962aa28df8129a9897253b148242419690a53757294cb"}}, "hash": "3f17ae9e449428509c86f52ebfb120a8347d008c6470ed296483a0e88ccbd955", "text": "In the context of regulatory AI, this multistakeholder perspective captures the trade-offs and varying perspectives of different stakeholders with specific interests, which helps maintain transparency and broader efficacy. On the contrary, the legislative proposal for AI regulation did not follow a similar multistakeholder approach.\nFuture steps may include, expanding upon the multistakeholder perspective. There has been a growing concern about the inapplicability of the framework of the bill, which highlights that the one-shoe-fits-all solution may not be suitable for the regulation of AI and calls for subjective and adaptive provisions.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4bcbb9c5-077e-4e4e-b40c-f6a75032a85f": {"__data__": {"id_": "4bcbb9c5-077e-4e4e-b40c-f6a75032a85f", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "e8511380-03a2-42ee-9304-9a6044348b4a", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3f17ae9e449428509c86f52ebfb120a8347d008c6470ed296483a0e88ccbd955"}, "3": {"node_id": "263fff6d-4c29-47c2-a639-42e8632f22be", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "f41a7ef58b78871da2fb1064ed3952e1db7a3496995f598bb493eef69e6478cf"}}, "hash": "b685517f5b7b16083ec962aa28df8129a9897253b148242419690a53757294cb", "text": "=== Australia ===\nIn October 2023, the Australian Computer Society, Business Council of Australia, Australian Chamber of Commerce and Industry, Ai Group, Council of Small Business Organisations Australia, and Tech Council of Australia jointly published an open letter calling for a national approach to AI strategy. The letter backs the federal government establishing a whole-of-government AI taskforce.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "263fff6d-4c29-47c2-a639-42e8632f22be": {"__data__": {"id_": "263fff6d-4c29-47c2-a639-42e8632f22be", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "4bcbb9c5-077e-4e4e-b40c-f6a75032a85f", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "b685517f5b7b16083ec962aa28df8129a9897253b148242419690a53757294cb"}, "3": {"node_id": "cede9f70-aeea-4752-a903-ff301f877f77", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8ed1709e5e3b6a168fe02db4c9e84b42825b6704782215a476de5954477c3872"}}, "hash": "f41a7ef58b78871da2fb1064ed3952e1db7a3496995f598bb493eef69e6478cf", "text": "== Regulation of fully autonomous weapons ==\n\nLegal questions related to lethal autonomous weapons systems (LAWS), in particular compliance with the laws of armed conflict, have been under discussion at the United Nations since 2013, within the context of the Convention on Certain Conventional Weapons. Notably, informal meetings of experts took place in 2014, 2015 and 2016 and a Group of Governmental Experts (GGE) was appointed to further deliberate on the issue in 2016.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cede9f70-aeea-4752-a903-ff301f877f77": {"__data__": {"id_": "cede9f70-aeea-4752-a903-ff301f877f77", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "263fff6d-4c29-47c2-a639-42e8632f22be", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "f41a7ef58b78871da2fb1064ed3952e1db7a3496995f598bb493eef69e6478cf"}, "3": {"node_id": "42a45e7b-2556-47a9-a9e7-9b3a1f2584fd", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3f452f564c2d7fe17f259d4556db6d579129a5c6dcc8a19a4f077ad77870ef40"}}, "hash": "8ed1709e5e3b6a168fe02db4c9e84b42825b6704782215a476de5954477c3872", "text": "Notably, informal meetings of experts took place in 2014, 2015 and 2016 and a Group of Governmental Experts (GGE) was appointed to further deliberate on the issue in 2016. A set of guiding principles on LAWS affirmed by the GGE on LAWS were adopted in 2018.In 2016, China published a position paper questioning the adequacy of existing international law to address the eventuality of fully autonomous weapons, becoming the first permanent member of the U.N. Security Council to broach the issue, and leading to proposals for global regulation.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "42a45e7b-2556-47a9-a9e7-9b3a1f2584fd": {"__data__": {"id_": "42a45e7b-2556-47a9-a9e7-9b3a1f2584fd", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "cede9f70-aeea-4752-a903-ff301f877f77", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8ed1709e5e3b6a168fe02db4c9e84b42825b6704782215a476de5954477c3872"}, "3": {"node_id": "34fb0525-c09e-43d9-af23-6cd13886c5a7", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "40eb882a8163af4b498f5034b7be09bd23ea097d51ac9604d05a02dea1ccbad3"}}, "hash": "3f452f564c2d7fe17f259d4556db6d579129a5c6dcc8a19a4f077ad77870ef40", "text": "Security Council to broach the issue, and leading to proposals for global regulation. The possibility of a moratorium or preemptive ban of the development and use of LAWS has also been raised on several occasions by other national delegations to the Convention on Certain Conventional Weapons and is strongly advocated for by the Campaign to Stop Killer Robots \u2013 a coalition of non-governmental organizations. The US government maintains that current international humanitarian law is capable of regulating the development or use of LAWS.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "34fb0525-c09e-43d9-af23-6cd13886c5a7": {"__data__": {"id_": "34fb0525-c09e-43d9-af23-6cd13886c5a7", "embedding": null, "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "65363817-5336-4d6c-bfb7-08d430301961", "node_type": "4", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "8d5bdfda888a86b408f3f5fc2b7f2baba4341507db7647dbf122c91fe5b995a7"}, "2": {"node_id": "42a45e7b-2556-47a9-a9e7-9b3a1f2584fd", "node_type": "1", "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}, "hash": "3f452f564c2d7fe17f259d4556db6d579129a5c6dcc8a19a4f077ad77870ef40"}}, "hash": "40eb882a8163af4b498f5034b7be09bd23ea097d51ac9604d05a02dea1ccbad3", "text": "The US government maintains that current international humanitarian law is capable of regulating the development or use of LAWS. The Congressional Research Service indicated in 2023 that the US doesn't have LAWS in its inventory, but that its policy doesn't prohibit the development and employment of it.\n\n\n== See also ==\nArtificial intelligence\nArtificial intelligence arms race\nAI alignment\nAlgorithmic accountability\nArtificial intelligence in government\nEthics of artificial intelligence\nGovernment by algorithm\nLegal informatics\nRegulation of algorithms\nSelf-driving car liability \u00a7 Artificial intelligence and liability\n\n\n== References ==", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"ab184f8d-c760-412a-811c-1ba1cc8fe001": {"node_ids": ["042edef9-b751-4724-8c31-1b1c502bdbc4", "d9bee6e0-0494-4c2a-b4e9-88b6be0c408c", "2c308e00-297d-4274-831f-6526049e6691", "0d299ea1-def4-4e06-8320-c1c351cefe01", "3aca4643-ff45-4ebb-a3bf-a2d0e7770665", "f8d3eb6b-479f-4023-a950-7bc3ca480d12", "7bd111ce-34ab-4314-a3bc-ddfcd30bb2b6"], "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}}, "d8e52e57-c639-4549-8810-1ffa5dcaacaa": {"node_ids": ["aae2e016-7238-4cb0-939f-e6070a8aa2b0", "d270bfb9-b64d-4290-93b6-9bc0809d9f72", "d63bf570-de6e-4923-a692-79b972e8f7d3", "93d2cd52-2b06-4b7c-9537-6bfdc89a54dd", "12a650d5-e667-4693-9374-2b69a03b3105", "ba35a5ec-c74a-44f2-bb73-bdd946eaca53", "87850a17-646f-484d-8adc-8ebb7997d04a", "546e8a40-0e9a-4836-80e3-26eea1c95f35", "f0f0ffbd-6f4f-4d5e-ae19-763997e3e798", "4c64169d-1b94-40a4-8b8c-de43afec38a9", "9240cce1-811f-4bb8-80c4-610d31ed8dac", "e461dcfe-4b53-4350-84b2-ea0fdc9c1293", "15cdbad5-4acd-41da-8bb5-bc07c0710d00", "b8bf6fae-609f-40c7-9d2a-4b67964724fe", "ea183e0a-5dcf-4aca-8fc6-fac83ebd942a", "1b807f77-f3b1-4277-af65-2bc92fb8f713", "0e574837-6d90-42f9-9879-33a3e0dec967", "593b5131-d881-49bb-b3e9-5379e5466ac2", "a4bda4c5-1458-4b2a-bb57-e0f4ad833f59", "b0d89754-d4f7-420f-acc4-430472dc2e7a", "8db0584b-a51e-4010-a9c4-3718632d43e0", "3fc9e14a-e721-47a9-b556-7b5751ad1a69", "c4f010f0-118f-4eb0-b956-5de06903cfa2", "ad60a16f-6058-4440-ba73-fecbb2e10134", "d6969ce0-d9c8-49e2-aa92-f35d981c10de", "2145fe14-0623-467a-9a70-0b4be7ffd744", "aa1dbc24-3b70-4a7b-a09f-e05f62f85363", "43e95adb-dcc3-47bb-9d35-1e42b4c9bce6", "070f5bb1-d53e-4c4b-830c-83b53a1b20dd", "03d6e643-8914-4a8e-a200-2627e93fc952", "6e6505f3-1b0f-428f-b72f-b283ed3b9bee", "63bb412f-cc66-43f8-a42d-b0f00b896c84", "d98a3d14-b13e-459a-a706-98da4cabd9a6", "e71fe797-b5a4-410e-be86-66632df1cea7", "caabac1a-04d2-46e5-9ef5-d769c1197ceb", "5eff0e2c-96ba-4971-b74d-6bdd581b5d45", "0d6545b5-779a-4d61-8d4f-65bc48bbeb34", "e18fbb09-ad51-4a5b-abc7-2e0199837f68", "6ab731ed-436c-472f-8af6-7f2ff8db3ebf", "02f37161-01e6-4be4-b903-cdda8ee5db03", "0cc21dd4-d3c5-4d0c-91f3-459545059e5a", "882e9675-a534-4714-a7b0-e2ee539ef6d5", "c067fafb-01af-4f55-bb98-4448ce136780", "9ce172bd-0660-46ce-a683-a32d5fa9f975", "fedd03f1-7661-495e-919f-bc326d0eee63"], "metadata": {"source_url": "https://en.wikipedia.org/wiki/AI_safety"}}, "30517438-0b0f-44d2-8c9b-b3a469e5cfcc": {"node_ids": ["eb7882f6-cda1-4c50-b0de-c670755f0775", "4867c6f0-db3e-43b4-be54-9b631158da63", "7c9c08cc-729b-46a0-9d97-9db67fd7a53f", "46660992-36a4-450e-bc57-4871f3733ad0", "fd8e1107-a286-4a48-8433-ce7067338e76", "a36a5e4b-93a5-47af-8938-c71355dd27cb", "067e6976-1d78-44f6-a1b3-e8d70b337530"], "metadata": {"source_url": "https://en.wikipedia.org/wiki/2023_AI_Safety_Summit"}}, "65363817-5336-4d6c-bfb7-08d430301961": {"node_ids": ["fdf71849-d38d-40e6-ac92-fc5063ce1561", "23aa273d-a776-454b-b599-691c5ef8f2c1", "2be8cca2-c579-4f24-8609-801ee3c988dc", "d73866da-49bc-471d-b20a-c8c506be2ce5", "e9d5a6d0-4aac-48e3-9771-7cefafd9d857", "d111559a-76cb-43bf-801b-b9e82bfe92db", "fcc64e6c-0718-4eb2-8283-bfd3d0e21de4", "c1655410-1d99-48aa-abf1-bc37f679eee5", "9230d434-6f20-4780-b715-0ea35874a80f", "6242057d-19c2-440d-a21d-e0b78a357c10", "b8df5e67-02dd-42ad-80a7-dd8541387f8d", "a2db172b-2255-40da-9047-78e2fe627598", "8a4474f1-718b-41f0-b5ef-85f5eda713c9", "4419fd28-5091-4e41-97dd-ab737346391b", "c74b22da-33c7-4853-9f12-ba1eb5029004", "0f79b9e9-8a3c-4dd7-8d4c-217a396c4d74", "fa15c857-9e1e-4f0c-9da6-8fb2efb31ab1", "b691ee06-afd7-4fd2-b456-366c024208c6", "570af176-733d-4f36-8770-3255c534391c", "60b50137-4a7d-426c-86bf-3c98ebb88a0a", "aa280449-4d53-42bf-ac81-153b123044e3", "1578dfbb-2b39-477d-a77e-e92ab04390f1", "d7098edc-db47-4a53-b513-e683c1adb530", "cb7cd854-1a79-4046-af28-cd663855f957", "8a6e4da9-99b0-4594-a4d4-6d8553356aac", "3bb84b19-fc82-4e58-ad90-3532bba67f4d", "75121b9f-721c-4abf-b29f-983265977ec4", "8778830c-3d49-4dad-bd4c-080be200c8d2", "af170c16-e002-479b-971b-c8c7a29ccbb0", "45fd9949-e19e-4bfe-b443-186b464973f0", "f7d7a1ba-be32-45af-9216-34266157bba6", "e37d5612-5f7f-4030-990c-791b5497a819", "c6554df6-82ae-4d6f-a6e2-8cbecc0ecea6", "b4b92fc7-7b46-47a6-b87f-1786ff556f4c", "d40f2aae-8322-4d24-83f8-dcb9eacb59db", "d5b94312-ee42-4e5f-a955-dfa0430f8008", "290e9d95-16d9-4e85-880e-1d316c9443df", "11e65c59-21b6-495d-bbc5-54298c09598c", "7b069af8-c943-4fb0-939f-5809a58eeb76", "772e1dbb-0835-49c6-ab2a-2180a3f771fd", "8daa2da7-00f7-4d5a-90f5-fa1df3cd8908", "6195c20e-fc01-4d0b-9dee-801976e855ca", "72e70df6-1bcf-45ed-a43f-18cee1431209", "df2ac080-26d7-4347-9f0c-a3e097959365", "6c19ff9a-3d85-473c-bf54-f306495fae04", "5129b3ff-01ab-4c25-8e2d-abc44be4b62e", "cf4c59b0-be0d-4047-95ee-f8261fec9e11", "c7993032-c771-46f6-9019-da47b37813ae", "f32305e1-faf3-43fa-bca1-c9a8b3ff45b9", "a1912be1-07e2-4e6b-b577-9c8148b714ce", "4c857409-72e7-460d-b8d4-316fd13856c6", "e061f964-72eb-486f-bf2b-77a476712fd5", "64df011c-f258-4eee-bb38-f13816abcc4a", "dc084ac4-97b1-4297-8fd2-96e5452fe484", "bdcf2ecb-e0ca-42fb-83f8-f0ebae060696", "ec0111d0-404b-48fc-96af-e1861ff7c2cf", "2d04f17c-7afa-4e47-8c30-ec989dcd0e1a", "02820d6b-5f5e-4dfc-90d0-d7ab9ae9402e", "277b92e6-a05b-4d83-af32-cebb2203090b", "b60653b7-3fbe-497e-ac43-df0ffe9177dd", "2d433ebe-7697-4aaf-9aab-9fe004c4412c", "be5c068d-5450-4c4e-88f1-c04163b345b1", "cdb0ad52-7a9d-4f4b-b561-e6b422f74002", "d950918f-c029-4bd2-b943-fd13e36392d4", "bdd68646-85b9-4e9e-86a3-f8197a1ae594", "68837bf5-a10c-4ea0-873c-3636efee4108", "46313293-0201-45e0-9a4b-a0db39c0cf85", "cc61d19e-f3b0-4e0f-aef7-4fa3d71e6bf9", "9d069afd-6296-4a25-bbd9-699780c73856", "d65371b3-499b-4700-9b3c-e30619e15c0f", "c8bc0862-97bd-4ca8-8217-fc0585303cac", "d6242ba1-4b98-4595-be68-48a170bf9120", "ac35d610-8788-42e0-b097-50f0d50e51de", "fddbddea-bc4d-4f8b-b38b-9e9bcdb6ae3e", "595aebba-f9e5-413d-b27c-547e074b82e7", "70f378c9-0dbe-492b-a7a1-70ae65c4d445", "1fafb5e2-7d1e-4be8-a5ea-666ae33cf0c4", "276f85bf-ecad-4562-a48d-8a7a07742bae", "e8511380-03a2-42ee-9304-9a6044348b4a", "4bcbb9c5-077e-4e4e-b40c-f6a75032a85f", "263fff6d-4c29-47c2-a639-42e8632f22be", "cede9f70-aeea-4752-a903-ff301f877f77", "42a45e7b-2556-47a9-a9e7-9b3a1f2584fd", "34fb0525-c09e-43d9-af23-6cd13886c5a7"], "metadata": {"source_url": "https://en.wikipedia.org/wiki/Regulation_of_artificial_intelligence"}}}}