{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Gen Tutorial\n",
    "Note book written by John Adeojo\n",
    "Founder, and Chief Data Scientist at [Data-centric Solutions](https://www.data-centric-solutions.com/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "import yaml\n",
    "import openai \n",
    "import os\n",
    "\n",
    "script_dir = \"C:/Users/johna/OneDrive/Documents/api_keys/\"\n",
    "index_path = \"G:/My Drive/Data-Centric Solutions/07. Blog Posts/AutoGen/autogen_tutorial/indexes/\"\n",
    "configurations_path = \"G:/My Drive/Data-Centric Solutions/07. Blog Posts/AutoGen/autogen_tutorial/\"\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file=\"configurations.json\",\n",
    "    file_location=configurations_path,\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-3.5-turbo-16k\"],\n",
    "    },\n",
    ")\n",
    "\n",
    "def get_apikey(script_dir=script_dir):\n",
    "\n",
    "    script_dir = script_dir\n",
    "    file_path = os.path.join(script_dir, \"apikeys.yml\")\n",
    "\n",
    "    with open(file_path, 'r') as yamlfile:\n",
    "        loaded_yamlfile = yaml.safe_load(yamlfile)\n",
    "        API_KEY = loaded_yamlfile['openai']['api_key']\n",
    "\n",
    "    return API_KEY\n",
    "\n",
    "openai.api_key = get_apikey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "\n",
    "from llama_index.readers.base import BaseReader\n",
    "from llama_index.readers.schema.base import Document\n",
    "import wikipedia\n",
    "\n",
    "class WikipediaReader(BaseReader):\n",
    "    def load_data(self, pages: List[str], lang: str = \"en\", **load_kwargs: Any) -> List[Document]:\n",
    "        results = []\n",
    "        for page in pages:\n",
    "            wikipedia.set_lang(lang)\n",
    "            wiki_page = wikipedia.page(page, **load_kwargs)\n",
    "            page_content = wiki_page.content\n",
    "            page_url = wiki_page.url\n",
    "            # Create a Document with URL included in the metadata\n",
    "            document = Document(text=page_content, metadata={'source_url': page_url})\n",
    "            results.append(document)\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool 1: Does a query based search for Wikipages\n",
    "import wikipedia\n",
    "from llama_index import download_loader, VectorStoreIndex, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "from llama_index.text_splitter import get_default_text_splitter\n",
    "from llama_index import StorageContext\n",
    "from llama_index import load_index_from_storage\n",
    "import json\n",
    "\n",
    "def load_index(filepath: str):\n",
    "    # rebuild storage context\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=index_path)\n",
    "    # load index\n",
    "    return load_index_from_storage(storage_context)\n",
    "\n",
    "def read_json_file(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def create_wikidocs(wikipage_requests):\n",
    "    print(f\"Preparing to Download:{wikipage_requests}\")\n",
    "    documents = []\n",
    "    for page_title in wikipage_requests:\n",
    "        try:\n",
    "            # Attempt to load the Wikipedia page\n",
    "            wiki_page = wikipedia.page(page_title)\n",
    "            page_content = wiki_page.content\n",
    "            page_url = wiki_page.url\n",
    "            document = Document(text=page_content, metadata={'source_url': page_url})\n",
    "            documents.append(document)\n",
    "        except wikipedia.exceptions.PageError:\n",
    "            # Handle the case where the page does not exist\n",
    "            print(f\"PageError: The page titled '{page_title}' does not exist on Wikipedia.\")\n",
    "        except wikipedia.exceptions.DisambiguationError as e:\n",
    "            # Handle the case where the page title is ambiguous\n",
    "            print(f\"DisambiguationError: The page titled '{page_title}' is ambiguous. Possible options: {e.options}\")\n",
    "    print(\"Finished downloading pages\")\n",
    "    return documents\n",
    "\n",
    "\n",
    "def index_wikipedia_pages(wikipage_requests):\n",
    "    print(f\"Preparing to index Wikipages: {wikipage_requests}\")\n",
    "    documents = create_wikidocs(wikipage_requests)\n",
    "    text_splits = get_default_text_splitter(chunk_size=150, chunk_overlap=45)\n",
    "    parser = SimpleNodeParser.from_defaults(text_splitter=text_splits)\n",
    "    service_context = ServiceContext.from_defaults(node_parser=parser)\n",
    "    index =  VectorStoreIndex.from_documents(documents, service_context=service_context, show_progress=False)\n",
    "    index.storage_context.persist(index_path)\n",
    "    print(f\"{wikipage_requests} have been indexed.\")\n",
    "    return \"indexed\"\n",
    "\n",
    "def search_and_index_wikipedia(\n",
    "        hops: list, lang: str = \"en\", results_limit: int = 2\n",
    "    ):\n",
    "\n",
    "    # Set the language for Wikipedia\n",
    "    wikipedia.set_lang(lang)\n",
    "\n",
    "    # Initialize an empty list to hold all indexed page titles\n",
    "    wikipage_requests = []\n",
    "\n",
    "    # Loop through the identified hops and search for each\n",
    "    for hop in hops:\n",
    "        hop_pages = wikipedia.search(hop, results=results_limit)\n",
    "        print(f\"Searching Wikipedia for: {hop} - Found: {hop_pages}\")\n",
    "        wikipage_requests.extend(hop_pages)\n",
    "\n",
    "    # Index the gathered pages (assuming 'index_wikipedia_pages' is a defined function that you implement)\n",
    "    index_wikipedia_pages(wikipage_requests)\n",
    "\n",
    "    return wikipage_requests\n",
    "\n",
    "\n",
    "def query_wiki_index(hops: List[str], index_path: str = index_path, n_results: int = 5): \n",
    "    index = load_index(filepath=index_path)\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"compact\", verbose=True, similarity_top_k=n_results\n",
    "    )\n",
    "    \n",
    "    retrieved_context = {}\n",
    "    \n",
    "    # Iterate over each hop in the multihop query\n",
    "    for hop in hops:\n",
    "        nodes = query_engine.query(hop).source_nodes\n",
    "        \n",
    "        # Process each node found for the current hop\n",
    "        for node in nodes:\n",
    "            doc_id = node.node.id_\n",
    "            doc_text = node.node.text\n",
    "            doc_source = node.node.metadata.get('source_url', 'No source URL')  # Default value if source_url is not present.\n",
    "            \n",
    "            # Append to the list of texts and sources for each doc_id\n",
    "            if doc_id not in retrieved_context:\n",
    "                retrieved_context[doc_id] = {'texts': [doc_text], 'sources': [doc_source]}\n",
    "            else:\n",
    "                retrieved_context[doc_id]['texts'].append(doc_text)\n",
    "                retrieved_context[doc_id]['sources'].append(doc_source)\n",
    "\n",
    "    # Serialise the context for all hops into a JSON file\n",
    "    file_path = index_path + \"retrieved_context.json\"\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(retrieved_context, f)\n",
    "    \n",
    "    return retrieved_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm_config = {\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"name\": \"search_and_index_wikipedia\",\n",
    "            \"description\": \"Indexes Wikipedia pages based on specified queries for each hop to build a knowledge base for future reference. Use before query_wiki_index.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"hops\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\"\n",
    "                        },\n",
    "                        \"description\": \"The search queries for identifying relevant Wikipedia pages to index, each corresponding to a hop in the multihop question.\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"hops\"],\n",
    "            },\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"query_wiki_index\",\n",
    "            \"description\": \"Queries the indexed Wikipedia knowledge base to retrieve pertinent information across multiple hops\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"hops\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"string\"\n",
    "                        },\n",
    "                        \"description\": \"The search queries to search the indexed Wikipedia knowledge base for relevant information, each corresponding to a hop in the multihop question.\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"hops\"],\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    "    \"config_list\": config_list,\n",
    "    \"request_timeout\": 120,\n",
    "    \"seed\": 100,\n",
    "    \"temperature\":0.7\n",
    "}\n",
    "\n",
    "# The llm_config_no_tools remains the same, excluding the 'functions' key.\n",
    "llm_config_no_tools = {k: v for k, v in llm_config.items() if k != 'functions'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "What was the population of London in 2001 and what did it increase to in January 2023?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: search_and_index_wikipedia *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"hops\": [\"Population of London in 2001\", \"Population of London in January 2023\"]\n",
      "}\n",
      "\u001b[32m***************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION search_and_index_wikipedia...\u001b[0m\n",
      "Searching Wikipedia for: Population of London in 2001 - Found: ['Ethnic groups in London', 'London']\n",
      "Searching Wikipedia for: Population of London in January 2023 - Found: ['List of European cities by population within city limits', '2023']\n",
      "Preparing to index Wikipages: ['Ethnic groups in London', 'London', 'List of European cities by population within city limits', '2023']\n",
      "Preparing to Download:['Ethnic groups in London', 'London', 'List of European cities by population within city limits', '2023']\n",
      "Finished downloading pages\n",
      "['Ethnic groups in London', 'London', 'List of European cities by population within city limits', '2023'] have been indexed.\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"search_and_index_wikipedia\" *****\u001b[0m\n",
      "['Ethnic groups in London', 'London', 'List of European cities by population within city limits', '2023']\n",
      "\u001b[32m***********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function Call: query_wiki_index *****\u001b[0m\n",
      "Arguments: \n",
      "{\n",
      "  \"hops\": [\"Population of London in 2001\", \"Population of London in January 2023\"]\n",
      "}\n",
      "\u001b[32m*****************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION query_wiki_index...\u001b[0m\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function \"query_wiki_index\" *****\u001b[0m\n",
      "{'059d706d-0631-4273-aa22-8fddeeb86588': {'texts': [\"London's continuous urban area extends beyond Greater London and numbered 9,787,426 people in 2011, while its wider metropolitan area had a population of 12–14 million, depending on the definition used. According to Eurostat, London is the second most populous metropolitan area in Europe. A net 726,000 immigrants arrived there in the period 1991–2001.The region covers 610 square miles (1,579 km2), giving a population density of 13,410 inhabitants per square mile (5,177/km2) more than ten times that of any other British region.\", \"London's continuous urban area extends beyond Greater London and numbered 9,787,426 people in 2011, while its wider metropolitan area had a population of 12–14 million, depending on the definition used. According to Eurostat, London is the second most populous metropolitan area in Europe. A net 726,000 immigrants arrived there in the period 1991–2001.The region covers 610 square miles (1,579 km2), giving a population density of 13,410 inhabitants per square mile (5,177/km2) more than ten times that of any other British region.\"], 'sources': ['https://en.wikipedia.org/wiki/London', 'https://en.wikipedia.org/wiki/London']}, 'bd650699-67dc-4647-bc59-a62e4ce2386e': {'texts': [\"== Demography ==\\n\\nThe 2021 census recorded that 3,575,739 people or 40.6% of London's population were foreign-born, making it among the cities with the largest immigrant population in terms of absolute numbers. About 69% of children born in London in 2015 had at least one parent who was born abroad.The population then grew by just over a million between the 2001 and 2011 Censuses, to reach 8,173,941 in the latter.\", \"== Demography ==\\n\\nThe 2021 census recorded that 3,575,739 people or 40.6% of London's population were foreign-born, making it among the cities with the largest immigrant population in terms of absolute numbers. About 69% of children born in London in 2015 had at least one parent who was born abroad.The population then grew by just over a million between the 2001 and 2011 Censuses, to reach 8,173,941 in the latter.\"], 'sources': ['https://en.wikipedia.org/wiki/London', 'https://en.wikipedia.org/wiki/London']}, '1f5c2c89-6a64-41cf-b65e-6240b85a9410': {'texts': [\"In January 2015, Greater London's population was estimated to be 8.63 million, its highest since 1939. During the Brexit referendum in 2016, the UK as a whole decided to leave the European Union, but most London constituencies voted for remaining. However, Britain's exit from the EU in early 2021 only marginally weakened London's position as an international financial centre.On 6 May 2023, the coronation of Charles III and his wife, Camilla, as king and queen of the United Kingdom and the other Commonwealth realms, took place at Westminster Abbey, London.\\n\\n\\n== Administration ==\", \"In January 2015, Greater London's population was estimated to be 8.63 million, its highest since 1939. During the Brexit referendum in 2016, the UK as a whole decided to leave the European Union, but most London constituencies voted for remaining. However, Britain's exit from the EU in early 2021 only marginally weakened London's position as an international financial centre.On 6 May 2023, the coronation of Charles III and his wife, Camilla, as king and queen of the United Kingdom and the other Commonwealth realms, took place at Westminster Abbey, London.\\n\\n\\n== Administration ==\"], 'sources': ['https://en.wikipedia.org/wiki/London', 'https://en.wikipedia.org/wiki/London']}, '2a24c378-c913-48cd-b625-45d011a47035': {'texts': [\"In January 2005, a survey of London's ethnic and religious diversity claimed that more than 300 languages were spoken in London and more than 50 non-indigenous communities had populations of more than 10,000. Figures from the Office for National Statistics show that in 2021, London's foreign-born population was 3,346,000 (35%), up from 1,630,000 in 1997.The 2011 census showed that 36.7% of Greater London's population were born outside the UK.\", \"In January 2005, a survey of London's ethnic and religious diversity claimed that more than 300 languages were spoken in London and more than 50 non-indigenous communities had populations of more than 10,000. Figures from the Office for National Statistics show that in 2021, London's foreign-born population was 3,346,000 (35%), up from 1,630,000 in 1997.The 2011 census showed that 36.7% of Greater London's population were born outside the UK.\"], 'sources': ['https://en.wikipedia.org/wiki/London', 'https://en.wikipedia.org/wiki/London']}, 'd1129daa-c088-4e4f-9aad-e072c436c583': {'texts': ['=== Black population of London ===\\nAt the 2011 census, the total Black population of London stood at 1,101,688. This is a rise of 39% from the 2001 census, when the population stood at 781,751.\\nInner London and Outer London have a near-equal black population.'], 'sources': ['https://en.wikipedia.org/wiki/Ethnic_groups_in_London']}, '55ece390-3fc8-4256-a663-bb684f9fed8c': {'texts': [\"Those aged 65 and over are 13.6% in Outer London, but only 9.3% in Inner London.The median age of London's residents in 2018 was 36.5, which was younger than the UK median of 40.3.\\n\\n\\n=== Ethnic groups ===\\n\\nAccording to the Office for National Statistics, based on 2011 Census estimates, 59.8 per cent of the 8,173,941 inhabitants of London were White, with 44.9% White British, 2.2% White Irish, 0.1% gypsy/Irish traveller and 12.1% classified as Other White.\"], 'sources': ['https://en.wikipedia.org/wiki/London']}}\n",
      "\u001b[32m*************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to chat_manager):\n",
      "\n",
      "According to the information gathered from Wikipedia:\n",
      "\n",
      "1. The population of London in 2001 was approximately 8,173,941. \n",
      "   Source: [Wikipedia - London](https://en.wikipedia.org/wiki/London)\n",
      "\n",
      "2. The population of London in January 2023 is not specifically mentioned. However, in 2011, the continuous urban area of London had a population of 9,787,426, while its wider metropolitan area had a population of 12-14 million depending on the definition used.\n",
      "   Source: [Wikipedia - London](https://en.wikipedia.org/wiki/London)\n",
      "\n",
      "INFORMATION GATHERING COMPLETE\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mreporter\u001b[0m (to chat_manager):\n",
      "\n",
      "The population of London in 2001 was approximately 8,173,941. However, the specific population of London in January 2023 is not provided in the indexed information. In 2011, the continuous urban area of London had a population of 9,787,426, while its wider metropolitan area had a population of 12-14 million, depending on the definition used.\n",
      "\n",
      "Please note that the exact population of London in January 2023 is not available, but the information from 2011 provides an estimate of the population size.\n",
      "\n",
      "[SOURCES]\n",
      "- [Wikipedia - London](https://en.wikipedia.org/wiki/London)\n",
      "\n",
      "PLEASE REVIEW\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mmoderator\u001b[0m (to chat_manager):\n",
      "\n",
      "The answer is approved. The Reporter has provided accurate information based on the indexed content and has appropriately cited the sources.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mmoderator\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import autogen \n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\") and x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=5,\n",
    "    system_message=\"You should start the workflow by consulting the assistant, then the reporter and finally the moderator\"\n",
    "    \n",
    ")\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message='''\n",
    "    As the Information Gatherer, you must start by using the `search_and_index_wikipedia` function to gather relevant data about the user's query. Follow these steps:\n",
    "\n",
    "    1. Upon receiving a query, immediately invoke the `search_and_index_wikipedia` function to find and index Wikipedia pages related to the query. Do not proceed without completing this step.\n",
    "    2. After successfully indexing, utilize the `query_wiki_index` to extract detailed information from the indexed content.\n",
    "    3. Present the indexed information and detailed findings to the Reporter, ensuring they have a comprehensive dataset to draft a response.\n",
    "    4. Conclude your part with \"INFORMATION GATHERING COMPLETE\" to signal that you have finished collecting data and it is now ready for the Reporter to use in formulating the answer.\n",
    "\n",
    "    Remember, you are responsible for information collection and indexing only. The Reporter will rely on the accuracy and completeness of your findings to generate the final answer.\n",
    "\n",
    "    ''',\n",
    "    llm_config=llm_config,\n",
    "    # human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "reporter = autogen.AssistantAgent(\n",
    "    name=\"reporter\",\n",
    "    system_message='''\n",
    "    As the Reporter, you are responsible for formulating an answer to the user's query using the information provided by the Information Gatherer.\n",
    "\n",
    "    1. Wait for the Information Gatherer to complete their task and present you with the indexed information.\n",
    "    2. Using the gathered data, create a comprehensive and precise response that adheres to the criteria of precision, depth, clarity, and proper citation.\n",
    "    3. Present your draft answer followed by \"PLEASE REVIEW\" for the Moderator to assess.\n",
    "\n",
    "    If the Moderator approves your answer, conclude with \"TERMINATE\" to signal the end of the interaction.\n",
    "\n",
    "    If the Moderator rejects your answer:\n",
    "    - Review their feedback.\n",
    "    - Make necessary amendments.\n",
    "    - Resubmit the revised answer with \"PLEASE REVIEW.\"\n",
    "\n",
    "    Ensure that your response is fully informed by the data provided and meets the established criteria.\n",
    "\n",
    "    criteria are as follows:\n",
    "     A. Precision: Directly address the user's question.\n",
    "     B. Depth: Provide comprehensive information using indexed content.\n",
    "     C. Citing: Include clear references from the indexed content AND the Wikipedia link source as a citation.\n",
    "     D. Clarity: Present information logically and coherently.\n",
    "\n",
    "    ''',\n",
    "    llm_config=llm_config_no_tools,\n",
    "    \n",
    "    # human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "moderator = autogen.AssistantAgent(\n",
    "    name=\"moderator\",\n",
    "    system_message='''\n",
    "\n",
    "    As the Moderator, your task is to review the Reporter's answers to ensure they meet the required criteria:\n",
    "\n",
    "    - Assess the Reporter's answers after the \"PLEASE REVIEW\" prompt for alignment with the following criteria:\n",
    "     A. Precision: Directly addressed the user's question.\n",
    "     B. Depth: Provided comprehensive information using indexed content.\n",
    "     C. Citing: Clear references from the indexed content AND the Wikipedia link source as a citation.\n",
    "     D. Clarity: information presented logically and coherently.\n",
    "    - Approve the answer by stating \"The answer is approved\" if it meets the criteria.\n",
    "    - If the answer falls short, specify which criteria were not met and instruct the Reporter to revise the answer accordingly. Do not generate new content or answers yourself.\n",
    "\n",
    "    Your role is crucial in ensuring that the final answer provided to the user is factually correct and meets all specified quality standards.\n",
    "\n",
    "    ''',\n",
    "    llm_config=llm_config_no_tools,\n",
    "    # human_input_mode=\"NEVER\"\n",
    ")\n",
    "\n",
    "user_proxy.register_function(\n",
    "    function_map={\n",
    "        \"search_and_index_wikipedia\": search_and_index_wikipedia,\n",
    "        \"query_wiki_index\":query_wiki_index,\n",
    "        # \"generate_response\":generate_response\n",
    "    }\n",
    ")\n",
    "\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, assistant, reporter, moderator], messages=[], max_round=20)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)\n",
    "user_proxy.initiate_chat(\n",
    "    manager, \n",
    "    message='''What was the population of London in 2001 and what did it increase to in January 2023?'''\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
